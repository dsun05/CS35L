So if we had more time, I'd want to be talking about more Python. And I'd like to also do a little bit more client server computing. Turn down the microphone a little bit here. Right, so client server. And in order to talk about that, we really need to talk about networking.

And to some extent I have to give you an introduction to CS1. I turned it off too far. All right, let's try it again. Can you hear me? Yes, okay. Client server computing, networking, this is really CS118, which we should cover in, you know, a half an hour or something like that. And then we can finally do a demo. Oh, well, you need to talk about, you know, things like, oh, I don't know, HTML and CSS.

and JavaScript and then we can talk about note. That's a lot to talk about in one lecture. And we have another assignment coming up, right? Assignment three, the easiest assignment in the class, right? It's just a kind of a hello world problem, very simple sort of, well, a little harder than hello world, but not that hard. And it's written in note.

I should do things in a different order than I would do if things were really organized. I'm an engineering type and so I like to know how everything actually works way down to the machine level and we would sort of see that this way. Well, skipping over the Python, right? But I thought we'd try today doing it in the opposite order. Let's start off with Node and then we'll work our way back and see how it's working and all that sort of thing.

I have to do a little bit of client service. Because that's the essence of the third assignment is to have you do something that's a little different from what you've done in CS31 and what you've done earlier in this course. Until now, you've always written a program that runs on a computer. It's classic computing. It's been done this way ever since the 1950s, right?

Now we're going to do client server, which is still pretty classic. Now we're talking 1970s. But there's something new here. We have two computers. Or, since we're software people, we shouldn't think about them as computers. We have two programs. They're both running simultaneously and they're talking to each other. And we want the entire system to work.

It's getting two programs to work. As you can probably guess from the name client server, what we can think of is here's a network. Some people like to draw it as a nice little cloud. I tend to draw it often that way, but I'll draw it as just a line that you can hook up anywhere in the network. Somewhere on the network, there's a client. Somewhere else on the network, there's a server.

And the way the application runs is some of it runs in point A and some of it runs in point B. This is not the only way to do stuff on the network. Not by any stretch of the imagination. But it's kind of the simplest. If you're going to have a network, it's interesting only if you have at least two things on the network. Otherwise, it's just a single computer. The simplest way you can have two things going is with the client and server.

Now, the typical way this is done is the following. The server has a database and this database stores the state of the system. Now, it's not a complete description of the state of the system, but let's put it this way. It's the important part of the state that you care about in your application. Okay? The client doesn't store any state.

Again, it does store some stuff, but it's nothing important as far as the application goes. Right? So if the state here is, I don't know, the registrar's database of all the courses that are at UCLA and who's enrolled in which courses and all that sort of thing, that's kept on a server deep underground in Murphy Hall or someplace like that. Right? And what's on your laptop doesn't matter. Your laptop doesn't say whether you're enrolled in this class. It's that server underneath Murphy Hall that says, right? So your laptop will be the client, right? And you will talk to the server and get enrolled in the class or not. Very typically in a client-server system, there will be multiple clients, right? Client 1, client 2, client 3. But what they'll do in a client-server system is they'll talk to the network, but they'll only talk to the server.

They won't talk to each other. If client one wants to affect what's on client two's screen, that is, for example, you enroll in the last slot in the class and now the next person can't enroll because the class is full. The way that happens is the clients talk to the server. The server's in charge of who's actually enrolled, right? And the client can ask the server to make a change and if the server likes the client, the server will say, change the state, but really it's the server that's in charge here and the clients make requests. Of course, nothing really interesting will happen unless there are clients who make requests. In some sense, I guess you could say the clients are also in control, but really at the bottom, the server's in charge and if the server decides to do nothing, then nothing will happen.

that you're going to be building for your project. And it's the kind of application that homework three is warming up with. Any questions on this basic structure? Can you see anything that might go wrong? Right? Since you're engineers, the first thing you should say is what can break? Because you're in charge of either breaking things or making sure they don't stay broken. Yes?

Oh, excellent question, right? Two people try to enroll at exactly the same time. What's going to happen? The server will pick a winner. One person wins, the other person loses. Or if the server is not very well programmed, the server will send an error message to both clients saying, I can't enroll you because there's some problem at the other end, right? Or even worse, the server says, I'll let you We both in. Actually, that's pretty good from the student's point of view. But if we have a certain classroom capacity, that's not going to work, right? So there's new ways to screw up in client-server applications that weren't really all that possible in single sort of computer applications. Lots of new ways to go wrong. And you will get to explore a lot of them in this class. It's going to be so much fun. Any other comments on the big picture?

and what can go wrong? All right. So let's take a look at sort of a sample way of doing that. And, you know, I should warn you. Let's do a little warning down here before we get too much further. Here's the classic speedometer that's in my car. I still have one of these analog things.

says we're going faster than before. Okay? Until now, I've been going through things in a lot of detail. I'm telling you every little place where all the square brackets go in regular expressions and all that sort of thing. Okay, now I figure you know how to learn that stuff. And so I'm going to start to go faster. I'm not going to give you every little detail about JavaScript because you already know how to learn this stuff. Also, I'll make a I'm not a JavaScript expert. Not like I am an expert in some of the other stuff in this class. I haven't written, you know, 10,000 lines of JavaScript code. So I'm giving you some of this stuff just by reputation or inference. I mean, I've done a little bit, enough to do the class, but I'm not really the expert. That's another reason I'm going fast, because you'll be learning not from me, but from each other, from the TAs, from the LAs, from the internet and all that sort of stuff and then at the end I'll get to test you on all this. It's going to be a lot of fun. All right? So I hope you don't mind the little confession. You know, some faculty members never want to tell you that they don't know something. They'll do a little song and dance and hope you don't notice. I'm very happy to tell you when I don't know stuff. All right. So where were we? So what is Node? It's often called Node. It's often called Node and it's often called node.js because that's the name of the domain name. You can sort of go off and visit that website and found all about this technology. So it is a software technology and to some extent at its heart it's the following. It's a JavaScript runtime for handling sort of asynchronous events.

It's a lot of other stuff, but this is what it is at its heart. And essentially what is going on here is it's a different way of writing software than what you're used to. You're used to writing programs with for loops and function calls and all that sort of thing. And in that style, which is the So in the traditional style, you're writing a recipe for what should happen and it always gets executed in the order that you tell the computer. You're in charge when you write the program of exactly what will happen next. Okay? What Node is about is a different style of program, not taught in CS31, dealing with asynchronous events. Asynchronous events means your program's running and it's doing It's running in a larger environment and in that larger environment things happen. And those things come up against your program and they pound on the door and say it's time to stop doing what you're doing and do something else because I've got this important thing to handle. Classic example is a web server. Web server is constantly getting requests from the outside world. Some of those requests will take a long time to handle. While it's handling one request another request will come in. Now the web server has two requests to deal with. Should it finish off the first one before it starts the second one? No! That would be a disaster. That means when one person asks a tough question the web server is stuck until that question is answered. We don't want your sort of node applications to behave like a CS31 program. We don't want that disaster.

Instead, the web server should be able to say, "Oh, I've got this long running thing going. I'll do this other thing." Right? "Oh, and here's the third thing I'll do." And sort of juggle a lot of things to do based on requests that come in sort of out of the blue. Or sometimes the request won't come out of the blue. It'll be something like the following. An end user asks something of the web server.

The web server has to make a database request to a database server, some other server. And then it has to wait for that answer to come back. So these asynchronous events can be externally generated or they can result from other events that have already happened. And we want to be able to handle all of that and that's basically what's going on. If you try to write this kind of runtime down in a sort of CS31 You'll see that what this runtime does at the top level is a very simple loop. It'll look like this. While true, do the following. Event e equals getNextEvent. And here, I'm assuming that this is a sort of a built-in function.

that waits for the next event to occur that comes in from the outside world or maybe from inside from some earlier thing that you've done. If there's nothing going on, it hangs. If the web server is totally idle, nothing's going on there. But eventually you get an event. And then we handle that event. And loop around and get the next one.

But there's a key sort of important part of this loop. And that key important part is whatever this stuff does, this event handler here, it must finish quickly. And by quickly, okay, I'll do a little It depends on the application on how fast that can be. But the point is, it should not take milliseconds. That's too long. It certainly should not take seconds. That's ridiculous. You want this event handler to just take, you know, a few million instructions, nothing big, right? It has to run fast. In particular, something the event handler cannot do.

is something like this. Look inside this event handler. If you see this code, that's probably wrong. Why? Because when you read from a file or read from a network connection or something like that, the read will wait until the data's there. That wait is going to take too long. You know, this could take seconds. That's ridiculous.

You are not allowed to do any reads or writes. Don't do that either because the write might take a while. You can't do anything like that in your event handler because if you do, what's going to happen? This top level code will be stuck handling one event while other events arrive. And those other events will, say, pile up in your input queue or something like that.

and users will start complaining. This web server is a disaster. I sent it a request and it doesn't give me an answer for like half a second. It's really slow. See how this works? Okay, so there's a big constraint on what you can put in here. Don't put anything that takes a long time. Now, suppose you're writing a web server that's in charge of some, I don't know, machine learning training application. So your web server takes requests from the outside world, uses it to train some big ML model, and that training session is going to take two hours. Very common application these days. Some web training sessions last a lot longer than two hours. I was just in a talk earlier this afternoon where they estimated the cost of building one machine learning model was 70 million dollars worth of CPU time. Right? Well, obviously this event can't be something like that. Right? If it involves a long computation, the way you fit it into this model is you break that long computation into smaller pieces.

is small enough to fit in your time limit. And at the end of its quick action, maybe it'll just train one little part of the model, it says, "Oh, okay, generate this other event for something else to do." Right? So instead of writing this program that has this loop that looks like this, which you can't write over here if n is at all large, Instead, you just do something like this.

Or more likely, it'll look something like this, right? F of n, and this will be n plus 1.

And this is what your event handler will do. It will just call a single relatively fast function to get its work done.

and it'll say, "Okay, let's generate the next event." So this kind of application means you have to split up your program into digestible pieces, right? Where by "justible" I mean each little piece runs fast and then you glue the pieces together by scheduling events. And what that means is that at the lowest level you can still write for loops as long as they're really small and fast, that's fine.

any big loop or any big recursion, don't do that. Instead, split up your program in a different way. Okay. Any questions about this kind of programming? It's a different way of building applications. All right? And, you know, you might see the phrase event-driven. Event-driven applications. Event-driven applications are by some measures the most common forms of programs on the planet because they're extensively used in the Internet of Things. If you look at the program inside the thermostat in your home or the braking system in your car, it's probably event-driven. Comments? Yes?

Oh, yeah, as long as allocating the resource is something fast, like all you're doing is calling new or ML, you know, just calling a constructor and that'll finish quickly. Sure, you can allocate a resource like that. But if allocating a resource is going to take a long time, then you better not do it in the Invent Handler. You better do it some other way. Typically what you'll do is you'll take this resource allocation action and split it up into to smaller pieces. Each one can run fast. There may be some delay between one phase and the next, but that's what events are for. Yes? Yeah, this is like the short order cook that's, you know, cooking eggs for one customer while cooking pancakes for the other while cooking toast for the third person, right? And they're constantly sort of, they're interrupt driven.

Also they tend to be kind of busy people. They're kind of hard to talk to. Their brains sort of get scrambled after a while. Has anybody here ever worked as a short order cook? Different world. Well it's a different world entirely. But anyhow I'm sure some of you have been event driven in your lives. It's a different way of thinking and to some extent if you've worked in one of these event driven sort of environments you'll have a better understanding for this kind of You always have to be prepared to drop what you're doing and do something else completely. Other comments about this model? Remember what I suggested in the first round of questions, right? Whenever the professor shows you something like this, you should think, "I'm in the School of Engineering. I should worry about how it breaks." What can go wrong with this?

Any thoughts? Yes. Yes. Right. You'll have more overhead this way than you will with the typical CS31 way. Absolutely. Because you have the overhead of generating event and, you know, get next event. These functions are probably going to be pretty fast.

because of the heart of the event handling routine, but they're still, you know, a lot slower than just execute the next instruction, right? So there's going to be some performance overhead here. That's very true compared to sort of doing things in the standard C or Fortran way. Yes. Right.

You have to deal with errors. Boy, that was scary. You have to deal with errors in an event handling program. But to be fair, even when you're writing a conventional program, you have to deal with errors there. Right? So if the way you deal with errors is in sort of a very conventional way, you call a function, it returns a value, it says an error returned, then you figure out what to do next. You can just take that code and sort of put it in your event handlers, right? And if an event fails, it can store that failure somewhere. So the next event will know that, hey, the earlier guy failed, all that sort of thing. So it's doable, right? But, you know, it's still going to be a pain. If you're used to dealing with errors via exception handling, things get hairier here, right? Because you can't have one event throw an exception that the next event will catch.

It doesn't work that way at all. So you're going to have to sort of deal with errors in a more conventional way than with a try-catch sort of thing if you're used to doing things that way. I like people thinking about errors because in practice, in software construction, at least the kind of code I write, more than half the time I'm worried about error handling, when things will go wrong, right? Maybe it's the engineer in me or maybe it's just because I like to write reliable programs I'm not I'm not sure. Can other things go wrong with this? Yes? Can things happen out of order? Oh, yes. Right? You expect the events to arrive in order A, B, C, but the outside world is in charge of them, and it ships them to you in B, C, A. Now, what do you do? In some sense, that's more opportunities for errors, and you're going to have to check for that. Good point.

All right. Well, so to some extent, you know, this is what JavaScript is doing, right? You know, it seems like in some sense it's a mistake, right? It's slower. It makes you sort of think about code in a weird way and all that sort of thing. Is there any advantage to doing things in this strange way? And there is one important advantage that I should tell you about, right?

And that advantage is the following. There are no locks on resources. What do I mean by that? Ordinarily, if you want to write a web server that can handle lots of requests simultaneously, a standard way to do it is to write multi-threaded code. A multi-threaded application is one that you are-- that same application is using multiple cores on your machine. Each of those cores has its own instruction pointer, so each of those cores is running some part of your program and all the parts of your program are collaborating by getting their own work done and communicating to each other via shared memory that has locks. These locks prevent races in multithreaded The reason you need the locks is because if two different threads, maybe corresponding to two different web requests, try to enroll somebody in the class at the same time, they have to make sure that they don't step on each other's toes. And the way they do that is each attempts to lock the resource representing the class. The one that wins and gets the lock will then be able to enroll that person. Finally, when they give up the lock, the next sort of thread will then get access to the class, find out that the class is fully enrolled and give up, right? So you use locking to prevent race conditions. Race conditions occur when two different threads access the same thing at the same time. These races are perhaps the most common error in multi-threaded programs. They're very hard to You can have the same program run successfully a thousand times in a row because it has race condition bugs, you just didn't trigger them, and then the thousand and first time it doesn't work. We don't have to worry about that here. Reason we don't have to worry about it is because when this event handler fires, nobody else is running. This is a single threaded while loop.

So in some sense what an event driven application gives you is sort of something that acts like multi-threading. You can juggle several different web requests sort of in the air at the same time. But whenever you're actually doing something you know none of the other web requests is running and so you don't need to grab locks on resources. Grabbing locks first off takes some time and second reduces the amount of parallelism so you don't get the benefit of the multiple cores, the locks are just a disaster, right?

And you're avoiding that disaster here. You should thank your lucky stars you're avoiding it. And later courses in this department, you'll be doing multi-threaded applications and get to deal with all that problems, but at least for this assignment, you don't have to worry about it. All right. Can anybody see another problem with this approach?

There's got to be a catch besides the ones we've mentioned. In fact, I've already hinted at one of the major catches to having an event-driven program. Right? Here's a plus, but it comes with a minus. There's a big advantage of multi-threaded applications over event-driven.

And that's the reason why people write all these multi-threaded applications. Right? If you look inside, I don't know, Chrome. Chrome is heavily multi-threaded. Why? Yes. Performance. Right? The advantage of a multi-threaded application is you get true parallelism. You have multiple cores in your laptop. You've got an eight-core laptop. If you're lucky, all eight cores are executing actual code that gets useful work done. It runs eight times as fast.

So a downside of this approach is that you don't get any parallelism. This is how I write parallelism, two parallel lines and then ism. You get sort of a fake parallelism where there's just a single sort of thread of control, a single CPU, a single core, whatever you want to call it, and it's jumping around handling lots of requests, but of course it's limited by how fast it can run.

The reason this sort of thing can work with Node though is that for many applications CPU is not a problem. You've got CPU to burn. Your web server's CPUs are mostly just sitting around not doing anything. They're twiddling their thumbs. Your browser is kind of the same way. If you take a look at how often all eight cores of your laptop are actually running, it's a vanishingly small percentage of the time the laptop's operating.

So for many applications, the fact that you don't get true parallelism is no big deal. And in fact, that's certainly going to be true for assignment three. Can anything else go wrong with this style of approach? Yes? Yes. Right? If one of your event handlers crashes, doesn't work, or worse, loops, just goes into a loop. Everybody's stuck. Right? So that's a downside of this approach. This approach is suitable if your application is sort of one big happy application written by people who trust each other and they're all competent and everything works. Right? But if you assume that some of the software is written by people who have buggy software or worse, they're adversaries, they're trying to make the system crash, then this approach obviously is totally bogus, right? Because any one of these event handlers can bring down the entire system. Oh, all right. Other comments about the approach? All right, let's see if I have any of my own. Okay. Now, you know, this idea I should mention is it's not just node.

There's sort of lots of other systems that do something like this. For example, Ruby, a system we won't have time to examine at all in this course, has something called the event machine. The event machine is a way of building while loops that look like this and you can deploy them in several different places in your Ruby application.

Python has lots of different approaches that work like this. One classic approach is the twisted application that runs under Python, the twisted module. It's popular enough that Python now has its own asyncIO sort of package built into Python and basically asyncIO lets you do stuff like this. It's called asyncIO because you don't ever do reads or writes. Instead, you just have something that starts a read. Right? And then when the read request is done, you get an event. You can start a write. Starting a write immediately finishes. Doesn't take very long time. And then when the write finishes, you get another event. And that's basically what async.io is for. And once you have async.io, you can do sort of event-driven programming in Python. Question?

All right, so ordinarily, if I'm talking like read and write in, you know, C++ or C something like that, these are synchronous. You, you know, the actual system call will look something like this, right? Read zero buff a thousand, right? Where this is an array.

And this says, please read up to a thousand bytes into this buffer. Once you've read them, then return and tell me how many bytes you actually read successfully. And if n is negative, that means there was a read error and all that sort of thing. But the point is, this call to read doesn't return until those bytes are sitting there in the buffer. And it has to wait until then, because immediately after this, we might want to print out one of those bytes.

Right?

So this call to read is synchronous. It waits till the read is done. And that's the classic way people do I/O. When you put in print statements in your C program, when the print returns, you expect that stuff to be printed, right? Similarly with C++. So that's not what we want with async I/O, and it's not what we want in node. We want the ability to say let's just do IO but we'll just start it and when it finishes we'll get notified by an event. Other comments about this technology? All right well you can build anything with Node, any kind of application that you like. You can build whatever you like but they built a web server.

You can also build a browser.

And you can build other stuff.

But basically in this class you will use a web server.

And you know I should mention by the way node is written in JavaScript.

And if you know anything about JavaScript, I will talk more later in this course about it, but basically JavaScript is designed for browsers, right? It was originally designed for browsers back, you know, in the late 1990s. So what's going on here? What they've done is they've taken this technology designed for browsers and say, let's just run this JavaScript code in a different One that doesn't have a display, doesn't have a mouse, doesn't have a keyboard, doesn't have anything like that. It's just JavaScript running in its own process on a Linux server somewhere. And JavaScript can do that. It's just a programming language. And so they wrote Node, a Node web server in Node, you know, not in the browser, right? It's running in some random process.

For example, it's possible to imagine a new version of Emacs, I'll call it JS Emacs. It works just like Emacs except its extension language is JavaScript, not Lisp. Simple software engineering exercise. I could assign it as a homework problem for this class. Well, alright, for the next class, right? Right, well in a sense that's what they've done. They built a web server that way, right?

except you get rid of the display and you get rid of the keyboard and all that sort of thing. Okay? And there's lots of libraries available. For this web server. There's a whole ecosystem of running node in web servers and running JavaScript code there. Now, if you're building a client server application, what that means is if you have an idea and you can write it down in JavaScript. You can implement it in JavaScript. Now you have a choice. You can either have your JavaScript code run in the browser, if that's an appropriate place for it to run, or you can have it run in the web server, if it's a better place to put it as the web server. Or maybe you'll decide, depending on the application, am I going to run the code here or there, right? It makes your code more portable, makes your idea is more flexible. You can decide on an application per application basis whether to run some stuff on the client or on the server. And that's one of the big advantages of Node as opposed to lots of other ways that you can build web servers. So let's take an example of this. If I go and there's a Node in here somewhere. There we go, Nodeish. I have a directory called Nodeish. And in that directory, I have a file called app.js. And if we look at that file, here's some code. This is JavaScript code and it's designed to run in the node environment. So since I haven't yet given you a single line of JavaScript code, I feel it's only fair to write a little bit and put it on the screen and explain it. Remember, I'm not a JavaScript expert, all right? So if you have any detailed questions about this, I'll probably say, oh yeah, that's an interesting question.

First line says I'd like to use the HTTP module. So let's require that and we'll call it HTTP. Now I could call that module anything I like. I could call it X or I could call it foobar. But it's very typical that you'll take a module that's named X and then give it that same name in your program. I should mention here that JavaScript Unlike Python and unlike C++ doesn't really believe in classes. It has some sort of object oriented attitude but it doesn't have the same sort of thing, classes that sort of Python does. So, you know, a module is just another object and I'm calling it HTTP and later on I can, you know, assign HTTP to something else. Second line, I'm assigning a local variable called IP, well a global variable I should say, and it's the string 127.0.0.1. That's an IP address. By convention, that particular IP address is the address of the machine that we're currently running on. So this particular web server is going to be very much just a toy web server. It's not going to run on the internet and serve anybody. Instead, it's only going to run on my laptop and any client that wants to talk to this web server also has to be running on my laptop. In some sense we're missing half the point of a client server application but that's okay it's just a tour. The third constant is an integer. It's the number 3000 and we're assigning that to the to the variable name port well the constant port I should say. Now JavaScript has strings as you can see in line two and it also has numbers as you I should warn you that its attitude towards numbers differs from most other programming languages. In Python, there's the integer 3000 and there's the floating point number 3000.0. And they're quite different things, right? They're represented differently internally. One, if you add to it, you'll get rounding errors and all that sort of thing. JavaScript tries to keep things simple by saying, no, no, no, no, we just have numbers.

numbers. And this 3000 is just the same thing as 3000.0. There's no difference between the two. It'll print it out as 3000 because it happens to be an integer, but it's, you know, there's really no difference between integers and floating point numbers. And I'm telling you this now because you'll run into it eventually. It's not really important for this application. All right. Line four. Well, we finally are using that HTTP constant that we set up in line one.

That constant has a method, create server, associated with it. And we're going to call create server with a single argument. And that single argument is an unnamed function. In JavaScript these things are often called callbacks. So the way you write a function without any name in JavaScript is you do something that looks like this.

This is a function with three arguments a, b, and c. If you call it, it will execute all the code inside the curly braces. And we can take this function and give it a name f if we want to, but we don't have to. We could immediately just take this function, not give it a name, and, you know, pass it to some other function. That's what we're doing here. We're just calling HTTP create server and saying oh here's the call back that I want you to execute when you get a web request right and what create server is going to do is you know eventually it's going to set up an event loop like this inside the server that we're creating and somewhere deep inside that event loop it will say well if I get a web request as opposed to all the other kind of events that could happen to me. The way I'm going to handle it is by calling this callback. The parameters to the callback are two objects. The first is the request object, which create server will arrange to be an object that tells us all about the request that came in. The second argument is the response object, which create server will arrange when we call this call, when it calls this callback, it will arrange for the response object to be some sort of random vanilla response. And our goal is going to be, we fill in the details of that response so that the server will know what to ship back to the original client. Alright, so our particular web server is really simple. No matter what the request is, if the request is, please make a cup of coffee, if the request is, please enroll me in this class, if the request is, tell me what time it is, we ignore it.

and we just issue the same response. Doesn't matter what the request is. What's our response? Our response has three components. The first is, is this a successful response or a failed response? If it's a failed response, what kind of failure is it? By convention, status code 200 means I succeeded. There's a whole list of status codes. They're all documented somewhere, but for now, we'll just remember 200 means it worked. Anybody else know any response codes off the top of their head? If you're a web developer, you know all these numbers. Yes? 404 is one of my favorite. That means it didn't exist. Yes? Yes, exactly. There's all sorts of numbers here, but 200 means it worked. Header. Set header says take the header that was the vanilla header that was in the original response and change it so that that it has the following line somewhere in the header. The line will look like this. Content type with a capital T. Text Plane. When you ask a web server a question, it gives you an answer. But at the start of the answer, it gives you meta information about the answer.

to be part of that meta information. Our meta information says the response I'm about to give you is plain text. It's not anything fancy like HTML or XML or JSON or anything like that. It's just plain text. Okay? So that's what that set header line does. The third line, response dot end, basically specifies the contents of the response, the end of the response. We'll see later why it's called the end.

So what this callback does is it fills in the response with a minimal set of values enough so that this will actually be a web server that will actually run and then it returns. At that point it's up to the web server that creates server created to figure out what to do with the response. What it will do is convert this information into a bunch of bytes that it will ship back out to the original client that asked the question.

Question. Like request and response by convention, would you change the names? Oh yeah, the name request and response, they're just local variables like A, B, and C over there. I could call them Q and R as long as I changed all instances of response to R, it would still work. It's just local variables to that callback. Good question. Other comments, yes?

It should be some value other than 200 because it didn't work?

It should be below response Oh, but all we're doing in this callback is we are storing into an object.

In JavaScript, objects are pretty simple.

You can think of an object as a collection of name value pairs.

We are setting three components of that object to particular values.

And, you know, in some sense, it can't fail. Well, until we call end. End might screw up because you ran out of memory or something. But those first two guys, they can't fail at all, right? And I don't know, if end runs out of memory, it's probably going to crash the whole server. So to some extent, it doesn't really matter whether we call set status code first or set it after set header. We're just storing slots in the response. All right.

Yeah, it's an object-oriented system just like Python or the object-oriented part of C++. Every object is really represented by a pointer to the actual value. Other comments? All right, so that callback will set up the response object, but it won't actually do anything, right?

The code that actually does something is buried inside the web server that HTTP create server created. All we're doing in our callback is kind of fiddling with how that web server behaves at just the right moment so it'll behave the way we like. Now, this last three lines are where we actually do something with our server. Notice we had a line called const server equals that sets a constant called server to be this object.

and we're going to call the listen method. What that does is it tells the server, "Hey, it's time to wake up and listen to the outside world." The place you're going to listen is specified by the first two arguments. The port is the TCP port. We'll talk more about that later. The IP address is the IP address you're listening on. You're going to be a web server that's listening to requests coming in to IP address 127.0.0.0.0.0.0.0.0.0.0.0.0.0 on port 3000. Okay? And notice that the last argument to the listen method is another callback. When I write something that looks like this, I'm writing a nameless function that takes no arguments, and when you call that function, it executes the code in the curly braces, right? So we are not actually executing this console.line.

log line now, we're just stuffing that into a callback. That callback will be executed whenever the server starts up. Okay? So two different callbacks. The second callback is going to be executed when the server starts. The first callback is going to be executed every time the server gets a web request. All right. Well, let's get started.

Let's give it a start except let's take a break and we'll start up at five past the hour.

All right, let's start up again. In the break it was pointed out, I forgot to mention the second line from the end uses some new kind of technology that we haven't seen before. And this string that's in that second line from the last is not surrounded by It's surrounded by "grave" accents, back quotes, right? This symbol instead of this symbol. And that's a special sort of syntax in JavaScript that says this string is not just the characters that you see, but if you see dollar sign, open curly brace, name, close curly brace somewhere in that string, please substitute the value of that variable for all the stuff in dollar signs. So this is equivalent to saying server running at HTTP colon slash slash 127.0.0.1 colon 3000 slash, right? It's just that it's documenting itself somewhat nicer than if we just wrote out the same constant twice. So we should be able to start this up.

by running something like this. Assuming that you have Node installed on your computer. And what Node has done is it has set up the server, right? It's actually just executed this JavaScript code. The first thing the JavaScript code did is it executed that line that said, hey, I'm running. And here I am. And we can connect to that particular server by just using a browser and specifying that particular IP address which localhost is just a synonym for and talking to it and in kind of small print here you've seen the response right we got a very simple response that said whatever that event handler was told to say if at this point we type We can see information. This is meta information about the system and notice it says type:text/plane. That's because the browser saw that line in the header of the response and said, "Oh, I know this is plain text rather than being HTML." All right, so any questions? We now have a client server application.

one, but every time we ask it something, it'll always tell us the same answer. But as you can imagine, you can write something more interesting than this if you put something more interesting in the node code. All right, any questions about sort of starting this up? All right, so I've done the demo. Now I can go back and talk about more general principles about building these kinds of applications, right?

We have sort of the client-server basic model. Which I've already mentioned. But I feel like I should also talk about alternatives. With client-server, you have one or possibly lots of clients talking via the network.

to the server. The server has the state and in some sense is in charge of the application. At least it's if there's any information that needs to be shared amongst various clients, the server's got it. The clients can only communicate to each other indirectly via the server. But there are alternatives to this, including peer-to-peer. In a peer-to-peer application, you have a whole bunch of peers connected to the network. Each peer has its own state, its own idea about the state of the system. And in some sense this is nicer because if a peer wants to know something like who's enrolled, it can just ask its own copy of the state. Say, "Who's enrolled in this class?" And the copy will contain that information.

But, as you can imagine, this kind of system is going to be more complicated because now you have to deal with the possibility that this peer has enrolled somebody in the class. These peers don't know about it yet. And this peer has to arrange to propagate the information in its copy of the state into the other copies. Generally speaking, peer-to-peer systems are even more complicated than that. Some peers will have more of the state than others. Maybe no single peer has a copy of the entire state of the system. They have to collectively agree upon, you know, who's going to be in charge of which part of the state and so forth and so on. So this is going to be a more complicated approach. However, it has some advantages over client server. Client server approaches have scaling problems. Once you have a thousand or a million clients, this server is going to get overloaded. So they won't scale very well. A few hundred clients, as long as the server is well-written and it's not doing all that much, you're fine. But once you talk about zillions of clients, watch out. A peer-to-peer system, if done right, it's going to be more complicated and maybe it's going to be less efficient with small numbers of peers, but it should scale better if it's written well. Another approach, is what I'll call a primary secondary approach. I guess I should write it over here. There's no sort of generally accepted term for this. To some extent, I'm just making up a term, but that's all right. Somebody has to do it. Under this approach, there is a primary sort of machine attached to the network that's in charge of your computation, and you have a bunch of secondary machines.

Each doing a little bit of the computation.

So you can think of the secondary machines as being worker bees.

Each one of them in charge of doing a particular part of your machine learning or a particular part of your database system or whatever.

The job of the primary is to schedule the work for the worker bees.

Right? It knows what each of these secondary machines is doing.

It keeps track of all the tasks that need to be done. When a secondary machine says, "Okay, I'm done, boss." The primary says, "Oh, here's another thing for you to do." This kind of application or architecture, I should say, is suitable for a lot of big sort of things. Machine learning applications are often done this way. I've given you three examples of distributed architectures. As you can imagine, there's lots more where this came But the reason I wanted to show you at least two alternatives of client server is I didn't want you to think that everything that's done in Node or in similar technologies is always going to be client server. All of these models, client server, peer-to-peer, primary, secondary, have problems. Okay? And too many problems for us to talk about all of of them. But let's talk about some of the common problems in distributed applications. And I'm not going to talk about all of them, but I'm going to talk about two major categories of problems. The first problem is a performance problem.

to run fast. And as I kind of suggested, if you scale right, you can actually get really good performance out of these because you're doing a lot of the work in parallel. But if you're doing a relatively simple client server application or even some of these other applications where there aren't that many machines, the parallelism isn't all that great and you start to run into some other problems that you don't run into in CS31.

These performance problems can be divided sort of into two major categories. Throughput problems and latency problems. Throughput is how much work can you shove through the system per unit of time. You've got say a client server system. How many requests per second total can it take from clients?

before it starts to get overloaded. The more requests per second it can handle, the happier the person who bought the server will be because the server is, you know, they spent, you know, $10,000 on the server and it can handle 100,000 users. If you can raise that number to 110,000 users, they'll be happier, right? 110,000 requests per second, that's actually doing pretty well, right? So here, what we're trying to do is sort of do as many things in sort of quasi-parallel as possible. Do sort of actions in sort of parallel. Or another option that you can do that's very common, particularly in node-based applications, is you can do them out of order. When might that happen?

web requests A, B, and C. And it turns out that web requests A and C both talk about CS35L. And web request B talks about a different course, DANCE100 say. What the server may end up doing is it will handle A and then it will handle C because it has all that information about CS35L in RAM, easily available.

out of order in order to improve performance. Very common technique. And it sort of often falls out sort of naturally in an event-based web server. Because once things are sort of cached in RAM, those events happen faster. And so you handle the events as they come in. You will tend to handle events out of order from the way they came in from the outside world.

and even then sometimes things will come back in a different order than they were originally done. So this is going to be a common sort of performance trick that you use to make things go faster. Second sort of issue is latency. Throughput is the number of actions per second that your server can do or your client can do. Latency is how long the client waits.

between the time it does a request and the time it does a response.

Right? So here the idea is you want something to pop up on the screen, on the user's screen, as quickly as possible. Right? This is a different performance measurement from throughput.

But it's very important obviously to individual users.

A common trick to improve your latency is to cache on the client or sometimes on the server. But for now, let's focus on the former. And the idea here is pretty simple. If the client wants to ask a question of the server, but it already asked that question or a similar question a few minutes ago, just reuse the answer that you already got.

Just keep a cache of answers to recent questions. Keep that cache in RAM, say, on the client, or maybe on a flash drive on the client somewhere. Consult the cache instead of waiting for the request to get all the way to the server. It might be in Tokyo. It's going to take, gosh, I don't know, 250 milliseconds to get the thing there and back. Speed of light, whatever, right? That's forever. That's a quarter of a who wants to wait that long? I don't. You don't either. Right? So cache the answer on your browser and don't talk to that web server in Tokyo. These are two very common performance sort of tricks and I hope that you are putting the engineering hats on your head and saying what can go wrong with these performance tricks and that brings up a second class of common problems in dealing with these sorts of applications. And those are going to be correctness problems. These are performance issues and these are correctness issues. These correctness issues are in some sense corollaries of the common performance hacks that everybody uses to get their client server apps to go fast.

So, for example, if you do stuff sort of out of order, that might confuse the users, right? This sort of out of order execution, the users might say, "Hey, wait a second. I sent in my request to enroll before that other guy down the hall sent in their request. How come they got in, I didn't. Order often matters. Or I sent in a deposit of a thousand bucks into my checking account and then I did a withdrawal of 500. How come you did those actions out of order and charged me 30 bucks for a fee, right? For an overdraft fee, right? So order often matters and how do we sort of address that that issue when we have sort of played this trick but maybe played it too far or too hard. The standard technique for doing this, I won't talk about it that much in this class but you'll get a lot of this later in other courses in this department, is called serialization. And the basic idea of serialization is as follows.

The server got a whole bunch of actions to do, right? Action one, action two, action three, action four, and it executed them in say this order. And in some sense by sort of overlapping this I'm saying well it did some stuff in parallel, okay? But the way the server can explain what it did any user who cares about the result is it can say, "Well, you know, this has exactly the same effect as if I did the actions in this order: a1, a4, a2, or maybe even a3, a2. Doesn't really matter." Right? Do this first, this second, this third, and this fourth. If I had done the actions in this order, then my resulting state would look like this and Let's call that state ST. And that's exactly the same state that I actually computed here even though I did the actions out of order or in parallel or whatever. As long as the server can justify its resulting state and all the behaviors that people observe as if it did things in a serial order, then that's good enough. Any questions about the idea of serialization? In some sense, Serialization is a little strange concept because it's a way of justifying your behavior. It's almost like you broke the rules but you later explain to the judge here's what could have happened and if the judge can't observe anything wrong the judge will say okay you're off the hook. It's cheating in some sense but not really in another as long as the serial order said well you and then you deposited it and so we gave you a lateness penalty then that's good enough. All right so that's the correctness issue or the common way of addressing the correctness issue there are other ways. What about this problem with latency? With latency the common way you address that is with caching and the problem with caching is I'm sure you can imagine is those caches will often be wrong. So how do we address that? One option is to make sure that whenever you consult a cache on the client, that you make sure it's actually correct.

And by correct I mean up to date. That's one option. So the idea here is something like the following. Every state on the server, say, has a unique ID. Some big long string that uniquely identifies the state. We store that unique ID on the client along with a cache. Every time we consult the cache, we ask the server, "Hey, is the unique If so, we say, oh, our cache is wrong, now we got to go updated and all that sort of thing. But oftentimes the answer will say, oh, nothing's changed since last time. If we take that approach, we will save a lot of work because we can cache a whole bunch of stuff on the client. Oftentimes it's up to date, so we just need a quick little message to the server to say, are we up to date yet? The server says, yes, okay, fine. We can now use this big cache instead of copying a whole bunch of data from the server.

So that's one option. You make sure your cache is up to date and whenever you have to worry about that you go check that it still is up to date and do that as quickly as you can. Right? Another issue, well to some extent you can say sort of this is cache validation. How do you make sure that the cache is always going to be up to date?

is to change your computation, change the program that's running on the client, so that it still works well enough even if the cache is a little bit out of date. Right? So that's another approach. Work even with stale caches.

Some applications can indeed do that. A classic example is a video game application. You're playing some video game, there's another player in the game, you have a cache of what the game's shared state looks like. Your cache is going to be a little bit obsolete by, you know, three milliseconds. But you're not a super duper player, you can deal with a cache that's out of date. How many people here have a reaction time better than three milliseconds, right? Nobody, right? So it's okay. Oh, somebody raises their hand. There's always somebody, right? So in some applications, it's okay to have caches that are somewhat stale. Another example is a weather application. It's telling you what the weather will be like for the rest of the afternoon. So it's 10 minutes out of date. Unless you're in western Kansas or something, 10 minutes out of date is fine.

So this is going to be a correctness issue. You have to decide for your application whether your caches can be stale or not. That's a big deal. If they can't be stale, you have to decide how your cache validation is going to work. Because this cache validation implies, of course, latency. And latency is kind of the reason you avoided the cache in the first place.

All right, so any comments on these two major issues? Yes? Yes, it does, right? The goal of this approach is you want this to be a lot faster than actually just refetching the whole cache, right? So if your cache has a gigabytes worth of data and cache validation just has to ship it, you know 120 bytes or something then it can still be a win. Right, exactly. You still have the speed of light delay between you and the server and that's a downside of this approach. Other comments on these two major problems? Yes? Well, it's sort of cheating. It doesn't really prevent the out of order issue in the same that requests can be handled out of order with the serialization approach. But what it does do is it says, if we pretended that, say, student X's request got routed to Outer Mongolia before they actually got to Murphy Hall, and student Y's request got sent straight to Murphy Hall, then the fact that Y got enrolled and X didn't is okay, right?

excuse, right? What we want to--still, I mean, the point of serialization, though, is that the resulting computation still has to be valid, right? You can't look at the resulting state, for example, and find more than 160 students enrolled in this class because the limit on this class enrollment is 160, right? So you still follow the rules of the application. It's just that you might not have followed it in the order that people expected. Yes?

Yes.

What happens if you do the order and then it doesn't work?

Oh, yeah. I mean, serialization is not a solution to all the bugs in your application. If your application is buggy, it'll still be buggy. In some sense, serialization is the way that you prove your application is not buggy. You come up with an explanation for what it did, and if that explanation matches the behavior that was observed, you're okay. If you can't come up with the explanation, then your application was buggy. But it's not just saying the magic word serialization sort of insulates you from all possible problems. That's not how it works at all. Okay, question? Justify which example? Oh, well, you will say that, you know, You know here's ATM X where you deposited money and here's ATM Y where you withdrew money. You just say well ATM Y you know had a faster network even when it didn't really. But the user can't tell so you're off the hook. Right? So if you have just a single client say a single ATM and you deposit and then withdraw then you're not allowed to serialize that out of order. Right?

the point of view of the entire application, but from an individual client, it can still expect its actions to be done in a particular order if it issued them in the order and got the response back, that sort of thing. But we'll see later, though, that there are cases where the client can do request one and then request two, but request two gets done first. That's allowed, and we'll see that later. But you wouldn't do that sort of thing in an ATM sort of application. The bank users would get very mad at it.

Okay, other comments? Yes, in the back.

I have a question about the different models.

Uh-huh.

In terms of, you were mentioning that you think the client server model is up to scale.

Is there a difference where you actually just want to change your model?

So the idea is you maybe started off with client server, and then all of a sudden you got deluged with a million users, and now you've got to switch, that sort of thing?

Yeah, just sort of the situation.

Yeah, that is a situation that can happen and you kind of should have a plan, right? And there are ways to attack this problem that's sort of beyond the scope of this lecture. But let's put it this way. I like to call that a problem of success, right? A problem of success is when you all of a sudden discover that you're really popular and there's a lot of people that want to use your stuff and you're making lots of money.

Take that money and invest it in a better system. You want to have problems of success. That's one way of looking at it. Let's go back in time next. I'm doing this lecture in sort of reverse order. We did node, now we do the general thing. I want to go back in time. Question.

like what? Oh, yeah. Well, you can cache cookies, definitely. And in fact, you know, most people, you know, most applications will cache cookies, absolutely. But you don't have to. In some sense, they're orthogonal. It's just that they're closely associated, absolutely. I normally, by the way, throw away all my cookies at the end of every web session because I'm security oriented. I don't like to have those cookies floating around. So I clear the cache all the time with MyCookies, but not everybody's as paranoid as I am. All right, other comments? All right, well, let's go back before the internet. So what we're doing now is we're going way down the stack. We started off at the node level and then went down to the JavaScript level and then went down to the client server level and all that sort of thing. Now I'm going to go down practically to the to see how this stuff works really at the lowest level. Because the way the problems sort of bite you at the lowest level will turn around and sometimes bite you at these higher levels. That's why we're doing this. So before the internet, the way that we hooked together computers and other automated devices was with a technology called circuit switching.

It's a technology that's still used in some places, although other techniques have taken over. And here's how it works. Here we are in, I don't know, Engineering 6, 363, my office. Here's a phone. And back then, you know, they were dial phones and all that sort of thing. Very traditional phone. Just like the icons you see at a phone. Nobody has phones like this anymore. They were connected by a pair of wires.

just a pair of copper wires twisted around each other, so it's a twisted pair, to something called a central office. This central office would, you know, have a whole bunch of phones connected to it. But what I wanted to do is I wanted to call somebody at MIT, way over here in Boston, who had a similar setup, right? Here's a dial phone, right? Connected by a twisted pair of wires to a central office over here.

The limit on this length is fairly small. You know, maybe a half mile, something like that. Can't be much longer than that, but it's just wires that go to the central office. And the reason you can't make it any longer than that is because the signals sort of decay with distance and they won't get there. And also, you know, the power and all that sort of thing. Now, if I want to call my colleague in Boston, I pick up the phone, the number that I'm dialing gets sent to the central office. There's a computer in here that figures out, oh, we want to talk to Boston. But there's no wire that goes all the way from the central office at UCLA to the central office at MIT. That's just asking too much. Electrical wires just, you know, the signals won't go that far. So instead, what the telephone system had was it had a bunch of locations.

scattered across the country. So I don't know, here's Chicago, here's Houston, over here is New York. This is actually obviously much more complicated than this. Over here is Tucson, over here is Phoenix and all that sort of thing. And what this computer in the central office would do is it would basically query the entire network of all of these systems that were connected to each other by copper wires and say, you know, I'd like to, you know, here's somebody who wants to talk over here to this, you know, central office in Boston. Oh, I guess it's over here, right? And what this set of computers would do, because there's a computer inside each box, is arrange to find what they call the circuit that connects this phone to this phone.

So it's a twisted pair of wires here that connects to a twisted pair of wires in Phoenix, which compares to a twisted pair of wires in Albuquerque, which connects over here to Kansas City, which goes up to Chicago, which goes to Cleveland, which goes to Boston, and finally we're talking. So there's a single path of basically electrical wires. It's not really one long pair of wires.

It's a bunch of shorter twisted pairs of wires connected via switches in the central office. While I'm talking to Boston, I have this entire circuit reserved. Nobody else can use it because if they could use it, then we'd hear crosstalk and all that sort of thing. And as a result, when I talk, the guy in Boston immediately hears what I talk, speed of light practically, almost as fast as the speed of light. Okay, it has to go around some corners, but it's really fast. And when they talk to me, I hear them. In fact, we can talk simultaneously and we'll each hear the other person's conversation because, you know, we have two wires rather than just one. Okay, what's wrong with this system? Why does nobody use it anymore? It's very simple, very straightforward. The U.S. telephone system The system ran this way for decades, but we don't like it anymore. Why not? Yes? It's extremely unscathed. We had millions of phones that it all worked, right? The scaling occurs as follows. Each central office only had a few hundred connections. All of these other guys had lots more connections that I'm talking about here. They had a whole bunch of these things scattered.

of people grew and the number of conversations grew. They just added boxes. You're right about scalability, but it's not the number of people. There's something else going on. Yes? The number of changes for data corruption?

Well, okay, yeah. If one of these wires, say, goes bad, this wire goes down, then my conversation with my colleague at MIT goes, whoa, all right, and, you know, the phone connection drops, and then I have to pick up the phone and call again.

So that's a problem. But you know AT&T and its competitors spent a lot of money making this system more reliable. In fact, it was more reliable than today's telephone system. I get a lot more drops in conversations now than I did back in 1960. Well, all right, 1970, right? I mean, it's just like, you know, really, this is this it really worked well in terms of a reliability point of view. Yes.

Distance is fine. You know, yes, each wire here can only be a certain distance, but you just build more boxes, right? We'll put one here in, I don't know, Columbus or something, right? So as long as you are rich, like the tele you know, AT&T was the biggest company in the U.S. for a while, you just pay for it and it works. Yes? Each location has a chance to like listen in on the conversation? Oh, yes.

So you have to trust the phone company. Under this system, there's no encryption. The only security is the fact that if the telephone company gives away your conversation, you throw them in jail. That's the only security there was. So that is an issue. But to some extent, you know, how shall I say it? I've spent this whole lecture ignoring one of the biggest problems we have in software construction, which is security. Did you notice that? Over here, I talked about throughput and latency.

I didn't mention security. Security is like one of the big gorillas. I've been skipping it. I want to continue to skip that here.

All right?

Yes? Right. So what's going on here is its efficiency problem, right? One of the problems here is I have to reserve a complete pair of lines one way or another between here and Boston.

Let's face it, even the most animated conversation I have with a professor at MIT is mostly blank space. We're thinking or one person's talking, the other person is not. That means one of those two wires is basically useless. So this is inefficient. We could, if we were clever, use the same amount of copper to build a system and that system could carry more conversations. So that's one problem. Yes? Portability. Oh yeah, nothing was mobile about this. Right? This is, you know, don't get me stuck. I mean, there were portable phones even back in the 1950s, but it's terrible technology. But let's not worry about that. Yes?

and the phone companies spent a lot of money figuring good algorithms and you know they had they had algorithms that use sort of statistical approximations of the expected phone calls in the next minute using Erlang distributions that sort of thing they were really good at that there was one other thing though that motivated people from switching away from this and it's a very sad motivation but it has to be said and the sad motivation is the possibility of nuclear war. If you take, say, Chicago and blow it up, then the airbase commander in Boston who's trying to talk to the airbase commander at LAX, their conversation will be interrupted and you won't be able to fight the war as effectively as you could if we had a better system.

And for this reason, the Department of Defense in the 1960s and 1970s said, "We need to invest in a better networking system, one that can survive wholesale destruction of nodes and still let our commanders talk to each other." The seminal work in that area was done by someone called Paul Baran, who did that at the Rand Corporation, which is short for Research and Development in Santa Monica. And what he designed in the 1960s and kept after people saying, you know solve this problem because there could be a nuclear war you know any day. We need to sort of come up with a better approach and his approach you know because Rand Corporation was full of people that were thinking about what to do in case of a nuclear war. His approach is called packet switching. It takes quite a different attitude towards how to do communications. You still at at least at the start assume the same kind of physical network. So we assume copper wires, we assume a whole network of boxes connected to each other via copper wires, but we don't do the conversations this way. We do the conversations in a way in such a way that even if Chicago vanishes our conversations still keep going. And the basic idea of packet switching is as follows: you divide your communication contents into small packets.

Each packet contains only a few bits.

How many depends on the technology and all that sort of thing.

But a typical value here is a thousand bytes, right? Eight thousand bits. The exact number depends on the technology. You don't want to make the packets too small because if you do they can't carry enough information to do anything. But also you don't want them to be too big because if they're too big they're going to sort of break other parts of the system and we'll see what they would break if we made them too big. In some sense circuit switching is packet switching in which the packets have infinite length or length equal to the length of your conversation. Well, that's not exactly true either. Anyhow, all right, so this is the basic idea. So now, if we are here in Santa, in, in, at UCLA, right, Engineering 6363, and we use our now fancy new phone, I have one on my desk, it uses packet switching, it still has a, you know, it has a touch, you know, all that sort of thing.

The conversation is split up into roughly one kibibyte pieces. And we send off each of these packets into the network. It hooks up to something that acts like a central office except we don't call them central offices. We call them routers. And the router makes, in theory, an independent decision for each packet as to how to get that packet to where it wants to go. So each packet individually says, "Where do I want to go?" "I want to go to Boston." "Oh, I want to go to Boston too." "Oh, I want to go to Boston 3." Oftentimes you'll see a string of packets that all go to the same spot, but you don't know that in advance if you're a router. You just keep getting packets over the place and whenever you get a packet, you look at where it wants to go and you say, "Probably Phoenix." Phoenix is going to make progress. Right? But it's possible that this router will start sending stuff to Phoenix, but then maybe Phoenix is flaky or maybe if it's overloaded or maybe it hasn't heard anything from Phoenix because, and it worries that Phoenix is now, you know, a pile of rubble. So it'll send packets off to Denver and so forth and so on. You still have a complicated network.

all over the country. Eventually you get to Boston where there's a phone on the other end. But each of these routers independently just gets little pieces of data. It's almost like they get like a little post-it note and the post-it says I want to go to Boston. They say maybe if I give it to Georgia, Georgia will figure out what to do with it. And if this packet switching approach works well enough, We've addressed this problem. Granted, maybe some of my conversation will have been lost because the packets happened to be in Chicago when Chicago went down. But the rest of my conversation will keep going. Kansas City will notice that Chicago no longer is this. Oh, I'll rewrite it, reroute to Memphis and so that sort of thing. Right? So anything that goes down is considered to be a problem to be routed around. Yes?

Well, one possibility is that these routers not only can send data, but there's actually a control link with a little heartbeat. And it listens to the heartbeat of the other side. And if there's no heartbeat, it says, must be dead. There are other approaches, but that's one. Yes?

Oh, they don't.

They won't get there in the right order. Boston can get these packets completely in the wrong order.

