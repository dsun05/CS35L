So last time we were talking about building stuff, right? And I forgot to say, but you know, one of the themes about building and delivering software is that it's a painstaking process. There's a number of things that can go wrong. general pattern for this kind of thing though, which is that originally people did this by hand and we've been trying to automate ourselves out of this job ever since. So you can start off doing everything by hand, which means you have the source code and you just hand it to a developer and say, oh, you know, build this and get it running for me. Right? Everything is human-done. And to some extent, that's what you do in CS31 and 32, right? We give you a programming problem. You write the source code. You build it and run it by hand, okay? And towards the end of the last lecture, I talked about, okay, what we want to do is we want to automate ourselves out of the job. For one thing, it's kind of, you know, it takes a while or it may take a while for larger projects for people to compile this and compile that and compile the other thing. They may do it in the wrong order and all that sort of thing. There's a lot of things that can go wrong. So we like to, in effect, write a program to do this somewhat mundane task that has to be done right and it's kind of boring. That's, by the way, a constant theme in software construction. Whenever you have something that's boring or repetitive or something like that that you're doing when you're writing software, you should be thinking, how can I write a program to do this for me? And to do it for me and, you know, basically put me out of a job. Don't try to say, oh, oh, I'm going to get paid for doing this painstaking thing over and over again because eventually you won't be. Somebody else will automate you out of the job. So you should be the one to do it first, right? And what we did last time was he said, okay, we'll automate this by writing a script. You can write it in the shell, you can write it in the Python, you know, whatever your favorite scripting language is. I'll pick the shell. Why not? Okay. And what people discovered was that although this approach works, you write programs that will build your programs, right? So they're in some sense second order programs. It has some real problems. And what I'd like to do is talk about how these problems were overcome in traditional systems where we're talking about C++ programs and that sort of thing. But I hope you can see the same kinds of problems will occur in any build system that you use, whether it's Python oriented or JavaScript oriented or whatever. All right. So let's take a look at one of these shell scripts that builds our software. All right. Here's, you know, it starts off hash bang bin sh. And what it'll do is it will go through all of our source code and compile it individually. source code modules so we'll go and compile them all right maybe there's three that's good enough and then maybe it has to do a little something else I don't know maybe there's an extra source code file foo4.c that the user has specified something right so echo I don't know here And then here we can say something like $1. And then we'll compile that. And then once we have all of these source code things compiled, we have a bunch of.0 files. Let's link it together. GCC, FU1.0, FU2.0, FU3.0, FU4.0, minus O, and we'll call this my program. So that's how to build it. Right now I'm not worried about distributing the software, I just want to know how to build it. So this is where we ended up at the end of the last lecture. We got a shell script like this that automates the job. And I hope you can see that although this works, it does build the software, it's automated ourselves out of the job, it's still in some sense not very good. There are ways that we can improve this. For one thing, suppose we run this program, right, that generates our program, we later discover there's a mistake. in one of the source files. Supposed to be. I'll turn it up. Oh, yes. It's on. It just wasn't loud enough. Is that better? Okay. So suppose now the problem is there's a bug in this program. We want to fix it. And so we say, oh, let's edit. I don't know. Foo1.c. All right. Using your favorite text. Now, we can rebuild our program by running this script. We'll call this script build. So we can now run./build. But that's inefficient. The reason it's inefficient is that it recompiles everything. All four files all over again when all we really need to do is recompile foo1.c. We need to do this step here. These steps, we don't need to do them over again. We do need to do this step down here. So what we'd like is some way of running this build script that's smarter and faster. Now for a small program like this, it doesn't really matter. It'll compile fast enough probably. But if you have, you know, hundreds of source code files, this can make a big difference. So what we would like now is some way to sort of automate ourselves out of a job, but automate ourselves more efficiently. How can we do that? Any thoughts? Well, here's one thought. I'd like to write a program P so that you give this script as input. and it generates as output a reduced script which only executes the commands that are necessary and skips all the ones that aren't necessary. If I could have a command like that I could then run this. Filter out the commands you don't want, pipe the result into the shell. That's going to be faster than just running the build script. So we want to think about some way of doing a higher order program like this that partially executes a script. Now it turns out that there is such a tool but it doesn't work this way. It works in a different way and this tool for partially executing build programs is called make. There's lots of other commands like Make in the Java world has and other systems have others but Make was the first one and is the one that's most standardized so I'll talk about it. The basic idea of Make is to take your shell script and give Make more information about what's going on so that it can easily do the equivalent of this of just partially executing the script but doing enough of it so it's just as good as a full execution. And the way that it works is that make takes as input not a shell script or a Python program or anything like that, but a make file. And the common name for that make file is capital N, A-K-E-F-I-E. The make file has the same commands that we had before, but it gives make more information about each of the commands. and do partial evaluations like what we want what would the make file look like here it might look something like this if you want to build my prod then you will first need to have these four dot o files one dot o two dot o three dot o and two four dot o Once you have these four files, then you can execute a command to actually build what you want, right? Which is going to be our GCC command. It's the same as what we wrote over there. Okay? And we do something similar for each of the other things. Foo 4.0 is perhaps the most interesting because it depends on having Foo 4.c. And, you know, you have to compile it. But foo4.c also is something that you need a recipe for building. And the way you build it is with that echo command. Echo, I don't care, whatever, greater than foo4.c. So what we're doing is we're breaking down our script into little pieces. And for each of the little pieces, we are decorating it with extra information that says here's the target and over here are the dependencies. And what make does, by default, if you just type make, is it finds the first target in your make file and says, I need to build this. I need to have these. Well, in order to build foo 1.0, I need to look at the foo 1.0 command. So in each case, what it'll do is it will be lazy. If it discovers that we need to build foo 1.0, but foo 1.0 already exists, and its timestamp is newer than that of foo 1.c, it'll say, oh, well, I've already done this in a previous make call. I don't need to do it again. So the idea here is to skip steps where the target is newer than the dependencies. And that's the fundamental way that it optimizes out unnecessary steps. If you have hundreds of modules and you just change one, you We should just do one recompile and then maybe a link and be done rather than having to do the whole rebuild again. All right. What can go wrong with this? Obviously it's a win in terms of building faster. But are there opportunities for build problems because we're using a tool like make rather than something simpler like a shell script? Any thoughts on what could go wrong? You guys are engineers. You should always be thinking about what scrubs can happen. Yes? Louder, please. Well, fine. So, for example, it could be that foo1 doesn't include a bar.h. So this command depends not only on what's in foo1.c, but also depends on what's in bar.h. In that case, what you're supposed to do is say this. And then maybe this one also depends on bar.h. Well, it doesn't, but you know, foo2.o as well. foo2.c bar.h. Right? And then you have the command down here. So you can have several different dependencies for a single target. And if any one of the dependencies is newer than the target, then you're going to have to rebuild. So if we change bar.h, we'll easily figure out, oh, we need to recompile foo 1 and we need to recompile foo 2. So it'll work. Any other thoughts about what might go wrong? Yes? Oh, yeah. Suppose you say A defends on B and B depends on A, right? Something like this in your make file. If you do that, make yells at you. And it can do that because it builds a graph of all the targets and all the dependencies in the make file, draws, you know, in its mind, sort of arcs and makes sure there are no cycles in the graph. And that's something that it can easily do. Yes? Oh, my favorite topic. No, they thought of that. All the timestamps are stored in universal time, so it doesn't matter what time zone you're in unless you're running Windows, which screws this up. All right. Okay, so far so good. Any other thoughts? But you're on to something about timestamps. Right? If you log into CSNET and run make on one machine, it'll use that machine's clocks. If you run it on a different machine, it'll use that machine's clocks. Now the CSNET ops staff try to keep all their nodes' clocks in close synchronization to within a few milliseconds. But suppose you run make on one machine and then within a few milliseconds you run it on another machine, right? So a problem with this approach is going to be the problem of clock skew. Even if everybody's running universal time, if you run on different machines where the clocks don't exactly match, the timestamps will get somewhat messed up. And it could be that you will do unnecessary builds, builds where you didn't have to do it because you already did it on some other machine. Or worse, you will sometimes skip compiles that you should do. because of the clock skew you think the file is up to date even though it's not. These problems are pretty rare on CSNET because it keeps the clocks very closely matched but they can happen. While we're on the top of this topic is there a way to fix this other than you know you could say keep every clock 100% in sync that's impossible let's just take it as given that there will always be clock skew. Is there any any way that you can do something like make without relying on a clock. Some build systems do that. They don't use timestamps. They use something else precisely because they want to avoid clock skew issues. You do a checksum. The idea is every step looks at the checksums of the dependencies. of the target, it stores somewhere else what the checksums used to be, and if the checksums have changed, then it says, oh, well, I need to do a rebuild. In other words, don't trust the clock. Keep track of some other stamp, a checksum, rather than a timestamp. Those systems avoid clock skew problem, but of course, they're more of a hassle because they have to store the checksums somewhere, and then that turns into a management hassle. So a lot of people, you know, still use timestamps. As long as you're Clocks are good enough, you'll be okay. Let me tell you one other advantage of this make approach compared to a ordinary shell script or Python script approach, which is since what the make can do internally is keep track of dependencies, right? Internally it builds a dependency graph. these guys and you know there's also food on see one dot see and food on see this is just a partial but you get a feeling for what the graph looks like right what make can do once it's built this graph internally is it can start doing builds but unlike the shell which has to execute the commands in order it can run this stuff in parallel It can do a build of foo4.0 at the same time that it's doing a build of foo1.0. And GNU make and other makes have that option with the, can do that if you specify the minus j option. This means run in parallel. Ordinarily, it just runs in parallel on a single machine. So if you're on LNX SRV 11, it'll run everything there in multiple processes and that sort of thing. You can also rejigger it to run in parallel on multiple machines, but that's going to be overkill probably for our applications. This can increase your speed of your builds enormously if you have a machine with a lot of cores. This is how Linus Torvalds recompiles the Linux kernel. His workstation has 96 cores. The last time I check, he runs make minus j all the time and he builds the kernel just in a few seconds. Can you see any other issues involved though with this sort of thing? Any problems that might occur? Here's one. Suppose foo1.0 depends on bar.h, but suppose we forgot We have a bug in our make file, right? Because there's a dependency that if you go look in the source code, foo1.c includes bar.h, but in our make file, we don't reflect that dependency. And what that means is that make will sometimes fail to build foo1.o even though it really should. We changed bar.h. Make doesn't know that bar.h affects foo1.o, and so it screws up and doesn't do a build that it So a common problem with this kind of approach is forgetting dependencies. You can also have the opposite problem, inserting extra dependencies, putting in wrong dependencies. For example, suppose foo4.c does not include bar.h, but we do this. We tell, make, there's a dependency exists that doesn't. This way, the problem will be less severe because what it means is we'll do unnecessary builds. Our builds will be slower. But we'll still get the right program in the end. The forgetting dependencies is going to be the bigger hassle. All right. This kind of issue of extra dependencies versus forgetting dependencies, you're going to run into a lot of software construction. systems. It's not just make. You'll run into it when you're using Python, lots of other things. People will forget to put in dependencies. You'll try to build a program. Your program won't work. Is there some way, some way that we can address this problem? Some way that we can say, I don't want to forget dependencies. Well, it turns out people have been trying to do that with some success, not entirely. There is, for example, an option in GCC and Clang in which what will happen is the following. It'll compile the program, and as part of the compilation, it'll keep track of every source file it looked at, every include file and all that sort of thing, and it'll put that somewhere in some other file. So GCC will automatically list dependencies for you. And then you can, if you like, arrange for GCC's output to be put into here for the next time you do a build. So this could be, say, the output of your compiler. The first time you do a build, it'll be blank, but that's okay. The.o file doesn't exist, so it has to build it anyway. Later on, it will remember what the dependencies are, and they'll be accurate, at least for the previous build. So that's one way around the problem. It doesn't solve the problem in general. Sometimes when you change the source code you're going to change how the dependencies work. But it might be better than nothing. Alright, any other issues that you can see with this approach? I have one. How well does it scale? Suppose you have a big program, something like Emacs or Chrome or something like that, with hundreds or thousands of source files. How do we scale this sort of thing? It's very common in this situation to arrange to put your source code in multiple directories for convenience if nothing else. is to do a different make in each subdirector. The idea is that you have the root of the source tree here. You have, say, three subdirectories. This might have another subdirectory. These subdirectories could be called D, E, and F. Inside this make file here, here's your makefile. If you look inside that makefile, it will say, well, in order to do a dbuild, what you should do is cd into d, and then run make there. This shell command, this is a conditional and, which you can put in a shell command. This says, you know, cd into that subdirectory and run make there, with the idea being that there is a separate makefile here. Each source directory has a makefile. That makefile is limited to concerns about that source directory only. That way you can manage your makefiles in a more local way. You don't have to have a huge makefile that knows everything. So that's one approach. The downside to this approach is that it becomes harder to parallelize. Here we have D build, E build, and F build. Each one has CD and run make. Now you have multiple make processes, each perhaps running in parallel. How do you coordinate them? That turns into a real problem. And for that reason, what I tend to see more often is a different approach. You have a single top-level makefile. You may have a bunch of subdirectories. Each containing, you know, all these directories contain a whole bunch of source code. But what this makefile does is it looks like this. at its top level it will say include d dot sub dot make include e dot sub dot make and so forth include directives and make files work much like they do in CNC plus plus they may basically say take the contents of this subsidiary file and just included here and pretend that I typed it here Now what happens is the top level make will see a fairly short top level make file but it expands into something long because all these subsidiary make files are included by reference. Then this top level make knows everything about the build and can intelligently run things in parallel, much more intelligently than this approach. So this is the kind of thing that you have to run into when you're trying to scale to a larger You have to think about how different parts of your application are going to build together and work in an efficient way when oftentimes what you want to do is change one little piece of your program and then do another build. Okay. Let's try a different problem. So far I've been focusing on this script. Right? And the idea was to take the commands that you originally would have run by hand, stick them in a script. With make, the idea is skip some of these commands, if you don't have to do them this time, and also run these commands in parallel. Now I want to turn our attention to a different problem. Suppose you're writing a C program, and you want to write a portable program. This is going to be a portable, low-level program. And somewhere in your code, what you want to do is you want to invoke the following function. Rename at 2. You can type man rename at 2 on a Linux machine, and you'll see what it does. It is a very fancy function that renames files. Lots of options. Rename file A to B with options. One of the options might be, for example, if A is a symbolic link, I want to rename the file that the symbolic link points to. I don't want to rename the link itself. Another option might be if B already exists, then don't do the rename and instead fail. So lots of options. This is a nice function to call. Well, on Linux, it's very efficient. Unfortunately, this system call is not available on many operating systems. If you try to run this on Windows, Windows will say, what's that? I've never heard of it. So if we're writing code that looks like this, how do we do it? How do we actually do it? Because we want it to be portable. We want it to run on Windows, and yet it won't run on. One option is we give up calling this function because it doesn't exist on Windows, but that means our program will run less efficiently on Linux. All right, assuming we're writing in C or C++, what's the obvious way to attack this problem? You guys are C experts, yes? Inheritance. Oh my goodness, right? So the idea would be that we want to would write a rename at to the inherits from but but Microsoft doesn't have a rename at to at all there's nothing about it in the API so how do we how do we how do we actually get it in there right so one option is to do something like this right if ms windows or maybe we'll do it this way if underscore underscore Linux That is, we use a preprocessor symbol. This is much lower level than what you were proposing. And this is going to be sort of slower code involving sort of rename or something, right? And some other stuff, right? That'll handle all the options and all this thing, but it'll involve sort of system calls, it'll be slower, but at least it'll work on Windows. But now, suppose later, suppose later Microsoft sees the light and adds support to this Linux-based operating system call. All of a sudden, our code's going to be wrong. What we really want to do is something like this. We want to not say if it's Linux use this system call. We want to say if rename at to works. And I'm just making up an identifier here, right? Where this identifier is a constant, it's visible to the preprocessor, and if it's true, we know rename at to works, and if it's false, it doesn't. This kind of issue I feel like I'm like gone to an audio high-end audio store and I'm trying to listen to see where the anyhow sorry so so this is the sort of thing that if you were writing say I don't know Python code you could do this kind of checking at runtime right you could say try rename that too if it doesn't work fall back on the slower code right but we don't want to do stuff like that here first off it won't even on the other platforms and second it's going to be slow we want our code to be fast. So a way to write this kind of portable code in C is to build on something I already kind of snuck in over here. Which is what we can do is we can write a program that sees if rename at to works and if so it will create a header that says So in our make file, we can say, oh, I've got a file called bar.h, or maybe I'll give it a better name, config.h. And the way I build it is something like this. GCC test rename at two works and you know we run the test program then echo one else echo zero p we stick this into config dot h I guess we're going to have to say more than echo one we'll have to say Define X1 else define X0 greater than Foo config.dynch. So what we're saying here is that we need to have a configuration file that contains in it information that our programs need. our portable low-level program here, let's call this low.c. If you want to build low.o, you need both low.c and configure.h. And then you can compile it. So the idea here is that as part of your build process, you go the system that you're building on, find out what its properties are, right? Here's the interrogation, right? And then you report what you found as part of a configuration file that later parts of your system can use. Here, I guess I should say X instead of rename at two works. I'd probably come up with a better name than X, but I hope you get the sort of feeling of what's going on here, right? So the idea is find out the properties of the system. Use that to write code that's both portable and fast. This will use the fast system call if it's available. This kind of probing the build system occurs all the time in big software builds, at least if they want to be portable, because they need to be able to run on macOS and Windows and all all sorts of other platforms like that. It's so common that this task here that I'm circling is not something people write by hand. This shell script is something that's generated by another program called AutoConf. In effect, AutoConf is a program for generating configuration scripts. So autoconf will generate a file called configure. And then as part of the build process, you should run./configure. And when you run./configure, it will do the equivalent of this. This is building on our general theme. We're trying to automate the build process. The things that we kind of have to write out long and do by hand, don't do that. Have another program do it for you. This helps to explain if you're building a lot of applications. The first step is to run a configure script. It's probing your system. Then after that, you run make because make has all the proper information that you might This idea even goes one more step. A problem with make is make is pretty simple. There's many features that we would like to have in make, but they don't exist because it was standardized many years ago and people don't like to change the standard. There's another program called automake. It generates makefile templates. That is, it looks at your source code. It looks at a small amount of configuration. It uses that to generate makefiles for your program that, you know, only use the simple features of make because automake has sort of figured out whatever other stuff that makes it work. doesn't support and it supports it by hand. It automates the generation of these make files. And I could go on from here. There are other programs that generate all of these other things. The idea here is that this automation toolkit is pretty extensive. It goes several levels of indirection. It can be a little intimidating for newcomers, but each individual step is relatively simple. The idea is you're building a tool suite that will automate yourself out of a job as much as possible. Now, if you're doing stuff in Python and you don't have to write any C code or C++ codes, you don't need to worry about this stuff, then you're going to have a simpler approach with package managers in languages like Python and Node. All right, JavaScript. They don't have to worry about precomputing, see preprocessor symbols and all that sort of thing. But still, when you look at these package managers, a lot of the stuff that I've talked about comes through because you need some of these capabilities. for any sort of packaging system. For one thing, they all end up doing some sort of configuration scripts. It won't necessarily be a program called configure a shell script that sort of does a whole bunch of stuff, but it will still give you a way of dropping into arbitrary shell code or Python code or JavaScript in order to control your build process. The idea is you're not running code in the application program, you're running code to generate the application program. And that's a common sort of theme in all of these systems. Another thing that tends to come up reasonably often is you need to be able to make patches to the source code. Ideally, you'd like to use those other packages unmodified. In practice, you're going to have to place some constraints on them. For example, you might say, I'm only going to work if this other package is version 7.3 or later. Or, I'm only going to work if this package is version 8 and you apply these extra patches to it. That sort of thing. Other information that you'll find in these package managers include things like licenses. information. So this stuff has been evolving with time. If I were to talk about, say, Python today, I would say, well, you can use pip. This is a common sort of Python packaging tool. It's not the only one to use. And if you have a Python package, typically you'll have a of the package that lives in the file. A common place to put that would be in a file called pyproject.toml. This attempts to be a package build agnostic file that you can use regardless of whether you use pip or something else and will describe your package. And so it will contain lines that look like this. Well, in your build system, you'll say, well, in order to do this, we need setup tools greater than version or equal to version 77.0, something like that. So this is telling the packager that it had better have setup tools installed and it needs to be at least this version otherwise our package isn't going to build. As you can tell setup tools has been around for a while. It's one of the older sort of back ends for building. And then you'll have under project you'll have things like your name. You'll have a version. You'll have authors. That kind of information. And there's a standard form for all of this. You need to sort of fill in the blanks to describe your project and all that sort of thing. Some more interesting information here is you might have a license, which would be, if you're me, GPL version 3. You can put in whatever license you want. since you like that sort of thing. And the idea is this is a configuration file for your package. But it's not a configuration for running the package. It's a configuration for, you know, building and installing the package. A common mistake in doing this sort of thing is kind of saying, "Oh, I don't, you know, this is just boilerplate. I don't really want to do this. I don't want to figure out exactly which version of setup tools is the right version," that sort of thing. Try to resist that temptation. Messing up configuration files is one of the first sins of software construction. Oftentimes, the configuration file mess up will bring down your entire system. that sort of thing. So make sure you have a good project description and that, you know, it matches what your project wants to do. All right. I feel like I'm preaching to the choir here. You guys all love Python packages, right? And you've each, and you've written like dozens of them already. So I'm not telling you anything new. Is this correct? Nobody wants to say anything. All right. Why don't we take a break and we'll talk about something completely different after the And I'm going to try to fix the sound system while we're at it. All right, so let's talk about something completely different. I want to talk about databases. I think it's possible to get a CS degree in this department without taking database course is that correct oh you mean you guys could be put out into the world without knowing anything about databases that's ridiculous right so to some extent we try to work around that problem by giving you a project where if you don't use some sort of database you're nuts and so you pick it up off the street And to some extent we try to work around that problem by I summarize our undergraduate database course CS what? 143 and we have an hour. Or 50 minutes or something like that. So we're going to do 10 weeks of lectures in 50 minutes. Alright? So first off, why do we need them? Alright? That is One way to think about it is, why aren't file systems enough? You learned about file systems in the first couple weeks of this class. You can store data in those file systems. What's wrong with that? Any thoughts? There must be some database experts in the in the maybe not all right well I'll give you my opinion first off file systems are too loosey-goosey they're not organized enough you can put whatever you want into them and what that means is when you get stuff out of them you got to write a program and it's got to do this and it's got to do that it's just a gigantic hassle all you want is to have a nice organized system of data and the database system will supply that to you. Second thing is when you make a change to your database, you want it to be reliable and consistent and a whole bunch of other things. You don't get that out of file systems. In a file system, if you need to change this file and that file to make something happen, then you'll be toast. What's going to happen? first file the system will crash now your files are inconsistent right so we want more consistency third reason if you want to find something in a file system it can be really slow yes you can run grep - r to recursively look for for whatever needle in that gigantic case that you're looking for It's going to be too slow once the file system gets too big. So, the problem with a conventional file system is that searches are slow. The only real way to sort of search them is to look at every byte. That's going to take too long. Obviously, you can improve that by writing your own indexes and that sort of thing. That's what Git does. In some sense, you can think of Git as having a little database of its own design that tries to work around these problems. Fine. Does that mean every application has to come up with its own database system? That would be crazy. We'd like to have a single database system that lots of apps can talk to. So that's basically why we need them. Now, if you talk to database experts, you will find that there's not a consistent to what is the best database system out there. Everybody has their own opinion. Some opinions are pretty strongly held. Nobody's right because there is no single database system that dominates everybody else. However, it's helpful to take a look at one or two successful database styles so that you can see kind of what you're up against. And the longstanding champion in that regard is called relational database systems. They've been around for ages. For like 50 years. We know a lot about them. They're very reliable. There's lots of good properties about them. And so we'll talk about some of those properties. And then once we're done with that, I'll try to say why We sometimes don't like them. But the basic idea of relational databases is the following. Data are a collection of tables containing rows and columns. That's basically it. If you want to have data in a relational database, you've got to stick it into a table. Every table has a fixed number of rows and columns that you know about. Maybe you'll have to come up with a new table or something like that. But that's how you do things. Everything is tables. It's sort of like POSIX file systems say, everything is a tree structure. And then the leaves are whatever you like. Here, everything is a collection of tables. And you can put whatever data you like into the rows and columns, but you've got to stick them in a row and a column. Right? Each column in a table has a single type. So columns are typed. You say this column is an integer. This column is a floating point number. This column is a character string of length 10, that sort of thing. So you can think of it as being like the types in a struct in C. Each row in a table is distinct. I'll put an asterisk next to that. What I mean by that is no two rows in a single table are equal. They have to differ in at least one column. The idea here is that the table is a set of rows. I put a little asterisk here because some people don't like this rule and so they'll say, "Okay, duplicates are allowed." In other words, a table represents a multi-set of rows rather than a set of rows. and keep going and pretend that the tables are sets. A key is a column, all of whose values are distinct. If I look at the table representing the enrollment in this class one of the columns is major. The registrar tells me for each of you each of the students in this class what their major is. That's not a key. We got a lot of CS majors in here and that column says computer science computer science computer science all over the game. Even the name of the student is not a key. I have had classes where I have two students with exactly the same first middle and last name. We have to be very careful when we're grading the exams to make sure we put the scores to the right person, right? However, we do have a key, which is the student ID. Each of you has a unique nine-digit ID, and that's the key. Keys are a big deal, and to some extent, the reason you have student IDs is so that Murphy Hall can put you into relational databases without collisions. Now, a table may have a primary key. It will have at most one. You can't have two primary keys. You can think of the primary key as being an index into the rows of that table. A good primary key for you guys is your student ID, right? That's another reason why you have student IDs. There's also the concept of a foreign key. A foreign key in a table refers to a key in some other table. This is a big deal because if one table has a foreign key pointing to another table, the two tables have to be consistent. You do not want to have a dangling foreign key. I don't want to have a table that says I'm pointing to the student table and here's the student ID 109763. 3.1, but it turns out there's no student with that student ID. That's a bug. So foreign keys, saying something as a foreign key is a big deal because it puts a constraint on the whole database. Another thing with an asterisk, because different database people have different opinions here and they can sometimes come to blows, is can an entry in a particular column of a particular row be missing. So for example, in the table of students for this class, can the student ID be missing? No, that would be a disaster. Can the major be missing? No, even students who don't have a major will have a major saying undeclared or something like that, right? Can the student's final grade be missing? And the answer is yes. We don't know what your grades are yet, right? So some relational database people say, no, you should never allow missing entries. These are called null entries. Other relational database people say, "Oh, null entries are fine. We'll allow them." Because in the real world, a lot of times you just don't know what something's going to be. We shouldn't force you to put something in there when you don't know what it is. A problem with null entries, though, is the following. Suppose we want to find a student, right? And so select student, da, da, da, da, da, da. And we say with final grade equals A, right? A natural query to make. If you allow null values, then when the final grade is null, this query is neither true nor false. So we don't know whether the final grade is A or it's not A. So null values tend to cause trouble when you're dealing with databases and this is why some people don't like it. You get three valued logic. This is either true or false or I don't know. And three valued logic, even two valued logic is tough enough without trying to have a third value. Alright, any questions so far? I guess I should ask, is this stuff new? Or am I, again, preaching to people who already know this? Should I go faster or slower? All right. How many people want me to go faster? How many people want me to slow down? All right. I will slow down a little bit then. All right. I always want to go faster. So, you know, you got to ask questions. Otherwise, I'm going to keep speeding up. Where were we? Once you have this idea that your data are organized as a bunch of tables, then you have the notion of the relational algebra. This is an algebra with operators that applies to tables in a relational database that lets you build larger tables out of smaller ones. Or smaller tables out of larger ones. That lets you hook together tables in sort of logical ways. The simplest operator is selection. And the simplest way of doing a selection is you have a Boolean predicate on a table's row value. So I could select from this, the table for this class, and pick every computer engineering major, and also you have to be a computer engineering major and you have to be a junior. That's a Boolean predicate. Major equals computer engineering, and class equals junior, would be a selection. A selection typically gives you a smaller table than what you started off with. It has fewer rows, but it has the same number of columns as before. You can also, of course, do set union and intersection. If you think of a table as being a set of rows, well, you can take the union of two sets, that's an operator in the relational algebra. You can take the intersection of two sets, same thing. This works as long as the tables have the same columns of the same type. Tables have to be compatible for you to be able to do set union and intersection. You can also do Cartesian product. And then in the worst case, if you have a table of size M and another table of size N, you get a table of size M times N. So a Cartesian product gives you a table that takes all of the rows in the original table and all the columns in the two original tables and comes up with a row for each possible combination in the original two tables. To be honest, these are less common. Theorists like to talk about it because they fit into set theory and mathematics and all that sort of thing. But as a practical matter, you know, Cartesian product is, wow, it multiplies things out pretty fast. These, the tables all have to be sort of compatible. And if they were compatible, why did you have two tables anyway? Next one is projection. Projection in some sense is pretty simple. You have a table, you throw away columns. You come up with a simpler table. So for example, if I took this classes table and I threw away student names and IDs, I would get a much smaller table because those two columns are missing. But also a lot of rows would be missing. Because the remaining rows in this table are things like major and class and that sort of thing. And there's a lot of CS majors that are sophomores in this class. And they would all collapse down into a single row. So projection is going to shrink your table both in number of columns and typically, though not always, in number of rows. And then last and most interesting is the join. which takes up the most ink in relational algebra because it's in some sense the most complicated operation that's really useful. And the idea is as follows. If you have two tables, R and S, you can join them and they have a funny symbol that looks like a bow tie. And the idea here is that this gives you all tuples in R and S that are equal in their common attributes. And these tuples are sort of generated out of R and S's as tuples. I feel like I should give you an example. So here's R. Here's S. And suppose R just has two columns, student ID and major. And S has two columns, student ID and class. So over here, we have student ID 000001 is a CS major, student ID 005093. is a computer engineering major. Over here, we have the same student IDs. And this CS major is a senior. This guy is a junior. If you take our join s, you glue these guys together, you throw away the common attributes, and you get something that looks like this. 00000001 CS junior this is 0 senior sorry this is 005093 CE junior it could be that some of them didn't match up here's another student ID 0090501 sophomore here's another one 0056931 and this is a This doesn't match anything in here, so we ignore it. This doesn't match anything in here, so we ignore that as well. So with a join, you're trying to glue together tables. In some sense, it's kind of a selection projection combination operation. But this operation you do a lot in a relational database in order to resolve queries that talk about multiple tables. If someone asks a question about multiple tables, the relational database will commonly do a join operation to come up with a joined relation that helps answer that question efficiently. All right. Any questions? Then I'll go faster. You need to deal with this stuff. When we were talking about the POSIX file system, after I showed you the file system, I said, How do you deal with this stuff? You can write shell scripts or C programs or that sort of thing. Relational databases don't work that way. The way you deal with them is via SQL. That's other ways, but SQL is the standard way of doing that. SQL is an acronym that stands for nothing. Well, officially it stands for structured query language. So it's kind of like a programming language, like a shell script, except it's not. It's a query language. And it's structured, but the word structure basically means, yes, you can do stuff with it, right? So it's really a way of asking questions of a relational database and hopefully getting some answers about it. So here are some things you can say in SQL. You can create a table. And I'll create a table called student with a primary key. The name of the primary key is ID. The type is integer. And I'll have a family name which is of type varcar255. That means it's a character string of a length most 255 could be less. Var is short for varying length. And then given names. Also var car 255. And a major. Which can be var car. I hope there's no major in UCLA that has more than 64 characters in it. So what we're doing here is we're telling the database to create a table when after it's created it will have no rows in it but the database will now know that the table is there and will know the types of all of its columns. This part of dealing with the database is called creating its schema. The database schema says what tables do we have? What are the types of the columns in this table? What are the relationships between the tables? So we might create another table called class, which has a student ID, int, and this is going to be a foreign key that references So this part of our schema is saying that this new table class that we're creating has a relationship to the first table that we have. Both tables are still empty, but we know if we ever create entries in this guy, there has to be an entry in this guy that is being referenced. And then a letter grade of our car. Actually, I think you can get by with two letters, right? Two characters? Put a two here. All right. See how that works? So this is how we set up the table. We can now do stuff by either updating the tables. There's lots of commands to do that. "insert into student values." And then we'll do the numbers. Anybody remember his major? These are actual UCLA students. Former. This is not their student ID. By the way, if I actually wrote down their student IDs, then I'd get into trouble, right? That's supposed to be private information. So these are just made up numbers, right? You get the idea, right? Yeah, that's good enough. Right? So you can throw stuff into the database. There are bulk load facilities that will do this more efficiently than this, but this is good enough. And then you can issue a select command. This says, give me every student in the student relation. I want to know their ID and their major, and I want you to sort in ascending order by major. That sort of thing. This is just the tip of the iceberg. There's lots more where this came from, but the point is that you can now deal with a database system. And the simplest way you can deal with a database system in your program is Open up a connection to Oracle or whatever it is. Ship it a bunch of SQL. It'll ship you a bunch of text back in response to your queries. Done. You now have a way of updating databases and querying them. And it's all text-based and it all works just fine. It's standardized and sort of everybody in the database world understands it. Any questions about this simple model for databases? Yes. What are what? What's the downside of this? If your data don't fit into this model, right? This is one particular model. It's the relational database model. If your data don't fit into a neat set of tables where the columns are known in advance and the relationships between the tables are known in advance, for you. This works very well for the student database that Murphy Hall uses. I don't know what technology they're using, but I'd be very surprised if it's not a relational database in SQL. So it works very well in many business applications and that sort of thing. Not too surprising. It was originally designed by a guy at IBM. They're very much into business. Works great. But it's not enough to do everything. So for example, Oracle a few years ago, or some years ago, decided let's implement a POSIX file system atop of SQL. You can do that. You can use tables to implement directories. You can use tables to implement files. A file is just an array of bytes, so you say byte number one is this and all that sort of thing. So it's possible to implement a POSIX file system atop of SQL. but it doesn't work very well. It's not a good match. So you really want to have something that fits well into this relational model. And for this reason, people have been trying to escape from the relational model and coming up with better models. Here are some example database models. I'll just go through them very quickly. One is the ER model. This is short for Entity Relationship. This focuses on three things. Entities. You're an entity. I'm an entity. This class is an entity. That sort of thing. So entities are like rows in a standard relational database. And then attributes. I have blue eyes, you have brown hair, that sort of thing. So entities will have attributes. And these act like columns in a standard relational database. The third thing we have are relationships. These act sort of like A foreign key essentially is a pointer from one entity, that is from one row in your relational database to another. The ER model focuses on these things, it doesn't focus on the tables. You can implement the ER model in a relational database, but you can also implement it from scratch if you like. Here's another one. The object model. To some extent, the basic idea is the following. Object-oriented programming is great. It's the way everybody should write code. Similarly, we should use all those good ideas from object-oriented programming in databases. All of this stuff about relational databases in the ER model, that's like writing C code. We should be writing C++ code. We should have something that works well with object-oriented languages like Java and that sort of thing. So let's do that in our database. In this model, we have objects. These are like entities in the ER model, right? I'm an object, you're an object, this class is an object. Objects have attributes. Pretty much everybody has attributes in all of these systems. But we have something new, we have classes. objects. You can think of it as a set of objects that all kind of behave the same. So these classes are sort of like tables in a relational database. You have a table containing a bunch of things. Well, each one of those things is an entity or an object, so that's a class of objects. And then you have methods. Methods are bits of code associated with classes. This part is new to the object model. But until now, we've only seen data. This is the first time that we actually see code, and that's the influence of the object-oriented programming model on database systems. All right. Let's do one more. This one is probably one that you used in your project. Well, I shouldn't say probably. Good chance. No SQL. As you can tell, this database model was designed by people who hated this. This is SQL. SQL is a disaster. Why? Because it shoehorns your data into this funny little model where everything is this nice little rectangular table. Real data isn't like that, at least not in many real world applications. So no SQL tries to be more loosey goosey and forgiving and you don't have to sort of get your data into this precise business-like format. So some ideas in NoSQL are a focus on key value stores. These are big hash tables, except they're in your database, they're not in RAM. So you can say, I've got this, I guess you can call it a table, except we'll call it a key value store. And it has a key, which is like the primary and it has a value which is kind of any object that you like and that's it. So one common property of this is that they are wide column. And that's kind of a buzz phrase. But what it really means is that columns are set dynamically. When you create a table, you don't have to say in advance what the columns will be. You can decide as you update the table. Also, you don't insist that every row in the table have exactly the same number of types of columns. Some rows might be different. That's okay. We're forgiving in a NoSQL database. So rows need not be homogenous. How do you spell homogenous? Homogenous. You can look it up. Alright. Another sort of variant of this are graph databases. In graph databases, you have entities that can point to other entities. You can set up an arbitrary graph. The graph can have cycles. It can have loops. They can be fairly nice. We also see a lot of what are sometimes called document stores. That is, you can put into the database some random XML or JSON document or PDF or whatever you like, right? You can put other junk into the database other than just character strings and integers and that sort of thing. All right, so these systems tend to be more flexible. You get a lot more freedom. But you give up some stuff to get that freedom. For example, if you just have this big key value store and you want to search for a particular value, good luck. Right? It may not be indexed and it may be very slow. Yes? A difference between what? Oh, well the idea here is that in NoSQL, You have a table that looks like this, right? These two rows have different numbers of columns. And this row has some gaps, right? You don't require every row to be the same, to sort of smell the same. Fair enough. At this level, Not much. Not much. I think part of it though is more the attitude, right? That is, with SQL, you commonly tell it like what the data types of all these things will be. But with no SQL, this thing could be, like an integer, this thing could be some complicated JSON object. And it's okay. They don't have to be the same type, even though they're they have sort of the same name of the column, that sort of thing. So it's a lot more freedom in NoSQL. All right. Any questions about NoSQL? I've really just given you the surface, right? There's a lot more where this came from, but that's why we have CS143. There's another major topic, though, about databases that I really should talk about. And I'm not sure I'll finish today, but I'll do my best. which are reliability issues when it comes to databases. As I briefly mentioned earlier, one of the key things you want out of a database that a file system isn't always going to give you is a high degree of reliability. When you store stuff in the database, it stays stored and all that sort of thing. And the database folks have nailed down some kind of things that are that you should know about because ideally a database system will satisfy all these properties and in practice some of them kind of mostly do but sometimes don't and they do that because they want to run faster. So there's a trade-off here between reliability and performance. Oftentimes people chasing performance will lose sight of reliability and maybe get into trouble, right? So you should know what you're giving up when you get that extra performance. The standard sort of acronym or buzz phrase for reliability and database is ACID. You want your database, I suppose, to pass the ACID test. It stands for four reliability properties, atomicity, consistency, Isolation and durability. A reliable database will have all four properties. Some databases will stretch things and maybe give up one property in order to get speed, but you should know which property you're losing. So atomicity means every transaction is a single unit. In other words, when you tell the database system, I'd like to make this change to the database, it either does it all or it doesn't do any of it. It's either all done or nothing is done. So, for example, here is a transaction, this insert statement. I'm saying, please make this change to the database. For the database to have the atomicity property, what it should not do is first insert Kareem into the database, and then later insert Coppola into the database. Right? Because if it did that, I could do a query and I could find Kareem, but Francis Ford Coppola would be missing. That's not allowed if you have atomicity. Both of these guys have to get to the database or neither of them. You're never halfway there. All right. Clear, I hope. Any questions about atomicity? This has to be true even if The system is right in the middle of processing that insert statement and the janitor trips over the power plug to the system. The power goes out. When it's done, neither of those two people should be in the database because it didn't finish. You can't have one guy and then the other. The dry technical term for this is database invariants are preserved. Oh, that's boring. What does that really mean? That means that when you create the database and you set it up with a particular schema and you say some of the database because of that schema, those properties are always true. If you say that there is a primary key in this table, then every time you look at the database and you look at all the columns in that table, they all have to be different. Because otherwise, it's not a primary key. It's not a key at all. If you say you have a foreign key from one table to the next, that means every instance of that foreign key had better point to somebody. in the other table. That's another database invariant. So consistency means no matter when you look at the database, the invariants are always true. And these invariants are the obvious invariants that you would see if you looked at the scheme. Third one is isolation. This has to do with a database that's busy. A busy database can get requests from lots of different people simultaneously or nearly simultaneously. It's as if lots of people are hammering on the database each with an SQL session and each doing inserts and selects and that sort of thing. So isolation means when multiple transactions operate at the same time. Transactions run simultaneously. They don't affect each other other than the way they would affect each other if they ran in sequence. They behave as if they ran independently. You can't have the fact that one person is putting in Kareem with this student ID and another person is putting it in with a different ID. They can't end up with Kareem having a mixture of the two IDs or anything like that. One of the transactions has to win, the other one has to lose. That's isolation. The last one is durability. And I guess that should be straightforward, right? Data survive. underline sort of hardware and software. If you're running a database on CSNET's LNX SRV12 machine, and it crashes, the operating system crashes, you don't lose your data. If the database is running on that machine and one of the flash drives flakes out, goes completely toast, you still don't lose your data. So sort of one of the aspects of durability is surviving at least individual failures of underlying hardware components. Any questions about these four properties? Yes? Well, like durability is never an absolute goal, right? I mean, if all the flash drives in Belter Hall go kuflooy, then your database isn't durable. So this is always relative to some sort of failure model you have for the underlying failure. Right? So in some sense, you're willing to give up at least durability all the time because there's no perfect durability. Are you ever willing to give up atomicity? Well, maybe you're sort of relaxed about atomicity in this particular case. You may have a database system that says if you do a bulk load of 50,000 and students, then that bulk load operation is not atomic. If you want to be atomic, you've got to do single inserts. It will give you some operations that are atomic and document them. Other operations for efficiency reasons are not atomic, but it will let you know that they're not atomic and will tell you how to recover in case the thing fails halfway through. So you can give up atomicity as well. And similarly for these other things, these are meant to be sort of common sense goals for a database, and ideally you reach all you typically have to give up something but you should know what you're giving up. Other properties about, other questions about these. All right, so does Firebase satisfy these properties? But how many people here used Firebase in their project? I didn't do a project. "Does it satisfy ACID?" One of the things I'm trying to tell you here, and I'll say it out loud, is you should know how reliable your database system is. Sure, if you're doing a student project, who cares, right? If it goes down, well, your grade will go down a little bit. It's no big deal. But if you're building a system that people rely on, you have to know this sort of thing about the underlying database. And it gets worse because typically when we're talking about ACID, we're talking about a single central database servers. A lot of database systems now are decentralized and for that we need some extra properties that I'll talk about next time.