We were talking about how to debug a program without using a debugger. And I want to continue in that. The debugger we're talking about in this class will be GDB. There's lots of others, but of course this is a canonical example. Later on we'll talk about debugging with GDB, of course. of debugging without GDB. Last time we talked at some length about using static checking. Various techniques involving GCC and Clang will do the same thing on how to check your program before it starts to run. And there's a lot of things that are very helpful there, but they're not enough. The problem with static checking is that it can't really infer everything that you want to know about a program it's not smart enough it can't be smart enough in general once you take the theory class you'll see that if a static checker could find out anything a dynamic checker where you could solve the halting problem which was proved a hundred years ago to be impossible all right so we need to have dynamic checking more powerful for debugging albeit slower because it slows you down and also in some sense less reliable because it only checks the particular run of your program. It doesn't mean you won't have this bug in the next run where you explore a different sort of test case that sort of thing. All right when you're doing dynamic checking one approach is to do things yourself. That is change your program so that that it adds extra checking, not necessary to actually get things to work. It's just you're worried that your program might be buggy. And so you want to catch the bug earlier rather than later. So you add extra code to make sure that the bug doesn't occur. So instead of just saying a sub i somewhere in your program, before you sort of do a sub i, you do something that looks like this. If it's not the case that 0 is less than or equal to i, and i is less than or equal to n, where n is the size of the array, then you call some sort of error function. Now, I'm assuming error is declared to be no return. We talked about that last time, right? A no return function doesn't return, so you know that you can't possibly get to this part of the code here unless I is in the proper range. This sort of thing is fairly standard among high reliability programs. As you can tell, it's kind of a pain. What, every time you do a subscript, you got to insert some code like this? Plus, you might make a typo in your code. It could be that this value is wrong. You should have said M or something because that's the size of the array. So a problem with doing things yourself is that it's verbose and it's error-pound and it's yet another thing to maintain. Still, sometimes it's sort of the best way to go, right? Just as a review, suppose you're trying to do something like this. is integer overflow. J and K, R and I, let's say they're all of type int. You're worried that this multiplication might overflow. What runtime check will you put in to make sure it doesn't overflow? Is there a way to check to make sure the multiplication actually worked in C or C++? All right, so you can say something like this. If j times k is greater than int max, here we're assuming these are typed in, then report an error, right? Will this code work? No, how come? Yes. They teach that misconception in CS33. The textbook tells you that misconception. And the textbook is wrong. In C, the rule is more subtle. C++ is the same thing, which is that unsigned arithmetic wraps around, module 2 to the n. Signed integer arithmetic has undefined behavior. The implementation can do whatever it wants. like this if j times k business because if j times k overflows, the implementation can crash or do whatever it likes and so you'll never even get to the error function. This has been sort of a sore spot in low-level language sort of development for quite some time and because this kind of multiplication is turning into something that the bad guys are attacking, they're looking for integer overflow bugs in your program, and exploiting them. The recent C standard has added a way to check for integer overflow. C++ is going to do it in C++26. The way you write it is something like this. If CKD mul of IJK error. because there's an include file called stdckdint.h that defines this macro. And what this does is it multiplies j and k, puts the result into i, it returns true if there was an overflow. And if you really want to be doing overflow checking for integer arithmetic, which is, like I said, kind of becoming a bigger and bigger deal, you've got to write code like this. Okay. How many people really want to raise their hand and say, I love this stuff. I'm going to write my code this way. Nobody. It's a hassle. So it's very common with dynamic checking to not do things yourself, to not write code that does subscript checking, not write code that does overflow checking. Sometimes you need to do it because you're running at a low level, but sometimes you say, just let the system do it for me. And for that we have some GCC options for you. One of them is this one. Sanitize equals undefined. This is the undefined behavior sanitize originally written by Google engineers that were tired of having their programs crash. So they modify GCC and also Clang. to have an extra flag. What this flag does is it tells the compiler whenever behavior would be undefined, for example, whenever this multiplication overflows, don't keep going. Don't wrap around arithmetic or that sort of thing. Instead, crash. Have your program immediately crash by calling abort or the equivalent of abort. So sanitize equals undefined. in some sense doesn't change the meaning of your program, right? Because your program has undefined behavior. If it multiplies and there's an overflow, the system can do whatever it wants. But in another sense, it does change the meaning because it tells GC, make sure that when my program does something stupid, that you crash reliably rather than crashing unreliably maybe five seconds later or five minutes later, right? insert extra runtime checking, slowing down the execution of your program to make it more reliable. Any questions on how that works or what the motivation is? There's a catch though. There's a catch to this flag. For technical reasons, it can't catch every instance of undefined behavior. And in fact, there's kind of three categories of undefined behavior. One is things like integer overflow. This thing catches that. And there's a whole bunch of other stuff in this category. That's what you want. There's a second category of fairly common areas that it doesn't catch. And these are going to be things like bad pointers. There's a second option to catch this category of undefined behaviors. It's called f_sanitize=address. This flag attempts to catch subscript errors and that sort of thing. For technical reasons, you can't turn on both flags simultaneously. It's a pain, but there it is. So you have to decide which which things you would prefer to sanitize. And then, there's the third category of undefined behavior, which for now I'll call everything else. This behavior isn't caught by either flag because it's just too painful to catch. More recently, GCC and I think Clang have also added some extra arguments like this to try to catch some of these other sort of trickier areas. All right. So one of them is going to be the following. F sanitize equals thread. This flag attempts to catch common mistakes when writing multi-threaded applications. All right. When you're writing multi-threaded applications, one of the most common mistakes you can make, is to have a race where two threads are accessing the same piece of storage at the same time. One's writing, the other one's either reading or writing, and then you get a collision and who knows what happens after that. This flag attempts to catch that by inserting arbitrary delays and extra locks into your program to see whether or not your code isn't following the locking rules. So this can greatly slow down a multi-threaded application, the race condition. And it catches a wide variety of race conditions, but not all of them. Right? So there's a common theme here. These flags slow down execution. Most likely, they'll catch stupid mistakes that you make, but they're not guaranteed to catch them all. All right? There's another one that, in some sense, isn't catching a correctness bug, is catching a performance bug. And this is the fsanitize equals leak check. This checks for memory leaks in your program. Okay? Now, a memory leak means you allocated some storage and you forgot to free it. The program keeps running. It doesn't crash. It still has the correct behavior that you want. It's just that it allocates more and more memory. Often these leaks tend to pile up, you have a whole bunch of stuff in RAM that you're not using, it turns into a performance issue and this flag attempts to catch leaks like that. The way that it works is just before your program ends, it looks to see at all the storage that you've allocated and make sure that you're still pointing to it somewhere. And if you're not, that meant it leaked. Same sort of issue. Yes? All right, yes. So I was going to get to Valgrind. Let's go to Valgrind. Valgrind does many of these checks on its own. All right. What's the difference? When you, you can take Valgrind and run it on any program that you like. Like on CSNET. You can say valgrind, I don't know, bin cat, etc. Password or something like that. This will run this command and check for leaks and check for some undefined behavior, not all, all that sort of thing. But the difference between the two is that valgrind operates purely on the machine code. instructions sitting inside vincat right so someone here there's a move on right rex p or something and it'll look at that and say oh that means we've got some storage and its address is put into the pointer p and if p is a global variable that means it's not leaking right so it It analyzes runtime behavior by looking at the machine instructions directly. An advantage of this approach is you don't need any of these special flags. Right? This will run on production code. That's a huge advantage. That means you don't have to sort of recompile your application with all these flags turned on and all that sort of thing. It means that the Applications, if they're not running under Valgrind, are running at full speed. They don't need to have all these extra sort of checks for, you know, integer overflow and all that sort of thing, because Valgrind will do it for you. There's two downsides, though, of using Valgrind. Because it's running on production code, in effect, it has to interpret each machine instruction individually. It looks at the machine instruction and says, what does this do? and then it's going to be way slower. It has a lot of performance tricks in order to make that go faster. So, for example, it will attempt to look at maybe ten instructions in a row, see that all ten of them are benign, and then the eleventh one it's not sure about, will then let those 10 instructions go lickety-split and then the 11th one it'll have to look at more carefully. So it has tricks but those tricks mean that when you're running under Valgrind you're still going to be way slower than if you ran with these flags turned on because these flags just put in the checks where they're needed you don't have to sort of have every instruction be looked at you just put in the checks as needed. Another downside of Valgrind is because it doesn't doesn't have access to the source code. It doesn't know what the instructions really want to do. So in some sense it's dumber. I'll put that in quotes. But it sort of has to be dumber because it lacks access to the source. All it can see is the machine code. It doesn't really know what the machine code is supposed to do so it has to to guess and sometimes it guesses wrong. So pros and cons to both approaches. I use both. I tend to use this one more, but certainly this one can be helpful as well. Any comments on the distinction between the two? All right. Let's take a look at some other options you can have. There's another option. I'm going to put this under a dotted line. It sort of means the opposite of what we've done before. You can use GCC with this flag. FwrapV. What that does is it tells GCC that when there's signed integer overflow, it should wrap around modulo 2 to the whatever this way that they told you about it in CS33. So in some sense GS33 folks should love this flag because it means the semantics of your program now are better defined than they would otherwise be. Arithmetic wraps around. So as a thought experiment or as a dumb question, I should ask, why isn't this last flag the default? Why do you suppose GCC and Clang don't give you nice wraparound arithmetic by default? It seems like that's what many programmers expect anyway. What's the downside? of fwrap b. Any thoughts? Yes? Yeah, this slows down the executed code, executing code in some cases. Let me give you a dumb example to show you how that might work. Suppose we have code that looks like this. Well, we'll make it even simpler. Now, can GCC, and we assume, you know, this is a type, and we assume N is some integer. And assume this is some complicated expression, so we don't know what n is. Can GCC optimize this code to be looking something like this? Is that a valid optimization? In other words, get rid of the control variable. Just call F3 times. Straight line code. No conditional branches. That's a win from a performance point of view for those of you taking CS33. Just unroll the loop. Loop unrolling is one of the standard sort of optimization tricks of many compilers. Unfortunately, if you use this flag, you cannot do that optimization. Do you see why? With this flag, GCC will have to take this code and just write it the way you've done it. And the reason it has to do so is suppose this value turns out to be int max minus 2. If n equals this value, and if integer arithmetic wraps around, then this expression is always true no matter what and this is an infinite loop and it has to be an infinite loop which means this optimization is wrong however if when you invoke GCC you don't use this flag which is what most people do then GCC can make this optimization because the behavior on integer overflow is undefined so if this I++ you know goes past intmax the compiler can do whatever it wants which means it can do this so that's why frap v is not the default even though many programmers naively assume it's the default and you're going to have to be careful about this not only in languages like c and c++ which try really hard to be efficient but also in other languages like java Java is a higher level language. It defines integer arithmetic to wrap around, which means all of a sudden you've got problems when you write code like this. All right. Let's see. What else do we have? So far, I've been talking about sort of undefined behavior. There's also this issue. Portability Checking. This kind of checking happens all the time in practical software development. I hope you are doing portability checking in your projects. You're doing web-based applications, and I hope you make sure that it doesn't work just with Chrome, but it also works with Safari and Firefox and that sort of thing. That is, your JavaScript code should be portable. to various browsers. So a simple form of portability checking, of course, is you run your code on many platforms. Unfortunately, this can be a pain. You've probably seen some of that pain even when I, I don't know, visit random websites at UCLA maintained by the UCLA administration. There'll be a little note at the bottom. The little note at the bottom says you must use Chrome or Firefox or Safari version N and greater otherwise this website may not work. Have you seen those notices? And what's happening there is that the developers of those websites, they've checked the websites on a small set of browsers and if you use Opera you're out of luck. They didn't have time to check it. So although this is a nice thing to do, there's some cost involved. Particularly once you start worrying about combinations of features. You're running Safari on an old version of macOS and it has plugins X, Y, and Z. Oh, maybe you should have plugin W. There's an exponential number of platforms, possible platforms. So this can be quite expensive. It would be helpful if you could do sort of how shall I say it portability checking sort of for all the platforms at the same time unfortunately we aren't there this of course also occurs question sure yeah yeah yeah yeah yeah and I'm glad you picked that problem to solve rather than the halting problem. All right. So this sort of thing also happens more in low-level software. If you log on to CSNET and build there, you can use either GCC or you can use GCC minus M32. Or if you're really, well, there's the, we'll just stop with these two. All right. GCC ordinarily generates 64-bit code. suitable for x86 64. But you can also generate 32-bit code designed for the x86. And you can run 32-bit programs on C-Stats machines because, like most Intel chips these days, they'll run in either 32-bit mode or 64-bit mode. And there's a chance that the 32-bit code would run faster because it's dealing with smaller energies. This is a simple way to do portability checking. Just build your program without this flag, build it with this flag, it should work either way. That's at least one simple way to check that your code will port to a wide variety of, say, embedded systems. A lot of the embedded world is still sitting in the 32-bit sort of model. All right. Any questions about this kind of checking all right well I Said static checking I said dynamic checking we've been talking a lot about a lot of dynamic checking models It's finally time to bite the bullet And talk about actually using a debugger I've been putting this off for like two or three hours worth of lectures because I hate using a debugger And so I'm going to start by reminding you, right? GDB and other debuggers are an inefficient way to find bugs and fix them. Well, maybe you are. An efficient way, certainly, to find bugs. You really want to not get here. Try to do something else. It should warn you that in badly run projects, this will take more than half your time. Basically, you're throwing all of these very highly paid software developers. They're making 350k a year. Come on. You're having them debug programs? You've got to be kidding me. Don't do that. These expensive people should be spending their time making sure that the programs don't have the bugs in the first place. So what you want is you want to prevent the bugs from happening in the first place. And we've talked about some techniques to do that and in my usual procrastinating way I'm going to talk about a few more before we get done. But there's another thing that may be less obvious. In any sufficiently large program there will be bugs. There's just no way around it. There's no perfection in real-world software. So what you want to do is knowing that your program has bugs, you want to make sure that those bugs don't wipe you out. You want to sort of minimize the bad effect of bugs. That's another thing you'd like to have. The bugs may be in the program, but you don't want that to sort of prevent the program from working. And another thing, and this may be even less obvious is you want to make the bugs easy to detect. Too often bugs in programs are sort of latent. They're there. You don't notice them. Maybe you don't notice them because you tried to minimize their bad effects. They're still there and then all of a sudden kaboom a bug really bites you badly. Right? It would have been better off if you detected the bug in the first place and the program had reliably done something rather than sort of crashed your application. All right, so what sort of techniques do we have for this? We have static checking. We have dynamic checking. What other techniques here? I should mention we have test cases. When you're given assignments in CS 31 and 32, and too often even in this course, test cases are assumed to be drudge work, or they're assumed to be, "Oh, the professor wanted me to prove that my program's working. I'm going to supply a test case that shows my program's working." You have to fall out of that temptation. The whole point of writing test cases is to find bugs in your program. If you write some test cases and the test cases all pass in your program, your reaction should not be, "Yay, my test cases are working." your reaction should be I wrote these test cases and I didn't find any bugs I failed. Right? That's the goal of test cases. Right? Sometimes in some organizations there are different sort of developers or sort of devoted just to writing the test cases and they're in a different part of the organization and in some sense they're competing with the main developers. Their goal is to prove that the main developers made mistakes I'm not sure I like that attitude but on the other hand I very much dislike the attitude of people writing test cases just to prove that their code works that's just that's just a total waste of time I can write test cases to prove pretty much any program no matter how bad works don't do that write your test cases to break the program so that's sort of one thing right this This is your goal. Make sure your test cases actually find bugs. Now, of course, you could be tempted to purposely put bugs in your program, but don't do that either. Another thing about test cases is you need to have a good infrastructure for managing the test cases. You need to sort of have organized test cases. In the simplest form of this organization, you might have just a shell script or some other script. And it'll be called run tests or something like that. And what it'll do is arrange to run all the test cases and make sure that they succeed or fail and report back and all that sort of thing. And that's a reasonable start. What you'll find in a sufficiently large application, though, this This isn't good enough. First off, you're going to have so many tests that you don't want to simply run the tests in sequence. These should be run in parallel. At least running on your multi-core machine, get all those cores working. Run all those tests. If you have a lot of machines available, run the tests on all of them. run tests script in mind. Another thing is that you will find that some test cases are simply so expensive that you can't run them all the time. You can't afford to run. They just take up too much CPU time. So you may have some options here to run subsets of your test cases. You'll have cheap test cases that you'll run all the time. Maybe after every commit. Why? Why not? They don't cost very much. The more expensive test cases you'll have to defer for, you know, run it on the weekend or something like that. And you'll need to have this sort of thing managed reasonably well. Any questions about sort of test cases in general? Right? In your assignment, I believe the script is called make check, right? It's a make file rather than a script, but it's the same basic idea. If you were to take these ideas and implement it in this framework, what would you do? Can you run your test cases in parallel? You should be able to. Make has an option called minus J. You can say - J 10 or something, right? That means run the commands in order to do whatever you're doing and run at most 10 of them in parallel, right? This means use up 10 cores on your machine rather than one. So you already, even in the assignment, sort of have the ability to do this. You also may need the ability to do a subset of the tests in case some are cheap and some are expensive. All right, any questions on test cases? Did I leave off some good ideas for test cases? It's kind of an unpleasant subject, right? Nobody likes to think about test cases. You're thinking about failures. And yet that's what engineers have to do all the time is think about how software can fail. No thoughts? Anybody here have good ideas for this test case? test case stuff being easier to do or more reliable? No good ideas? Come on! How can you do better with test cases? Has anybody read the assignment? I read it. Is there anything in there about test cases that I haven't talked about? Yes? Use an LLM to do test cases. Is this a good idea? It's funny because I just read a short note from a senior engineer at Meta where they've tried sort of various projects at Meta and using LLMs to make their software developers more productive. Or another way to put it is so that they don't have to hire as many software developers to do the same amount of work. And in these projects, they varied the amount of LLMs that they were using. Are you going to use LLMs to displace 20% of your work, 30% of your work, 40%? Are you going to have the LLMs write 80% of your code, 90% of your code? They tried to have a little meter, a knob that they turned, depending on the project. And what they found, at least what this engineer says they found, is that the optimal value was so somewhere in the 40 to 50% range. If they tried to have the LLMs do more than that, then they spent so much time dealing with the resulting hallucinations that their productivity actually went down. And if they only used LLMs 10 or 20% of the time, they weren't actually getting rid of as much of the busy work as they would have liked to have gotten. something like that. They also found in their work, according to this senior meta-engineer, that test case generation was a place where LLMs really shine. Boy, was I happy because I wrote the assignment before this guy published this stuff. So you should be in a sweet spot for LLMs for this assignment. That being said, it's somewhat of a a difficult area. Because it's easy to write test cases for a deterministic program. If you're writing a program to sort something, there's only one answer. It has to be sorted. It has to have the same stuff as the input, all that sort of thing. You're writing test cases for a random number generator? How do you do that? That's not as obvious. So you're going to maybe need a little bit more help from the LLM than those meta engineers did when they were writing test cases for some random website. All right. Still no comments or questions? I'm doing all the commenting here. This isn't fair. Yes? When did the meta engineer post this stuff? I heard about it yesterday. I'm not sure when the post actually was. And he didn't give any detail. You know meta is kind of closed mouth. And to some extent you have to be careful when people say this stuff because you know they're trying to say meta is the world's smartest place and all that sort of thing. So there may be a little puffery going on. But to some extent I believe it. What's that? Oh yeah they're using you know all of the meta LLMs. Absolutely yes. Yes. Well that's That's another thing in our assignment, right? We have, what, three major models that were suggested? None of them from Meta. Did you notice that? All right. The argument being that the big three that we mentioned should do a better job in doing test cases than smaller models will. Although, I mean, do we really know that? No. I mean, to some extent, if you'd like to use somebody else's model, just ask the team PA in charge of the assignment and I think it's Elaine she'll give you permission any other questions about test cases before you guys should be talking about test cases you're going to spend like 30% of your professional career doing dealing with test cases now is the time to ask questions about it boy I got saw a lot of long faces after saying that no comments it's all easy stuff Trust me, it's not. Yes? How would you create a test sheet for a random number? If you go talk to a math professor and you say, how can I test whether a set of numbers is random? The math professor will tell you it's impossible. There's no way to do it. You cannot prove that something is random simply by sampling from it. It's just, there's just no way. So in some sense, in the assignment, I'm asking you to do what's mathematically impossible. That being said, it's a practical assignment, not a mathematical assignment. So, in software development, when you write test cases, and when the program passes the test case, all the test cases, let's say, does that prove that your program is bug-free? No. All it means is that you pass the test cases. Unless the tests are exhaustive, which is only possible for trivial programs, the test cases simply give you greater confidence that the code is correct. That's all that it does. That's what we're asking for in homework 6. You're not going to prove that you've written a truly random number generator. That's impossible. You're just trying to increase the probability that what you've written actually is sort of random. Okay? So that's part of what's going on here. And I hope you take that into account when you talk to your LLMs and try to get test cases, right? Does anybody want to share thoughts? High-level thoughts. Don't tell me exact code. Thoughts on how to check whether a random number generator is truly random. Any thoughts? Yes? Right. So you check to see if... Take a look at all the bits that you generate. About half of them should be one. Half of them should be zero. If the percentage is way off from that, and you can use all of that statistics that we make you study to figure out what way off means here. Then you can say, oh, this doesn't look very random. It might still be random, but the probability is very low. So that's one thing you can do. All right. So here's my random number generator. Right. Every time you call it, it returns this number. We have you do what? 64 bit ints. Is that right? I forgot. I think it's 64. Right. So int uint 64 max. Return that. Now, if I did that, of course, that would take all the bits and turn them on. And so we really don't want to do that. What we want to do is just turn on every other bit. So let's see. How do we do that? What? We divide by three, right? I think that does it right? If you divide by three, then you get a 0 1 0 1 0 1 0 1, right? Right, and so this will pass that random number test that you just gave me Right, so it's it's a test But it's certainly not the only test you want to do right so one way to think about this assignment is suppose some evil software developer has written a bad random number generator and they're trying to fool your test cases. Right? Can you defeat them somehow? Right? So the idea is you have some insider who snuck in to Hillsboro, Oregon in the middle of the night 10 years ago and changed the microcode inside the x86 CPUs so that it looks pretty random but it's not really random. Can you catch this evil developer who snuck into Intel? Right? And obviously just the simple test that was specified so far isn't enough. You want something better. So ask your LLMs that. See what they come up with. All right. Where were we? We were talking about test cases. All right. This is a good time to stop. Why don't we stop and we'll talk about defensive programming after this. All right, well, we're almost ready to start talking about GDB. But before we do that, I have a little bit of terminology. And part of the problem here is that in computer science, it's notoriously unruly. Different software developers and different professors use different words to mean essentially the same thing or different things for that matter. So I'm going to tell you the terminology I'm trying to use here, but just keep in mind that if you talk to other professors in our department or other sources, you might see different terminology. First off, we have the word bug, right? And bug is sort of a generic term in my terminology that can stand for one of of three different things. The first thing is an error. And by error I mean a mistake that you the software developer made. It's a mistake in your head, right? You misunderstood what the user want or you misunderstood how JavaScript works or you just screwed and you wrote down, you know, you intended to do things in decimal, but you're doing it in hexadecimal. My mistake. It's just a mistake in your head. A fault is the consequence of an error that appears in the source code to your program. So it's a coding sort of, it's a latent problem. in your source code. Right? For a fault to occur, you had to have made an error. But not every error leads to faults. It could be that your mistake is simply thinking of the wrong thing when you write down a comment. The comment is wrong. The code is right. You made an error, but there's no fault. Right? That's possible. Usually though, when you have an error, you're going to have a fault. The third kind of bug is called a failure. A failure occurs when a fault is triggered at runtime and the user notices that there's a scrub. So a failure is an observable of mistake at runtime. For a failure to occur, you have to have a fault. But not every fault leads to failure. Maybe the fault is in a line of code that never gets executed in this particular run, in which case there's no failure. So when you are debugging, typically, not always, you're debugging because you got a bug report. The bug report will report symptoms of a failure. That failure will have been caused by a fault. That fault can be traced back to an error that somebody made. But the user doesn't know any of this stuff. All the user knows is the symptoms. The program printed out X, it should have printed out Y. So your goal in debugging is typically, not all, always, but typically to trace backwards from the bug report, which only talks about symptoms, to figure out what the failure was that caused those symptoms, and then to work backwards from that to what the fault is, which is the mistake in the code, and then often even to go further back, figure out what the underlying error was, because it's often the case that a single error can cause several faults, and you don't want to fix just the one fault that caused this misbehavior. you want to fix them all. So that's, this is all really common sense stuff to anybody who's debugged serious programs. I hope I'm preaching to the choir here about how to think carefully about what we mean by bug. But of course, you're going to see phrases like this used informally and people will sometimes when they say failure, they really mean error and vice versa and all that sort of thing. For example, the whole idea of having an error message is a nomenclature failure if you believe in this nomenclature. An error message isn't talking about errors. It's talking about a failure. It should be a failure message. But oh well. All right. So when you're debugging, you start off with the symptoms. All right. And so here are the steps you should take. about the steps not to take. In CS31, when you write a simple program and it doesn't work, here's what many students do. They take the program and they say, I think it's line three that's wrong. I'll change that 10 to a 20 and I hope it works now. I've been there too, right? You're lost. You don't know all of what's going on. You know that the solution is only five lines long because this is an easiest assignment. So you just keep trying lines at random until it works. That approach actually isn't too bad when you're doing really simple stuff. But it does not scale. And I think it should be obvious to everybody in this room, it just doesn't work. Don't change lines of code at random in your program hoping that it will work. You will waste your time, you'll waste the time of everybody else in your team. Right? So step one is not change the sort and by that I mean I want to find a way to reproduce the bug reliably. If you're writing a simple CS31 program it's pretty easy. Here's the test case it screws up. I've stabilized the failure. If you're trying to debug a big multi-threaded application with a timing problem, this part of debugging can often be the biggest job you'll have to do. Because you'll have a bug that only occurs once out of every 10,000 times when you run the program. How are you going to stabilize the failure? It failed for the user once. The user ran it again, it worked the next time. And it worked the time after that. So you're going to have to maybe spend a lot of time doing it. this. One trick here is to use the kinds of ideas that GCC does with sanitize equals thread. Look for here for ideas. What it does is it basically puts in a lot of instrumentation code into your program and a lot of slowdowns and a lot of sort of arbitrary locks. It tries to execute the program. even though it's multi-threaded in a particular way. So that every time you run this multi-threaded program, it's running in parallel, but it'll always do things in the same order. So slow down the program to make it behave in a reproducible way, and you use that to stabilize the failure. But this is an art. It's not a science. I'm not saying it's easy. It's just a lot of times this is going to be a big part of what you do. Once you've stabilized the failure, Once you can reliably reproduce the bug, then what you have to do is this sort of backwards reasoning. You have to reason backwards from the symptoms to the fault. And ideally, even back to the error that the developer made to create the fault in the first Step two can also take a long time step one when it takes a long time it's a disaster but even step two even if you can reproduce the failure it can often be not obvious at all as to what the bug is maybe the program is doing something heavily numeric and you're not a numeric expert that's actually a problem I wrestled with last week I had to go read up on lots of high-powered numbers stuff right this kind of reasoning can be trickier than it looks like All right, so the idea here is that debuggers like GDB are helpful at doing this sort of thing. You use them as reasoning tools. They help you think through how a program behaves. They're not, a GDB doesn't fix any bugs in some sense. It's not a debugger at all. you're just using gdb as a way of exploring program executions right we should this should be something called something like program execution explorer that sort of thing and maybe i even put an s here right the idea being that you've got a big program it's got a bug The program executed tons of instructions in order to get where the bug is. It went through a lot of ifs and a lot of whiles. There are a lot of choice points where it had alternate ways to go. In effect, you're exploring an exponentially large search space of possible executions in order to figure out why this particular execution caused the bug. So you really should be thinking that way with GDL. You're trying to sort of figure out why this execution or why a family of executions is the wrong one. All right. So here's how it works. GDB is a program. When you sort of, you know, sign into CSNet and run GDB of, I don't know, let's run GDB of bin cat, whatever you like. What happens here is GDB will start up. Here's the hardware. The hardware has an operating system kernel running atop it. GDB is executing mostly instructions, but occasionally it talks to the kernel. But there's another program involved here, which is this CAT program. When you start up GDB, CAT isn't running. It will give you a whole bunch of other stuff like here's how to get help and all this thing eventually it'll give you a prompt at this point all that's running is GDB and then the operating system and is sitting there and all that sort of thing what you can do though is you can tell GDB run once you do that GDB will arrange for cat to run in this case cat It could be any program, but it's running under GDB's control. In some sets, GDB is CAT's parent. When CAT runs, it's also executing machine instructions and occasionally system calls. But GDB has control over CAT in a very special way. The way it exerts that control is by talking to the kernel. It tells the kernel, stop CAT. I don't want CAT to run anymore. And the kernel arranged for cat to stop in its tracks. It can then tell the kernel, okay, let cat start running, and then cat can start running. While cat is stopped, GDB can also tell the kernel, let's mess with cat's brain. Cat has this local variable called output file. It's a string, and the string says ABC. Let's change that string to DEF. CAT's frozen. It can't do anything. And the kernel will obligingly arrange for CAT's memory to change. And then GDB can say, okay, let CAT keep going. And CAT will then continue with the wrong value in the variable. And GDB has this kind of control over its victim process in a very fine-grained way. It can change any piece of RAM that CAT can look at, GDB can look at. Any piece of RAM that CAT can change, GDB can change. GDB can execute a, cause CAT to execute a single machine instruction and then immediately stop and freeze again. That kind of level of control is very fine grained. In effect, if you want to, GDB can arrange for this program to be so confused that it's no longer the cat program, it's now the sort program. Because it can replace all of the executable code, all of the data structures, everything. Now, you would never probably want to use GDB to do that, but you have that power if you really need it. So in some sense, this is like, how shall I say it? I want to sort of say the whole process here is self-modifying code. except it's not really fully self-modifying. GDB can modify what this process is doing, but it won't modify itself. Any questions on the mechanism for how GDB works? Now, it's very common for other processes to be involved, right? For example, I run GDB under Emacs. You may have your own IDE and that sort of thing, but then I'll be having, I'll be telling and GDB will then be in charge of telling the kernel what to do with CAT. And you can even have further levels atop that. In today's lecture, I'm mostly going to be focusing on this level. I'm not going to be talking about all the shortcuts that you can use in an IDE. They can be more convenient than the commands I'm talking about here, but essentially the way they work is they go and send these commands to GDB. All right, so the run command, which is executed so often that there's a shorthand for it, and I usually just type R, takes some operands. So I can give the run command the following operands. And what this does is it causes GDB to start up the CAT program with this argument, or these two arguments, I should say. So this will be argv1 for main, this will be argv2. And it arranges for standard input to be from the file foo. You can almost think of the run command as being like, you know, run this particular program like I would run it from the shell. Except you just type R instead of the name of the command, because you've already told GDB what the command is when you invoke GDB in the first place. And when you do this, the command will run to completion. A very common thing is to do this, and at the end, GDB will say, oh, cat finished. It exited with status zero. And whatever it did, it did. Any questions about the run command? Yes? R is short for run. And if you just type R, like this, which is something fairly common for me to do, it repeats the last operand. So you're running the same program with the same arguments that you ran last time. It's fairly common to do that because you're trying to, you've got a bug that's, you know, you've stabilized. You can make sure it happens all the time. But you're not quite sure what leads up to the failure. So you've got to run it. And then you say, oh, I have to go back like five minutes. All right, let's start over again. and then run it again. That sort of thing. So I often type our return. Yes. There's a way of doing that and I can never remember so I just exit gdb and start over. Right. Or if I'm doing cat I'll do something like this. I happen to know this tells cat please run with no arguments. Right. Whatever. Other comments about run. All right? Oh, I forgot to tell you the most important GDB command. Let's do that. First thing you should learn for any program is how to get out, right? So Q, which is short for quit, is how to exit GDB. If you just start up GDB and run QWardway, it never even starts up the program. If you type Q in the middle of a debugging session, GDB will exit and it will also kill off the program that you're debugging. And it'll probably say, You sure you want to quit because this is program that's running that sort of thing but don't forget how to quit All right, let's see what else There are many commands that you might want to run before you issue the run command These are sort of setup commands You're preparing the environment for running the program before you start running the program So here are some common set up. There's a whole ton of them. I'm not going to go through them all but I'll go through the ones that I tend to use a lot. You can say this. This says when you run the program run it in with this being the working directory rather than wherever we happen to be. Right. So this means the program will be running under slash temp even if we're currently in some other directory. You can also do this. Set end path. You can set an environment variable with the set end command. This is the name of the environment variable. This is its value. And you can use this to specify the environment that the command that you're running will run in, which may be different from GDB's own environment. This next one is trickier. This is set disable randomization. Since the theme of homework six is random numbers, I feel I really should mention this one. All right, so what's going on here? As a standard technique, when programs in Linux and many other operating systems run, they run with address space layout randomization turned on. ASLR, which is short for address space layout randomization. What ASLR does, is every time you sort of make a call in your program in which the result is going to be a new newly generated object right so you call malloc you call the dynamic linker and link in some sub module that sort of thing but let's just take malloc for short this says please allocate a piece of storage containing a hundred bytes and give me a pointer to that piece of storage, right? If ASLR is enabled, then the runtime tries as much as possible to arrange for this pointer to be as random as possible. It tries to lay out your objects all over the address space in sort of random locations. If ASLR is disabled, the system doesn't do that. It tries to make sure that the You know the pointers that it gives you are relatively orderly right what it'll do for example for malloc is it'll give you a big it'll sort of first allocate a big hunk of storage and then just carve off the next piece of storage that like this right if you allocate several times you'll get objects right next to each other. Why do we want ASLR? It's because our programs are buggy. They have subscript errors. in them. Our programs are being attacked by bad actors who try to find these bugs in our programs before we find them and try to exploit them. The way a bad actor exploits a subscript error in your program is they give you your program data that causes it to maybe trash this object because we're saying a sub i, a points to here, i is out of range. we trash this piece of storage that causes a bug in your program that the attacker exploits. And in order for this to work reliably, it's very nice if objects are laid out in increasing order in a predictable way. That way the attacker can predict where objects will be and it will be easier for the attacker to find victims and override exactly the victims that they want. you're running on Linux with ASLR. Objects are scattered all over the place. The attacker can't predict where the victims will be even if the attacker finds a bug in your program. And this object will be accessed out of range because it's found that bug. It won't know what other objects it's trashed because the objects were laid out at random. Therefore, ASLR is a defense mechanism against this common sort of bug. But there's a problem. If you have ASLR turned on, you are fighting against the first thing I told you in debugging. Stabilize the failure. Make sure your bug is reproducible because you're now randomizing memory allocation, which means the bug might happen in this run. It won't happen in the next run. So ASLR fights against debuggers and for that reason GDB by default turns it off. When you're running CAT under GDB normally GDB will say let's turn off ASLR for you and that way if there's a bug in CAT every time I run CAT I'll see that same bug over and over again. So, why would you want to do this? This is going to make your program harder to debug, because it won't be as reproducible as it would be by default when running under GDB. Well, it could be that you've got a bug in your program that only happens when ASLR is turned on. I've run into that situation. off. Oh my goodness. Did they tell you about this stuff? The people that took CS33, did they tell you about this stuff? Address-based layout randomization? All right, well, now you know. Okay? It's a standard technique. Pretty much, you know, Mac OS, Linux, all these guys use this stuff. It's a pain for debugging. Now you know how to turn it off. And to some extent, this is a common theme in the sense that when you want to debug something, thing. Often you have to turn off safety features that are already present in the hardware or software because they get in the way of debugging. And then you have to be careful because if you're running a program in a special environment it may behave differently than it did in the original place where it actually illustrated the bug. All right. Let's see what else. We were starting up GDB. All right. I told you how to there's another way that you can arrange for GDB to be in charge of a program. And this other way is controversial and doesn't necessarily work on different Linux boxes. But I like controversy, so I'll tell you about it. You can run this command. This number here has to be a process ID. It's a process ID of an already running program and that already running program should be cat that somebody has already started up and cat is running and what you want to do is debug a process that wasn't started up under a debugger but something has gone wrong with it you're trying to figure out what's going wrong with this program that's been running maybe it's been running for months so you would like to run this program temporarily under a debugger So this cat here isn't necessarily a cat that GDB started. It could be a cat that you started 10 minutes ago. And now you want to attach to it and start debugging a program in the middle of its run. When you do this, the program immediately stops. It stops that process. And GDB is now in charge of the process. And since the process is stopped, you can now mess with the process's brain by setting its variables or looking at its variables more likely is what you do or you can single step through it, all that sort of thing, just as if you started up the program directly. This capability is very helpful for debugging production code, but it's also controversial. So, for example, in Ubuntu, this capability is turned off as a security The worry is that if an attacker can break into your system, the attacker then can start running GDB as you, let's say, attach to one of your processes and then interrogate its brain. Maybe somewhere in the brain of that process is your password and it will find out your password. in the kernel if you like and Ubuntu has disabled it by default. But I believe on CSNet it's turned on so you can just attach to your own programs if you like. Question? Right so yeah so so far I'm assuming that GDB and the program are you know are on the same computer. You can have remote debugging and you can even attach to a you're going to need you know usually that capability is turned off you're going to have to go into the kernel of the remote machine and enable that sort of thing because that you know obviously that's a very powerful thing to give some random foreigner the ability to do. Question. You monitor who's logged into your machine very carefully. What can I say? How would you protect against that? Yeah, I mean, this is why it's controversial, right? Because you really are kind of leading, how should I say it? Once the attacker can become egert on, say, one of the CSNAP machines, they can look at all my files, that sort of goes without saying, but they can also examine all my processes with this capability, And the problem that I think they're mostly worried about is slurping up my keys. So if I'm using SSH, they can get a copy of my SSH secret key, not the public key, and all that sort of thing. And so, yeah, that's a definite worry. The way you protect against it is if you're root on CSNET, you can turn off that capability for people if you like. We have decided at UCLA that you guys are all adults. You can only use GDB on your own processes. You can't attach to other people's processes. So you can only screw up your own stuff and we kind of trust you not to, you know, screw up your own stuff and we trust you not to let attackers log in as you. Our trust occasionally is misplaced and in that case the ops staff has to go clean up the mess. All right. Other comments? All right. So we have, what other commands do we have? Oh, I should mention how do you reverse that? You can probably guess what this command does. This command says, I don't want GDB to be attached to that other process and sort of second-guessing everything it does. I want to let that process go free-run again now, as if GDB had never sort of come in contact with it, except, of course, if GDB has fiddled with that other processes brain, that modification is still in place. Alright. Next set of commands is going to be related to the following. Most of the useful things that GDB can do to a process while it's debugging, it can only do when that process is stopped. And there's good reason for that. You don't want GDB to be messing with the other processes brain while the process is updating its own brain. There's too many race conditions involved. So a good chunk of GDB is having to deal with how to sort of stop the other process in a nice, reliable way. And the standard way of doing that is something called a breakpoint. There is a command in GDB called break, which I normally just type as B because it's so commonly used. In fact, why don't I just and what you type after the B is the location of where you want to put the breakpoint. Let's say for example you have I don't know some complex square root operation function or something like that. You could say something like this. Break on the complex square root function. This means that whenever any part of your code calls that function that your program will stop. and GDB will regain control. The program is now frozen and GDB can go look and see what were the arguments of this complex square root function and that sort of thing. Right? So this basically says plant a breakpoint there. You can specify here either function names or locations in the source code. Right? You can say put a breakpoint at random.c C line 963. Very commonly, you don't type the breakpoint command. Your IDE does it for you. You use the mouse. You say, I want it to stop here. And what your IDE does is it turns it into one of these breakpoint commands. You can plant several breakpoints in your code. You can have dozens, hundreds of breakpoints. GDB keeps track of them all. and gives them all numbers. And you can find out what your breakpoints are by typing the command infobreak, which can be abbreviated IB. This will list all the breakpoints and also extra information like how many times each breakpoint has been hit and all sorts of other good stuff like that. Personally, I find that once I have more than like two dozen breakpoints, I get lost and then I start wanting to delete the breakpoints which you can do with the command D short for delete and you can say delete 14 that deletes breakpoint number 14. Now how does the breakpoint stuff actually work? Well here's one way GDB can implement a breakpoint. When you say BCCSQRT or something it picks a location, right? You've got a bunch of machine code here. Somewhere in here is machine code for the complex square root function. This is a series of bytes with conditional branches and all that sort of thing. What GDB can do, since it has control over all the memory of the program it's debugging, it can take this first byte here and say, let's temporarily replace that byte. with an invalid instruction. I'll just write the trap instruction. Every time the hardware executes this trap instruction, your program breaks. And when the program breaks, the kernel informs GDB, hey, the cat program broke because it executed this invalid instruction. And now GDB has control. In order to resume execution, all GDB has to do is replace this single byte with the actual value that it should have been there and then it can let the program run later when it wants to but in the meantime program is stopped see how that works isn't that very cool so that explains why GDB can have as many breakpoints as you like because it keeps track of what the instruction used to be all right any questions about breakpoints once the program has stopped you can examine stuff and all that sort of thing and then you can type the C which is short for the continue instruction I'm sorry the continue command which tells GDB okay let the program run again at which point it will replace this trap with sort of the ordinary instruction and let the program continue to run more commonly though you'll execute a different kind of instruction you'll be interested in this particular and so what you can do is issue either the step or step instruction command this is short for step or step I step I says please execute one machine instruction and then stop right away again it's like putting a breakpoint on the very next machine instruction step is more complicated STEP says, please keep executing machine instructions until the line number changes, then stop. This is more likely what you want, but you got to be careful about the STEP command. because when you execute optimized code, it's often not, it's very common for the instructions that are executed to not be executed in the same order as the source code. That is, you can have source code that looks like this. to machine instructions partly from this and partly from this. So the order of execution at the machine level is different from the order of execution in the source level. When you issue the step command, what can happen is that, okay, we went from here to here in the machine code. The line number changed, so we stepped from here to here. You issue another step command, and they'll say, oh, now we're back up to here. So from the source code's point of view, you're executing here, then down here, then up here, then down here. And so the step command won't necessarily work the way you thought it should. Yes. Yeah. Typically, if you compile with minus O zero, say, please don't optimize, it doesn't do this sort of thing. Question. Yes. So this basically talks about source So what the compiler will do is it will put into the executable not just the machine instructions that you need to run but also a table for each machine instruction what line number it came from. And GDB will consult that extra table that's sitting in the executable as well as the actual stuff that you're executing. You may have to use the minus G option to generate these tables generate debugging tables and there's various flavors of debugging i typically use g3 because that had gives me more debugging information but the point is you have to help the debugger with this auxiliary information in the executable so that can you know do a reasonable job for commands like that So, step says basically, if the line number changes, you know, let me know. There is a variant of this that's fairly commonly used called next or next on. Oh, right. This command acts like step, except that if you call a function, it just lets the whole function execute. rather than step into the function. So if you're using the next command to debug this, you don't sort of see what's going on inside f. You just treat f of y as being sort of a single built-in operation, and you can debug and go straight to the call to g, that sort of thing. All right. Another related one in this same category is the command which is short for finish. This means keep executing until the current function finishes, then stop. It's like putting a breakpoint at the return address of the current function, a temporary breakpoint. I find this to be very helpful. And it will also tell you what the function return. Any questions on stepping? Yes? No, it stops right after the return instruction. So, yeah, you'll end up in the caller just after the point of the call. Other comments about these commands? Now, I should mention that once you get into this level of debugging, you're kind of lost in the weeds. It's very easy to be lost in the weeds and also to fall into the temptation of thinking that you're actually being productive. by single stepping through a program. Trust me, you're not being productive. If you are executing a lot of commands like this, that means you've probably made a big mistake earlier on in your development. Sometimes you need to do it anyway, but just be conscious of that. Now let me talk about some fancier things, more experimental, but you should know about it just in case you need this extra power. about single-stepping and continuing and that sort of thing. But GDB also has this very interesting command called RC, which is short for reverse continue. C means, okay, let the program keep going from here until it exits or it hits a breakpoint or that sort of thing. RC means take the program back in history. to the previous breakpoint and then stop there. Go back in time. And RC can be very helpful when you're doing step two. Because step two says you're trying to reason backwards from the failure systems, which may be crashed due to a bad pointer, backwards to figuring out what caused that bad pointer to exist in the first place. So this is very helpful, but if you know anything about how machine code actually executes, you should be sort of questioning how in the world can this possibly work? Because real computers can't go backwards. They can only go forwards. Yes? and then try to put a breakpoint a little bit earlier than you did last time. And this says, no, no, no, you can just go back. So there's a catch. And the catch is the only way this can work is for GDB to keep snapshots of what the state of the machine looked like earlier. So in order for RC to work, GDB has to run the program in a special mode in which it's constantly keeping track of every change the program makes to the state, so that GDB can undo it if you want to go backwards. So this command works only if you start up GDB with a special option. Now I've forgotten what that option is, but you can look it up. And what that special option does is it slows down debugging enormously because GDB now has to keep track of everything your program does because it has to keep a complete history. Although it's a very valuable command, you know, it comes at a cost. And oftentimes you can't pay the cost because the program goes on for a long time and that sort of thing. But when it works great, I've been, you know, when it works, I've been told it can be very helpful. Another fancier command, and this one is a bit more practical in my experience, I use this more often, is the watch command. You can specify any expression here. And what GDB will do is it will place a watchpoint on that expression. A watchpoint basically has the following property. If the value of this expression changes, the program stops, just like it was a breakpoint. So you can say something like this. Watch A less than B. If A is currently less than B, right? The program will keep running, but the minute that the program changes A or B in such a way that makes this expression false, then the program just stops right away. So, for example, if you know that the program has a bug, and the bug is that somebody is incorrectly setting the value of a global variable to the null pointer when it should stay non-null, you can say something like this. Let's watch that pointer. And the minute the code steps on the pointer and changes it to be anything other than the value that it currently has, GDB will stop and you'll see who the guilty party is. This can be a very useful thing to have. Now, there's a bit of a downside to the watch. I explained how breakpoints can be implemented efficiently. When code is running and you have a lot of breakpoints, the code could run at full speed. As long as it doesn't hit a breakpoint, everything's very fast. So breakpoints are efficient. Watchpoints are going to be trickier. Because what does GDB have to do in general with a watchpoint? It's going to have to single step your program in the worst case. They'll have to tell your program, execute one instruction, then it will wake up say oh yeah it hasn't changed execute another instruction although that works that's going to slow down your program by a couple of orders of magnitude luckily many cpus have limited support for hardware watch points for example the x86 64 has four sort of hardware watch points and by that i mean you can tell the hardware for memory locations and you can tell the hardware if that memory location changes its contents I want you to trap and the hardware will support that so as long as you only watch four variables GDB can arrange for the program to run at full speed until the variables change then it'll stop and take a look at what happened so this sort of watch a watch point command can be quite helpful as long as you you sort of know the performance characteristics of the machine that you're looking at. All right. Any questions on watchpoints? Let's do something even bigger. Checkpoint. This command I don't use very often, but you should know about it. The checkpoint command is it tells GDB, Take this process that you're running. Find all the memory that it can access. Here's all the RAM. Here's all the registers. It's the current state of this process. And save it into a file somewhere. So now we know the complete state of this process. And there is another command. So it'll give you a checkpoint and tell you this is checkpoint number nine and all that sort of thing. You can say restart. and where n is one of these checkpoints and that causes gdb to reverse the process it takes the save the file and it uses that to specify everything that's in RAM and everything in the registers so you can then restore the old state of your program in some sense checkpoint restart is a more economical version of RC because here you're deciding where to when to create the snapshots RC is creating them all the time right so this cheaper than doing this but will give you some of the capabilities of reverse continue all right question oh yeah if you restart from the wrong program oh yeah it's a total disaster yeah all right let's do one more command and I've saved the most commonly executed command for last the print command which is P for short. It has a whole bunch of options and that sort of thing, but basically you can put any expression you like here. And GDB will evaluate this as if it were a, you know, a bit of C code and then print out the resulting value. It will do that by inspecting the memory of the program that it's debugging. And it knows where all these variables are because the debug information tells it that. But there's something else that GDB can do. with the print command. It could do something like this. Where f is a function defined in your program. That is not only can it look at data, it can call functions in your program. And the way it does that is it temporarily takes over control of the program's instruction pointer and says, please go. So we are sort of hijacking the program to call f instead of what the program would normally do. When that function f returns, then GDB says, oh, I see now what's in the return register, and then it will use that to print out the value. When you then continue, it arranges for the program's instruction pointer to be the way it was. So what this means is you can do something more extreme like this. The exit function in C causes your program to exit. This says, "Please call the exit function." Which will cause your program to exit. And the P will say, "I'm sorry, it didn't return a value." That is, you can use the print function to cause the debug program to execute whatever code you like. Which is a lot of power. Be careful if you use this kind of power. Alright, any comments? Alright, have fun debugging for the assignment and we'll talk about something different next time.