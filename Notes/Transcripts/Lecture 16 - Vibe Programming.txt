As mentioned in the email to the class we have a special treat for you today. A guest talk by Professor Kerry Nachenberg who some of you may know and others will I guess be introduced to you. He wants to talk about VIBE programming to sort of bring us up to date with some of the latest things that are going on. So without further ado here's Kerry. Hi everybody. So I unfortunately do not have my clicker today. So I'm going to stand with my laptop. And this is going to be, hold on one second. This is going to be a demonstration based lecture with some slides. And you know what happens with demos, right? Demos never work. So that actually might be a good thing or a bad thing, depending on how you look at it. But we have knots here. So I just can't, I have this OCD with knots. I can't have the knots. Come on. Did I get it? Oh, I just made it worse. Okay. We'll just have to tolerate it. Okay. All right, everybody can see what's going on there. How do I get rid of this? I hate Zoom, I must say. It disturbs me fundamentally. Go away. Go away. Go away. All right, it won't go away. All right, that's okay. All right, so how many people have heard of Vibe Programming? Okay, great. Well, we're going to do some Vibe Programming. We're going to talk about what it is. We're going to talk about how it works, and we're going to actually do it live. in which case we might end early or it might be really fun and we might go the entire two hours we'll see what we can do. So before we start let me show you a little program that I've programmed on my own last week. Hold on one second. Oopsie I don't want you to see that yet. Okay so let's see here. Let's turn the volume up maybe this might cause a problem. So here we have Pac-Man. Yeah, you have a question? The zoom is muted? Yeah, we want to mute because I'm using this instead. Okay, so what you're about to hear, let me step away so we don't have feedback. What you're about to hear is and see is a Pac-Man game which I wrote. It took me about four hours with no coding whatsoever. Okay, so everything was done by large language models and it was just sort of a fun project because this is what I Okay. All right. So, hold on. You can see, you can actually create some pretty impressive stuff. I mean, that Pac-Man game, all the graphics were created by an LLM. Those are vector graphics. So it actually figured out how to draw, you know, like circles and ghosts and so on, you know, the Pac-Man with a little wedge out of it. It figured out how to do A-star pathfinding for the ghosts to get back to their pathfinding. which you've learned probably in CS32 maybe. It did all of that and I did not write a single line of code nor did I debug a single line of code. So it's pretty impressive what you can do. Okay. And we're going to be doing some Vibe programming today. So we'll be taking questions as I go and I'll just let's just start. So Vibe programming the term was coined by Andre Karpathy who's a luminary in the AI space and he writes just in case you want me to read it. There's a new kind of coding I call vibe coding where you fully give into the vibes embrace exponentials and forget that the code even exists and that's the promise of this and I would say right now it's not quite there I shouldn't have required four hours to reduce that Pac-Man game I would say about three of the hours I took was debugging and when I say debugging I mean having the large language model debug for me and root cause the problem okay but it's getting there it's much better than it used to be. Yes. What did he mean by embrace exponential? You know, I think what he's saying is that artificial intelligence, and I don't know for certain, but artificial intelligence is improving at an exponential rate. Okay? And so, for instance, the amount, if you give, they've done studies, Meter did a study, where if you give a language, large language model a task, it used to be that it could maybe spend a minute on a task before we'd get confused and then and go off to the deep end. And now you can give a large language model a task that might take a person an hour or two and it will do it without going off the deep end at least 50% of the time. And every seven months that time horizon is doubling. Okay so I think that's what he means by that. Okay and by the way ask as many questions as you like. Here's a definition from ChatGPT03 which I subscribe to. It's an informal AI assisted approach where you prototype by chatting with a model Iterating code tweaking prompts and following intuition instead of detailed upfront specs. Now, this is aspirational. I think if you are going to go today and basically iterate without coming up with requirements, which we'll talk about, you'll get something very different than if you plan ahead and come up with detailed requirements, which we'll talk about what requirements are. Have you talked about that at all or not really? Okay, you'll learn that in 1.30 and we'll talk about it today. But this is where things are going. And just another sort of data point. So Y Combinator, which is a startup accelerator and incubator, if you will, has this quote. A year ago, they, which is the Y Combinator funded teams, they get some funding in terms and they exchange for equity in their startup. They would have built their product from scratch, but now 95% of it is built by an AI. So about 20% of the teams on Y Combinator, it is quoted, are now building products largely with machine learning, with vibe coding, or similar techniques. So this is real. And if you believe in the exponentials, then this will get better and better and faster and faster. Maybe it won't, but current trends are that it will. Okay, so vibe coding is a spectrum. And by the way, all the images were generated with ChatGPT or Gemini. So on the left side, we have basically LLM assistance. And you've probably all done this, right? You need to write a little program. you have to write a function you don't want to have to write that function it's not really quarter the problem you're solving so you just delegate it to a large language model it generates a function you might even generate some tests for it which is really nice because tests are the you know horrible bane of our existence and it just does it for you collaboration is I think what many people think about vibe programming so with collaboration what you're doing is you're asking the model to do something you might debug the code yourself a little bit you might jump in change some things then you ask it to generate and then there's full AI generation which is what I did with Pac-Man where there's no coding at all. I don't even know what the code looks like. I haven't written code in TypeScript really so it's all in TypeScript. Okay. And I think we're going towards full AI gen. I think where we are today is collaboration and I think if you want to be successful today building let's say a startup from this which a lot of people are doing it's through collaboration rather than full AI generation. Okay. Now, how many people have used cursor here? Raise your hands high. Okay, a lot of people actually. That's impressive. How many people have not heard of cursor or don't use it? Okay, okay. So, I, you know, before I started playing with Vibe programming, I was just using ChatGPT. And, you know, I would basically give it a prompt. It would generate some source files. And it was really cumbersome because if you're generating a single function or a single Python module, it's really easy, right? But when you have to generate a program with 20 files, include files and graphics and JSON files and so on. It gets really ugly to have to cut and paste everything. So tools like cursor, you know, and there are other ones, Claude code and so on. There's a codex from OpenAI. These tools are entire integrated development environments or command line tools that can create and work with many files. So you don't have to go and cut and paste what ChatGPT produces into a file yourself with VI or or whatever text editor VS code, it will just do all this for you. And if you need to, it will set up your development environment. It will install modules that are needed, dependencies. It will actually run tests for you and actually see what happens on those tests and debug the tests. Okay. And so if you want to generate a single file, use ChatGPT. But if you want to do anything serious, you would use something like cursor, which is what we're going to use today. Any questions so far? It's all easy stuff. Okay. Now, How many people have heard of agentic AI? Okay, and how many people know what agentic AI actually means? Okay, fewer people. So this is actually maybe the most useful thing you'll get out of today. Maybe not, we'll see. So what is an agent? Well, an agent is a system, okay, a software system that couples, so it brings together a large language model, like ChatGPT or Claude, with external tools or APIs. Okay, so basically what do we mean by external? tools, maybe a tool would be going into a calendar and viewing your calendar entries or setting a calendar entry. Another tool might be creating a file on your file system or reading a file from your file system. Another tool might be launching a process on your computer. Those would be tools that could be used. Now a large language model in its own right only generates tokens. It just generates text, right? So there needs to be some software that takes the text the large language model generates and connect it with those tools so it runs the tools are run and that's what an agent is okay so it can basically plan actions using the large language model and execute those actions using some software that interprets the output from the large language model and does something with them so instead of outputting human readable text like you would with chat GPT where it gives you an answer in English and explains everything to you maybe write some code typically we use commands with an agentic model okay so these all the large language models are designed to output JSON amongst other things. So for instance, a large language model like ChatGPT could output something like this. Some JSON that says the command is do a Google search. Here are the search terms. Okay. That would be a command a large language model could output. Here's another one. Read a file off the hard drive and send me the contents could be a command that ChatGPT or Claude could ask you an agent framework to do where it would specify the path and file name. Writing to a file would be another command. These would be things that something like cursor would do. Okay. Which is a coding agent executing a shell command. This seems to be dangerous. But you know as long as you monitor these things it's okay. So for instance make a new directory. Run a git you know a git tool. Okay. These are the types of things we could do with executing in the shell. And informing the user of something. So send the user a message to be displayed in the user interface. This would be how it would be done. Okay. So let me show you. the workflow here. Any questions about what commands are? Okay. So basically what happens is when our agent is talking to the model, it's not interacting in English per se, it's interacting at least on the way down with JSON like this. Okay. With an encoding like this. So the model is basically telling us what to do. It's sending us commands. Okay. So here's the architecture and I'll step through a simple example. Okay. So a system like cursor is an agentic coding architecture. It has three components. So there's a client, okay? And the client is Cursor, which is basically a fork of VS Code. VS Code is open source. They took it and said, we're going to modify it and make it something better than it was, okay? Now, VS Code has access to your file system, right? It can open files in your hard drive and save files and launch processes and all those good things, okay? So that's that. We have a backend server that Cursor and team operate, and they may have some models there as well. And then we have a large language model, which might be hosted by OpenAI, It might be Google, might be Anthropic and so on. Okay? Any questions about the component so far? We're good? Alright. Here's the flow, high level. So the user inputs a command which is like, you know, fix this bug in X. Now there's a bunch of files in your file system that VS Code or Cursor has access to. But right now, you know, the large language model knows nothing about your system. It just knows there's a command, fix the bug in X. like fix a bug in this function. That prompt, or basically what you just typed there, is forwarded to a back-end server at Cursor, cursor.com or whatever. And basically they augment the prompt. So what they do is, on the server, they basically take your prompt, which is fix the bug in X, and then they add to it and say, by the way, to do this, I'm going to give you some commands that you can use. Here are the commands. And what are those commands? Like the ones we just saw. So these would be included in the prompt that's sent to the the large language model. Make sense so far? Raise your hand if you're with me so far. Any questions? Okay. So now, what happens next? The large language model responds, okay, but it's not going to respond in English like, hey, Carrie, let me tell you what you could do. It's going to respond with one of those commands that we saw right there. Okay, so it responds with a command, for instance, which is, okay, I want to write a file. The file name is called foo.cpp, and the data I want to write is a function in foo.cpp. and so on. Okay. That goes to Cursors back end server. It's forwarded to the Cursor Integrated Development Environment or IDE. And then Cursor takes that command, interprets it and says, oh, the large language model wants me to modify a file. It then writes to your local file system and creates or modifies a file. Okay. So this is the overall process by which agentic basically LLM, you know, Agente systems work. Okay. Any questions so far? Okay. So let's take a slightly deeper dive. Yeah. Question. So how does it know what commands? So if you see right here, hold on, let's see if I can, I can't use a cursor. If you see, hold on, right here, it says to do so, you may use the following commands. And that dot, dot, dot is a list of commands that we want to accept from the large language model. Okay. And that was like, that was these things here. So we might list those as the five commands or four commands. There might be 20 commands that it could use. Okay. And these systems can use many different tools. Does that make sense? Yeah. Okay. Yeah. Question. Yes. So I'll show you that in just a second. I'll show you that in a second. This is a very simple example of the flow and I'll show you a more complicated example in a second. Yeah. Yeah. Right, so MCP is a system, it's a protocol, it's an encoding using JSON, for example, where you can basically specify a command just like these, but maybe a slightly different encoding. That command will go to a server, which could be on your own computer or on a server in the cloud, and then be executed by that server and generate a result back. So basically it's like using these commands, but it's an official protocol. that Anthropic developed. It's now being adopted by OpenAI and Google and others. So it's basically going to be a easy way for an agent framework to basically get things done on the internet, to log in, to purchase a plane ticket, to look at your calendar, to change your calendar, to write a file. It can be used for any of these things. That's what MCP is. Okay? Good? Okay. So this is going to be a little small. So if you do have bad eyesight, come up closer. this is an approximation of how agentic workflows work. Because it's very confusing. Until you see these things it's confusing how this works. So here are the parts we have. We have the user. We have the cursor IDE which is like built based on VS code. We have the cursor server and we have the LLM on the far right side. So the user basically asks something like find the bug in the foo function. So it just forwards this, find the bug and the foo function. That goes to the cursor server in the cloud on some Amazon Web Services or Azure server. And it then creates something that looks like this. So again, this is more detailed than what we have in the last slide. It will say, find the bug and the foo function. You may use the following commands. Command list files, command read file, command write file, command execute shell command, and so on. It says this is what I accept. All the large language models are trained to be able to issue commands like this. They are trained to do this. They are fine tuned to follow instructions. So if you talk to an LRM like this, it's not going to say, "Hey, Kerry." It's going to send one of those four commands. Period. Okay? Or three commands. Three commands. So now we say, "Give me your next command." Okay? Now the large language model will reply with that text or something because it knows, currently, I don't know what file are on your hard drive. So it says, well, tell me the list of files on the hard drive. So the LLM will send that back to the cursor server. The cursor server will forward that back to the client. And cursor on your computer will now get a list of files in the subdirectory. Got it? Everybody make sense so far? Okay. Now, I just moved that up to the top just to make space here. No fancy animation. and what it then does is it gives the list of files that are in your folder on your computer says oh there's food out cpp and bar dot cpp okay so those are the only two files we're working with that's fine okay so now this is what's interesting remember that prompt we had before where we basically said find the bug in the food function you may use the following commands and then the llm responded with command list files right remember that well So what we do with an agentic framework like this is we continue to append onto this chat session. So we will keep everything from our initial chat session there in the buffers. Okay. And we will append one more piece of information, which is, hey, the client just said the files on my hard drive are foo.cpp and bar.cpp. Okay. So then we say to the LLM, give me your next command. So when the LLM processes this, it has the context of the fact that we want to debug a problem in the foo function. It also knows there's a file called foo.cpp and bar.cpp. It has all that information. So now, what would the LLM do? If it were you, well, it would probably say I want to read the foo.cpp file, right? And it will do that. That's exactly what it does. So it'll send a command that says read file with a file name of foo.cpp. Make sense? That is then forwarded to the client. And cursor running on your hard drive on your computer is going to basically use that and read the contents of foo.cpp. Make sense? What then we'll do is it will take the contents of foo.cpp, which might be a function, you know, int foo, int x, whatever it happens to be. And it will then take all of the prompts that we had previously. That's everything we saw earlier. It's the back and forth, the up and down, everything we had previously. Okay? And it will then add, hey, from the client, the contents of foo CPP were int foo this function with dot dot dot. I won't show you the details, but there's 500 lines of code. It will ship all the code up. Okay? As much code as asked for. Okay? Then it will say, give me your next command. Okay? And the large language model will crunch on that. It has all the context. It knows we're debugging a problem with the foo function. It knows there's a file foo CPP and bar CPP. It knows the contents of the FOO CPP file. It will then say, oh, I know what the problem is. I'm going to send back a command which says write. Write file, file name FOO CPP, and it will then have a changed version of the FOO function where it debugged the problem. It could send deltas back. It could say change line 5 and line 6. It could just send the whole file. That depends on the implementation. So that is forwarded to the client and the client cursor running on your computer. So notice, this chat window gets, or this chat here is like a chat you would do with chat GPT. It's back and forth and back and forth. What happened when I did this command? Well, I got this. Okay, chat LLM says, okay, then do that. Okay, what happened with that? Paste that in, goes back and forth. Okay, any questions so far? Yeah? Well, it's not that fast, and we'll see that. We'll actually see that. It actually can be reasonably slow. But these models, especially the mini models, are pretty fast. They're pretty fast. So in the order of seconds of latency. But some tasks like this might be 15 or 20 seconds. Yeah? Well, if the prompt gets so big, they're going to just drop it or truncate it or something. Yeah? models have gotten much better following instructions and they will rarely do that. But occasionally as you're working with cursor, they will say invalid command sent from server and it will retry. So they still have problems. Okay. Any other questions? Raise your hand if you're with me so far. We're good? Okay. Now the problem with this is all this crud accretes up here, right? All of these, all of this chat back and forth as you talk to the model and it talks back and runs a command, it builds up and that crud will eventually confuse them off. And so one of the things we'll learn about Vibe programming is we need to restart frequently. You don't want to just keep building up a chat conversation, which is what's happening. It doesn't look like it to you, but that's what's happening. Okay. All right. So what do we do before we Vibe? Okay. By the way, all generated by, you know, chat GPT, pretty impressive. Okay. Although some of them are a little bit off, you can still tell. Okay. By the way, he's building Pac-Man, you can see. Okay. Okay. So before you Vibe, you want to create coding rules. So you want to provide your LLM in Cursor, for example, or ChatGPT, however you're doing this, with a set of rules to govern its code generation. So what do I mean by this? So here's a command that you can put into the rules of Cursor. It says, when possible, try to reuse or refactor existing functionality rather than adding a new code. One thing you'll find about today's models, and this may not be the case in a year or six months or three months, is they say, oh, I need to solve this problem. Let me write this function to uppercase a string, even though I already have an uppercase string function that's in a library I've already defined, and you might end up with three versions of the same function because it doesn't look and reuse what it already has. Okay? So you have to incentivize it by telling it this, hey, you know what? Always look for existing code rather than, you know, writing a new function from scratch. Okay? Here's another one. Never introduce a new technology without asking. So here's another thing that happens often. You'll be working on a JavaScript app, with Angular, okay? And you'll be like, fix this bug. It'll go like, oh, to fix this bug, I really should be going to TypeScript for type safety and using React. And it'll rewrite your entire project with TypeScript and React, introduce 100 bugs, and then nothing will work. So you have to watch out for this, and you can give it a command like this in order to reduce the odds that will happen. Because otherwise, at least today, this will happen. And you can say, you know, something like, be careful to make the minimal set of changes, because sometimes these requests like fix the foo function, might change seven other things which don't need to be changed. Okay? Now, you configure a tool like cursor to add these. There's a rule section in the configuration. You paste these in along with other things you want it to remember. And every time you send a prompt like fixed function foo, these are included in the prompt that's sent up to the server and to the LLM. Okay? All right. Okay. Now, this is probably the most important thing you will need to do if you want to Vibe program. You might think that you can just go and say make a Pac-Man game. And I actually have some versions of Pac-Man games that I did by saying make a Pac-Man game. And maybe I can show you later. But you'll come up with some stuff and they'll be sort of sort of working but sort of not working. It won't look right. It'll have problems. And sometimes you can just vibe and it'll do exactly what you want. If you want to make a simple web app with five fields and use a blue background, it can do that 90% of the time. It'll do it correctly. It'll do it entirely. or JavaScript is perfect. But if you do anything more complicated, it's going to get confused right away today. So what are requirements? By the way, notice he's sort of like this. It's very entertaining. Okay. So requirements include the following things. Functional requirements. What should this look like? How should it function? What does it do? It's a video game. It should be graphical, right? You know, things that the user would see, ways it's going to behave. User Stories. What are User Stories? How would a user use a system? Well, a user's going to start by seeing the high scores. Then they're going to want to hit enter and they're going to start playing the game using the cursor keys and then if they die, then blah, blah, blah. Stories might be how you sign up for a website. Well, first I go to a sign up page and then I go to a payment page. User Stories, this tells the model, for instance, how a user would use a site. Frameworks and APIs to use, like, hey, I want to use TypeScript and React. Wireframes and mockups. This is amazing. If you drop in an image, like even a napkin, of a UI, it will recreate that UI for you with pretty good fidelity. If you do a Figma diagram, make a nice UI in Figma, it will recreate that UI pretty well. They're optimizing for that. UI flow diagrams. What UI screen goes to what UI screen? Business rules. How things actually work under the hood. What happens when I click purchase? Well, that causes a credit card transaction to go to someone else. So and so and so. Testing requirements. These LLMs will build tests for you. And not only that, they will use the test to debug the code they generate. So they'll actually build a test. They built a test for the Pac-Man maze. And then every time they change the maze, they'll run the test to make sure the maze is still a valid maze. It has the right dimensions, has the right dots in it. They will do that. And when it gets it wrong, it will say, oh, I got it wrong. Let me fix it. Okay? So you want to specify what testing requirements. And this is not coding at all. This is just what you want from the system. Security and performance if you want to secure make a website that secure give it information on that and acceptance criteria in other words how do you decide whether to accept something or not now most people just want to type in build a pac-man game or you probably wouldn't because you probably don't care about pac-man but build you know Pokemon go or I don't know whatever else it would be but if you want something that is really great you have to come up with requirements okay let's stop here We're going to be vibe coding an app. I'm going to go long, but that's okay. Does anybody have any ideas for a simple app that we can vibe code that would not be too complex, but would be sort of fun? I have one if you don't come up with anything. Pong? Pong would be a good one. Okay, anybody else? How many like Pong? Anybody have any other ideas? Okay. Wonky Kong. CS32, huh? I have been trained on that. It might be able to do it. Okay. Any others? Let's do Pong. We'll do Pong. Okay. It probably has Pong, but maybe we can make a slightly different version of Pong. So maybe a, what would be an interesting variation on Pong? Three person Pong. Okay. I was thinking about that too. Okay. Now, I, you know, we could start to try to come up with requirements ourself, but because we are vibing today, Let's go to 03 and let's ask chat TPT 03. Okay. I want to make a three person TypeScript based Pong game come up with a set of basic requirements that I could give to an engineer to help them build it. Keep it simple. Make it for a hobbyist rather than a big development team. Okay. Normally I would come up with requirements myself because I know what game I want. I know what I want it to look like and so on. But we're going to just have O3 do this for us because why not, right? So let's see what it comes up with. I have no idea what's going to happen. But let's see. It's thinking. And while it's thinking, oh, Okay, so it says it's talking about the game arena, player controls, left paddle and right paddle and bottom paddle has different keys, great. Core mechanics, ball start centered at 45 degree angle. Okay, we get the idea. Okay, so now we have our requirements here. Now, ideally you would go through this and change the requirements a little bit, make sure there's nothing missing and so on, but that's good enough for right now. So we'll just go back to our presentation here. To give you an idea of the requirements I used for Pac-Man, I spent some time typing these up ahead of time. So here's the game style and the tech stack that I told it to use. There's a welcome stage, the different stages, a playing stage, a player lost a life stage, a player completed the level stage, a game over stage. All of this I had to specify to think about ahead of time and come up with these requirements. Here's the game UIs. Here's the levels. What do levels look like? How big are they? What has to be on different levels. Here's Pac-Man game mechanics, like how does Pac-Man move, how does it turn, how does it eat dots, here are the ghost game mechanics. So all of this I came up with ahead of time. I did this on my own, I didn't use O3. Okay, now once you have requirements, you want to generate something called a PRD or product requirements document. Now why? My guess is that the requirements we just generated would be sufficient. They would probably work just fine. But it is more in the VIBE programming space that if you create a PRD, which is a special type of requirements document used in industry, you will get better results. Will you? I don't know, but I tried and got reasonable results, so we're going to try to create a PRD. So we're going to use a thinking model to create a product requirements document, which is a document of a specified format that if you worked at Google as a product manager, you would produce, or Facebook, or Amazon, or whatever. So we're going to give a prompt that says create a PRD based on the provided requirements that we just generated. And we're going to ask it, you know, we're going to basically do this because presumably, supposedly it lets the LLM do a better job. Why might this be? It may be BS. But it might be the case that the way these things are trained is with a product requirements doc generating, you know, a program and that's the way they train these models and that's why they work better. So let's go. Hold on one second. I'm going to just go back here. Come on. Okay. Okay. So let's go back here. Okay. We're going to say create a PRD in markup format from the requirements above. Avoid things like tests and other big company development. "Keep it simple and fast. Make it downloadable." Okay, and while it's doing this, one second, we're going to go into cursor, and we are going to close the current project, so much for Pac-Man, and we are going to open our new folder, which is basically empty, and we have space for a PRD. here, but it's empty. So we're going to cut and paste the requirements in here. Okay? So hold on one second. It's doing it here. And while it's doing it, we'll just continue. Well, let's just let it finish. It's almost done. Okay. So it's creating a PRD, which is just a more formal version of these requirements. And this is basically why VIBE programming takes so long, because you're basically waiting for these models. Okay? So here it is. Download the PRD. Starting the download. We're going to save it to Let's see. Three-Ponged P-R-D. Okay, P-R-D, perfect. And this should be going into... It did! Look at that. So we can get rid of our old P-R-D. Okay, so we have our three-Ponged P-R-D. And you can see this. This is a more formal document. Okay? So we have the purpose. We have success criteria, key features, gameplay mechanics, controls, technical requirements, stretch goals, out of scope, and so on. So out of scope is useful because that will make sure the model doesn't go and generate all kinds of data. to test for us, which will take a long time and so on. Okay, so we're doing well. So let's keep going here. So now we have formal requirements. Okay. Now we generate a task list. Okay. So basically what we're going to do, and again, look, she's writing backwards. It's sort of entertaining. So a task list is basically a list of ordered dependencies and tasks that will implement this product. Okay. So we're going to use a thinking model again. to create a detailed dependency where task list can here is the prompt we're going to use. This is the one similar to the one I used. So using the PRD below, generate a comprehensive dependency aware task list in execution order. For each task decide whether it needs sub tasks. So break it down for us. Enumerate them, ensure every task and sub task is clearly defined and actionable by an engineer. Finally validate for completeness and so on. Okay, so let's go. And we already had the, We already have the PRD, so we can just, we have it in our chat window, so let's just do that. Okay, and here we go. And we're going to change it below, and we're going to have it say, hold on one second, for the PRD above. Okay. So it's going to give us a JSON file, and this is the task list we're going to be using to work on. So we're going to basically be asking the model, go execute the next task and so on okay so let's see what it does it's going to produce a task list these generally are pretty good it's thinking it's trying to figure out the dependencies you know what I mean by dependencies like you have to do this before that because if you do this thing first then you'll screw that thing up so you know it'll figure out the the order here okay this is how I built pac-man basically and it takes a little while I know it's a little bit boring I would switch back to presenting but it's probably going to finish pretty quick quickly and the switch time is probably going to take just as long as having it finish. Maybe not. Okay. Here's our task list. Let's download. Okay. And we're going to have, there we go, our task list. Okay. Fantastic. So we are well on our way. And let's continue. All right. So now we have our requirements, which hopefully you would edit yourself, but we didn't have time. We have our task list. and this is what the model will use for development and so now we're going to implement tasks. Okay. Now again, if you want to use cursor and say, oh, add a button here, add a dialog there, make a drop down here, you could do that. But this is a more formal approach and this is apparently what the Y Combinators of the world are using in order to build real apps rather than just completely being iterative. Okay. Okay. So what does implementing tasks look like? So basically we repeat the following steps until we get, that are mostly working change. Mostly working is key here. Start fresh. So this is a big thing. Remember, as all of this crud builds up in the chat window, the models get confused. Some of them get confused. So you want to start a fresh chat session with Cursor every time or whatever you're using, every time you do a new task. Then ask the model to basically complete the next task or subtask. Just say complete the next task, figure out what you need to do, you do it. Yes, the model. Okay, so here's what I write. Consult the product requirements doc and pending completed task list in folder or whatever and select and execute the next pending task then update the completed task list when you're done. Literally, we're going to say knock off a task. Okay? Okay? Now, once you do this, you're going to get some code. You might be able to validate that code. You might actually be able to run it and see if it works and see if it looks like it's doing the right thing. Ideally you can you know especially once you get a framework up and running you can sort of see it added a dialog box or whatever it happens to be or button you know hopefully you can if you can't then you just go on to the next step okay because we can't really tell because we're vibing so we validate now this is the important step here often today these models will derail what do I mean by derail they will produce something that's totally not working they will produce something where they basically change the code from JavaScript to Python. They'll go and do all kinds of crazy stuff that will just not work. They will take a five-line change and make it a 500-line change. If any of these things happen or you see it, you want to revert. So there's a button in these products where you basically say, revert, undo. It'll use get. It'll just basically revert everything and then it'll go back to starting point. You never want to go and iterate if it goes off the rails because it will go so far off the rails, the iteration will just get you in trouble make sense so don't don't if it if it goes off the rails don't just try to fix it just revert okay okay so let's go and try doing our first task and then we'll keep we'll keep going here so we're going to go and here's our chat window here so you see this chat window it says plan search build anything so we're going to go hold on one second to this and We're going to copy our prompt. Just use what I actually present here. And we're going to say, oh, I don't want to do an image. Oh, come on. I'm sorry. One second. Okay. Okay. So here we go. Okay. In the folder specs. Okay. And select the next two. Okay. So here we go. So while this is going, let's just watch it for a second and see. So it's going to talk to us. I'll help you consult the PRD and task list. Let me examine the PRD. So it's reading it. It sent a read command down to our computer from the cloud and said, I need to read the PRD. It said, okay, I need to read the task list. It says, okay, let me see what the completed tasks are. What's done so far? Those were all commands read, read file sent down from the LLM. Okay. It's doing exactly what I showed you in that in that flow. Okay, now it's basically saying, okay, I want to use TypeScript, I want to set up an environment, so it's actually installing a bunch of software. Notice the LLM told my cursor agent install software and cursor basically said, okay, I'm going to run some shell commands and install some software. Okay, so it actually, it says, okay, now I need to update the config for TypeScript to specify our TypeScript rules. Okay, so it does that for us. Notice it's doing multiple commands without me. It's doing everything on its own. Okay? Now we create the folder structure. It actually says, well, I know what folders I'm going to need for a project like this. So it creates folders. Now we create the main HTML file, index HTML with a canvas element. So it's doing that. Now we update the package JSON. You get the idea? It's actually doing multiple commands. Each of these is a round trip to the server, back and forth, back and forth. The chat history is accreting. It's getting longer and longer. Every single new command is appended on, and all of this is sent up. over and over and over. And it launched it. It actually launched. Okay. It opened up my web browser and it shows us what it's done so far. Okay. So it actually started the server up and launched in my web browser. Okay. So there we go. So that's where we are so far. Any questions so far? Yeah. So you said it just do one step but it did many steps. Let's see. So hold on one second. So it says here task TC1 completed. So that's only task one. Okay. So T1 rather. So it won't let you necessarily reverted that granularity today. You can go in and edit it if you want. What you can do though is I can for instance I can go back up here and see it says restore checkpoint that's how we would revert to the last major command we did okay okay now it did so well let's just do another command so let's just say consult the PRD and pending completed tasks do another one so let's watch it let's see what it does sort of interesting you never know what it's going to do it does take some time I can see task T1 was already completed successfully okay proceed with an attending task T2 canvas and main game loop let me implement Okay, so let's see what it's doing. So it's basically going back and forth. Oh, you know what I did wrong? I did not clear. I did not clear our chat history. I should have cleared our chat history. It's probably fine, but it might go off the rails, so we're going to have to see. Okay. Oh, look at that. Okay. It did something. Okay. So there we go. So we now have a running at 60 frames per second, a TypeScript game which does nothing. Okay. So let's go back here. It says it completed that. tells us what it did. Okay. Would you like to proceed? So I'm going to just say, let's create a new chat. And I'm going to say, go. And this is the process you're going to use. Now I'm going to talk about debugging in just a second, because things will go off the rails today. Maybe tomorrow they won't, but today they do. It's looking at the PRD back and forth. It's going to start a new chat session, setting things up, getting stuff back. It's reading the requirements document. It's reading the task list. It's looking what's been completed so far. It's thinking through it. What do I need to do next? Isn't it? It's amazing, right? This is running shell commands. This is creating files. It will debug. It will add debug statements to your code, run the code, look at the debug statements, and fix things for you. Okay? It's pretty smart. Okay, now we're going to implement a T3 paddle module. Okay? It's going to create a paddle class to integrate into the game. By the way, what's interesting about these models is they have knowledge of the world. They know what Pong is. because there's probably been 10,000 articles on the internet about Pong. So a lot of what they're doing is based on that background knowledge of the model. Okay? So all those linter errors, it found linter errors, so it's fixing the linter errors for us. Okay, it's initializing the paddles in the constructor. Let me update the constructor. So let's see what it does. It's going to update the render method to draw the paddles. And by the way, it could be generating tests for us if we wanted it to. That would be, you know, we could ask it to do that. And you can see it's basically made 10 lines of change. It removed two lines. And so on. So here we go. Added eight lines to game TS. Here we go. So this is basically where you're, you know, this is most of my time was spent waiting for this. Okay. Oh, look, we have paddles. Okay. And let's see. Do they move? They don't move yet. Okay. But that's okay. So we haven't seen any bugs yet. I think we keep going until we have a bug and then we'll look at the next. We'll look at the next. Oh, it's still working. working. Hold on. Now, we can look at all this code if we wanted to, and you can debug it, and you can see how it's working. I think a great thing about this is you can actually look at the code one step at a time and see exactly how it solves the problem. I learned how to build third-party authentication with Google OAuth, so you can log in through Google to a website on my own just using this system. It's pretty cool. Okay, so let's accept everything, and let's start a new chat. And let's see, can they move? They can't. So they said they should move but they don't move yet. I don't know if that's required yet. In other words, I don't know if this step was supposed to have them be movable but let's do one more step and then we can start debugging. Okay, so we'll do one more step and then we'll do you take a break usually or no? Okay, so we'll do one more step. We'll take a quick break. We'll do another 20 minutes and I'll yeah. Okay, so it's going here. It's looking at the specs. It's looking at the you know the PRD, the task list. Okay. Okay. So we, oh, let's see. So we have the paddle module. This painting has ball module. Okay. So it's adding the ball. So I'm going to talk to it in a second when it's done with this. I'm going to ask it if the paddle should be movable because maybe they should be and it should have been done and it doesn't work. And we'll find out. Yeah. Question. Oh, T5 is the input module. Okay. So we'll wait then. We'll wait. Perfect. Thanks for helping with that. Okay, so here we go. And notice how it's launching. It's actually showing us its work. It's pretty amazing. Okay, so let's see here. I need to initialize the ball in the constructor. Okay, we'll let it go and then I'll go talk about debugging in a second. We may not need a debug. That would be really cool. No, debugging is very annoying and I'll tell you why. Yeah. Right now it's It's sort of like throwing darts while drunk and blindfolded. Oh, look at that. Look at that. Okay. I don't think, you know, nobody's dying yet. It shouldn't be able to get past them, but this is progress, right? This is progress. Okay. So let me ask you actually, okay, after it finishes here. Yeah. is too stupid to do that right now. Yeah, we can kill them off manually. I can actually tell it to kill all the old stuff. In fact, here, here, one second. One second. Please kill. Oh, that'd be nice to it. It's one more token. It's just like a fraction of a penny. It's okay. All right. So here we go. There we go. There they go. Okay. So now I want to ask it something. I would say, you know, right now the ball, can hit the players wall without the player losing a life. Is that expected at this step in the process or is that a bug? Don't fix anything, just tell me. So if you don't say that, often it will go and say, oh, that's right, and it'll just try to fix it. But I want to just find out whether there's a problem or not. So let's see, it's going to go and look, it listed, notice It did list of the items in source. So it basically said do a directory like we just saw in our example. Give me the files. Let me read ball.ts and so on. So it read the file. It's searching the code base for paddle collision detection ball. So there's a command that the model is given which is you can search the code base for something and we'll do a grep. And it's reading the task list. It's deciding in the task list whether or not we've done what we need to do. It says based on our analysis. Let's see what it says here. Hold on. One second. This is expected behavior of the state. So we're fine now. Okay. All right. Let's take a step back to the presentation so we can finish up and we'll try doing some debugging a little bit. I hope we don't have to, but we might. Okay. So there we go. Any questions, by the way, so far? We're good? Okay. Yeah, question. Okay. All right. Debugging. So when something happens, okay, and by the way, So when I say something happens, I don't mean it completely went off the rails. If it completely goes off the rails and generates crap and it crashes or doesn't work at all, revert. Revert. Okay? In other words, go to your last checkpoint. Okay? But assuming it generates some code and it does mostly work or it looks like it's in the ballpark, start fresh. So create a new chat session. Describe the problem. Now, you can include things like literally take a screenshot of the UI and paste it in. You can include logs if you have logs already. And what you want to do first is ask for root causes. So don't just say fix this because the LLM often will not have enough context to fix it. It'll go off the rails. It won't do a good job. So say root cause first. And don't just say give me one root cause. Say give me three or the top three or top five root causes. Okay. Then, if it makes sense, and this is a big if, ask it to add diagnostics to your code. What do we mean by diagnostics? I mean logging. Add logging to the code. So you can say, hey, based on your three hypotheses of what's going wrong, add logging or other diagnostics to the code so we can figure out which the problem is. It will then modify the code and we will then run the code. We'll collect the logs. Got it? We will then Request a fix. We will paste the logs, the screenshots, in the same chat, everything we have, and request a targeted fix based on this information. Please root cause the problem again. You now have new logs and generate a fix. Okay? And then if the fix fails, this is a big one, don't make a fix on a fix because these things will get more and more down the rabbit hole and they'll get buggier and buggier. If a fix fails, revert. You go restore to a checkpoint. and try again. Okay? Now, I would argue the easiest thing to do today is to try a different model. So basically, cursor is connected to ChatGPT. Cursor is connected to Gemini. Cursor is connected to Anthropics Quad. It can use all of them. And you can actually pick, I want to use Quad this time. I want to use Gemini this time. I want to use ChatGPT 03 this time. You can pick. and different models succeed at different things. So simply asking a different model to take a fresh look may be sufficient to solve the problem. Whereas if you ask the same model over and over, it could just get stuck in a death spiral. It might not work. Okay? So ask different models. You can also have it continue to root cause if you haven't found the problem. There's a number of techniques you can use, but always revert. Don't do fixes on fixes unless there's a substantially better behavior after your fix. Okay? Okay. So let's go. I'll give you the pro tips and then we'll go. Okay, let's start again, everybody. Okay, so what we'll do is we'll do a couple more slides. We'll do a couple more turns at Claude and then we'll take questions. How's that? We don't need to finish it, but we can try. Actually, why don't we, yeah, let's do some pro tips. Okay, so there are some books on this now, but it's really early, okay? And everything I'm sharing with you I discovered by watching YouTube videos, reading blogs, and also playing with things myself. So I don't think there's necessarily a perfect guidebook right now, and these models are changing very quickly. So everything I'm telling you now may change. The best way to figure out how to use these models is to play with them and see what happens, to be honest with you. But here are some pro tips based on where things are today. So this is the most important one I can stress for you. When you are Vybe programming, you must choose the right language and the right platform for the task. So for instance, if I said build a video video game. Okay. Most video games that the large language models are going to know about are built in TypeScript or JavaScript. They're web games. Okay. Nobody builds video games in Python. You could ask. You could ask Cursor to build a video game in Python and it will be able to do it up to a point, but it will get stuck, believe it or not, and just not do a great job. It will actually get confused over time in some cases. It's not going to do as well. What you want to do is you want to pick a platform that's relevant for your use case. If you're doing machine learning, don't ask it to do that in C++. You could, but it's better to do it in Python with PyTorch or TensorFlow. It's better to use PyTorch or TensorFlow than some random machine learning library that has not been trained on the Internet's data, on GitHub or whatever it is. So really pick a platform, libraries that are popular for the use case you want. you're going to have trouble. It's going to work a lot less well. That's number one. That's like the most important thing you can do. Other things that you can do in order to improve, add testing to your requirements. Believe it or not, these systems will produce tests and they will produce tests that actually work. It will actually run the test. It will look at the output for the test. It will unit test and say, oh, there's a bug in this. Let me fix it. You don't even have to go debug things. because it will run the test and find the bugs itself. So ask them to write tests, it will do so. Avoid fixes on top of fixes. We talked about this. If you fix on top of a fix on top of a fix, it will generate a ton of crud. Just revert and try again from scratch. Paste in API docs. If you want to use a particular API, let's imagine you want to build an app that does credit card transactions using some credit card provider that provides the ability to do transactions for your website. You can go and either provide a URL or literally cut and paste in the docs into a document in one of your directories, your folders, and it will use that data in order to know how the APIs work and it will generate better API calls. And ask for refactors. So once in a while you could say, hey, clean up this code, refactor it, use the DLY principle, do not repeat yourself, you know, eliminate duplicate functionality because these models today will implement duplicate functionality. They'll just duplicate things three times and it gets It's a little ugly if you actually look at the code. Okay. All right. So, let's finish off our VIDE programming. We'll take some questions and I'll be done. So let's see here. All right. Let's see. Back to cursor. So let's do the next step. Maybe we'll get a working game. That'd be sort of cool if we got a working game. Okay. So while we're waiting for us, any questions? Could be about anything. We had some people asking about the future of software engineering. I have a crystal ball but we could talk about that. Any questions? Yeah. If I check out Google's what? I haven't checked out their Firebase. They have a new app builder right in that? Yeah. I haven't checked it out. But there's like Replit I think is one. Google has a Firebase app builder. There are lots of app builders that are tailored towards building front ends. So I wouldn't necessarily do it in cursor. You could actually go and there are better tools that just are designed for this. that you could use right now. So definitely try those out. Other questions? Or you're enthralled by its modifications here? We need to update the render method to show pause state and some additional UI feedback. Yeah, question. Yeah. Traditional no AI, only reading a lot. What do you think about that? What do you think about that? I think that today not using AI is a death sentence. I think that today only using AI is a death sentence. So if it were me and I were doing professional software development, which I'm not doing right now, I would use AI, I would code review everything, One thing that Chris is really great at is automated refactors. So it'll actually refactor for you to clean up the code. If you tell it what to do, like, hey, make this five-minute line function into ten functions that make logical sense. If you use these tools, they're so much faster than doing these things by hand, but you can't trust them today to produce really good code. So if you don't use them at all, you're going to be really slow. If you use them too much, you're going to get really cruddy code and you're going to spend more time. So it's somewhere in the middle right now, but I think it's shifting towards no code. Yeah. And then. So we've been talking about this. We actually there's a lot of papers written on this and it's a difficult it's I you're not going to want to hear what I have to say but my where I'm coming around to to be honest with you is that you should not use them as much as possible for for tasks, other than understanding. Like explain this concept to me, give me a problem, tell me step by step how to solve this type of problem, that's great. If you use it to help code like co-pilot, auto-complete, it will hurt your learning, I think, is basically what the studies are showing, and I think it's very, it's obvious, you can see that's happening. So what I would say is, ideally, code as much as you can right now, build intuition, use these models as little as you can, Use them to understand and learn. So you could generate in cursor some code, but then ask chat GP2 to explain it line by line to you so you understand how it works and you learn how to do that yourself. But I would say that right now it's probably going to cognitively hurt. Yeah. Yeah. Yeah, that's right. I call it spinning. If you're spinning in circles, you can't figure out how to get out of it. Yeah, use AF for that. That's a great idea. Okay, we'll give it another task. We'll have it do that. Other questions or comments or thoughts? Will CS be a subject five years from now? I believe and I'm sure you agree with me that it will absolutely be a subject because there will be tasks that simply cannot be done by models probably for a long time and these models are getting better and better like they are improving exponentially I believe in terms of how long they can go without being you know making this mistake and so on and I think they will be superhuman in some ways but there will be things where you need to go and look and say you know what I need to understand the big picture it's generating some code that you that solves this problem and it doesn't solve it in the right way. You're going to have to have judgment and you're going to have to steer it and the only way you can do that is if you understand the fundamentals. So I don't believe that we're going to get away from computer science in the next five years. It may be the case though that a lot more people can build without being software engineers and build pretty good stuff. You know build a 90% solution, a prototype, even start a small business. Let's see. How's it doing? Oh, it's bouncing off the paddles. Let's see if we can move here. Can we move? We still can't move. Okay, let's just do another task and we'll keep talking here. Let's start it again. Do another task. Okay, but I don't think it's going to go away. Do you think it's going to go away? No. No. Yeah, but I do believe that they'll be superhuman in certain areas. There will be use cases that people are training and You're going to be able to make apps with backends and frontends and databases, bless you, and they are going to be every bit as good as the average human for simple apps. Yeah? Yes, they have lots of security vulnerabilities. Okay, so my thoughts on that are actually very interesting because I vibe coded OAuth integration for a website and it made all kinds of mistakes. But what was interesting, was the following. When I told it, you made a mistake, this is insecure, fix it, it was able to fix it and it understood that. These models today are not being trained to generate secure code. But I believe very firmly that they will be trained to generate secure code over time. It's only a matter of time before what is generated is going to be more and more secure and probably more secure than a human being would do it. It's just inevitable. Because if they don't train it that way, people won't be able to use it as much and they're going to do that. Yeah. Okay. Can we still? We can't still can't. Oh, what just happened? That seems like a bug, doesn't it? Okay. So let's see. Let's see here. Any other question? Well, at least it's bouncing. So let's see. Have we done the task where we should be able to sort of, oh, I think it's restarting. So let's see. Okay. So hold on. Oh, it's still going. They can do gravity, they can simulate all kinds of stuff. So by the way, this is how we develop Pac-Man by the way. So this type of thing, we found a bug, then I would start debugging and just this path. Yeah? How do you ensure security when you're bi-coding? How do you ensure that without just having something like blow up and then you have to do that? Well, so this is where having domain expertise is helpful today. So I would take the class with Dr. Reier, Reier? Reier? Reier? Reier. Reier. Take it with Dr. Reier. So you learn what's good and what's bad. And you know. Now the other thing you can do, by the way, is you can take the code generated by Model A and ask Model B, a reasoning model, a thinking model is better, and say what are all the security vulnerabilities in this? And it will generally tell you all of them. So today you can even use that in order to fix these things. Okay, it's still generating. So maybe we'll do one last. We'll run here and then we'll call it a day. Any other questions or comments? Yeah? What's your opinion on the future of the job market? That's a good question. What's my opinion of the software job market? Oh, look at this. We have a start screen. Hold on. Okay, so I still can't control the panels. Okay, so let me start one debug session here and then I will tell you my thoughts. Oh, I see. When it bounces out, that's when it restarts. Okay. So let's do accept all. Okay. Please debug why paddle movement doesn't work and give me root causes. Don't make any changes yet. Okay. And we're going to do that clearing the context. And by the way, we can pick Sonnet. We can do Opus. We can do O3. We can do Gemini. So you can pick which model. We'll continue using Sonnet for now, but we don't have to. Okay, so what do I think about the job market? So there have always been cycles in software. You remember when after the dot-com boom and everybody lost their jobs and software wasn't going to be a thing anymore. And then there was the India boom where everybody said we're outsourcing jobs to India. You're never going to be able to make any money in software. And I remember actually, I don't know if you were here, Enrollment dropped, I don't know, our classes went from 90 students to, by a third, to 60 students because people thought software wasn't going to be a thing. I happen to think that there may be a trough right now. It may be more difficult to get a job because people are right now optimizing for efficiency and for earnings. They want to make more money, more profit. But long term, or let's say medium term, medium term, I do believe that companies will not be successful if they are not building more. So if Google only optimizes for profit, but Meta decides they're going to actually build more and not just optimize for profit, but build more, they are going to pull ahead and Google is going to pull behind and they're going to have to all invest again. So I think we're going to go like this, where people try to optimize for profit and they're going to say, well, we can't do that because we're going to get left behind and so on. Now, if we go to a world where everything is vibe coded perfectly, I don't think it's It's going to happen soon, but in five years, believe it or not, it could happen. I mean, it's hard to believe. It's scary, but it's hard to believe, but it could happen. Then, all bets are off, but I would say then the opportunities are not going to be at Googles and Metas and so on and Amazons. It's going to be you learning about some business. Maybe it's logistics or shipping or maybe it's doggy daycare and solving a problem by basically figuring out what will solve the customer's problem and building. and I think we're going to have a million builders where right now we'd be a hundred thousand builders of independent companies where everybody's going to be solving a different problem because you don't need to know how to code or you need to know less. And you might have one or two people that can then shepherd the LLM and basically build and automate a million different things. Now the one thing that's going to be really important in that world is being really clear with requirements. If you just say build an app to run a doggy daycare, it's going to do a crummy job. If you say here are the main use cases that you have in doggy daycare. Here's what we need to do to make the customers happy. This has to be fast. This doesn't matter so much, but it needs to get done once in a while. We need to track the dog's names and how often they bite people and blah, blah, blah. If you understand those use cases and you have good requirements, then vibe coding can then produce amazing stuff. The most successful people will be clear thinkers about requirements and what is needed by the user, less the coding, I think, long term if things go where we think they are. So let's see, what does it say here? Okay, let's see here. It says, okay, root causes. Based on my analysis, I've identified several potential root causes why paddle movement doesn't work. Game state issue, most likely cause. So let's prioritize these for us. Delta time conversion issue. Okay, you know what? We could normally add logging, but I'm going to just go, I'm going to do a YOLO. So I'm going to say, let's just go and do a, a thinking model. We're going to go max mode. Okay, so this is actually going to spend more time. It's going to cost me some money. I think it's a nickel per request. So I might spend a 50 cents on this. Okay, given your root cause, fix the problem. Now, if this doesn't work, I would typically add logging and then run the game and then paste the logging in and then ask it to root cause again and fix it. Okay, but let's just do it and see what happens here. I don't think so they get you either way let's see so it's gonna go let's see what it does perfect I found the issue did it though I don't know we're gonna find out it often will say I found the issue and it won't work but sometimes it will ball velocity in pixels per second Okay, now somehow it's focusing on balls, but we want to focus on paddles. So I don't know if this is the right thing, but we'll let it go. We might have to revert. I see the issue now. But why are you focusing on balls? Okay, wait. Oh, you said fix the paddle class. Okay, here we go. Let's see here. All right, hold on. This will be my last thing. We can call it a day. A lot of time is spent just waiting for this thing and watching it and saying, no, it doesn't seem right, or oh, that seems good. But I have no idea. I'm not looking at the code. Let's just look at how many files it actually creates. It's created assets. Let's see. Oh, we don't have that much, actually. Source. There's all the files it created. Look at all these files. It'll show you the changes here. Oh, look, it's adding logging. It's adding logging. Okay, let's run. Okay, it's going to run the game. So here we go. All right. Who thinks it's going to work? Raise your hands. Who's like, no way? Okay. I tend to go with you. First time it never works. Hold on one second. Launch the game. Let's not use Sonic Max so I don't have to pay tons of money here. Okay. Kill all servers and relaunch and open in Chrome. Okay. So let's see. Come on, baby. Look at this. Look at this excitement. I cannot wait to see what happens. Okay, here we go. Nope. Oh, oh, oh, oh. Oh, it crashed. All right. Welcome to VIVE Programming. I hope you enjoyed it. Thank you for your brave presentation. Yes, I just produced Buggy Cone and got applause. Feel free to reach out if anybody has any questions and we'll make sure there's a link to the presentation that you can take a look at pretty soon. All right. And you're all here and all set. Take care. Can you send out the email? Yeah. Okay, great. Apoio. So I wanted to just talk a little bit more from the point of view of someone who isn't into vibe coding, although I have used AI to do a little bit of software development. And what I wanted to do was sort of look at, so this is software development because it's a software construction course, right? And what I wanted to do is get some actual numbers. And I looked for hard numbers in this field and it's tough, right? That is, as an academic, I would like to be able to do controlled experiments and say that, If you use CHAT GPT, your productivity will go up by X percent. If you use CLOT, it'll go up by Y percent or maybe go down or whatever. But those studies are few and far between. I'll give you some numbers I did find, but I want you to just, before I give them to you, I want to sort of remind you to take them with a grain of salt here. It's very hard to do real research in software engineering experiments are so expensive nobody can afford them. Google can't afford them, Microsoft, Apple, none of those guys can afford to do this. Let's take a thousand software developers and do the same project two different ways? You've got to be kidding me. Nobody's going to do that. All right, so with that in mind, here's some surveys which are not as good as experiments because what they do is they ask software developers what they do and how they do. how they perceive how that results in productivity improvement. So one is from JetBrains. And it's the state of developer ecosystem, right? And JetBrains is a particular software development organization ecosystem. They do a state report every year. They're currently gathering data for the 2025 report. This is the 2024 report, which is based on data that's almost a year old now. And here are some number, one number that jumped out at me. 63% of developers say they save less than four hours weekly by using these tools. I expect, you know, this number will go up this year. That is, they'll save somewhat more. And four hours a weekly means you have a 10% performance improvement, assuming a 40-hour week. So this is a significant savings in software development. But, you know, this means 30, 7% said they saved more. Some said they saved 8%, that sort of thing. The graph has sort of a mode which is somewhere in the 2 hour range. So that's one number. The basic thing that they listed, if you ask these developers how they save time, is first, less time searching. Particularly in large software development projects, you spend a good amount of time figuring out where the code is because you know a small part of a large project you want to integrate to something else you got to search through it the AI tool will help you do that in some sense you can think of it as a substitute or an improvement over using something like an ordinary Google search another thing they say is faster coding or faster development in general. Just speed up how long, you know, take less time to write code and all that sort of thing. And another one is faster completion of repetitive tasks. To some extent, what these developers are saying is the stuff I don't like to do, have the AI do it. It's boring. It's repetitive. It doesn't require much ingenuity. Let the AI do it. So this is sort of a standard thing that you would have expected out of any AI system, and that's what they're saying. So that's one number. I'll do another number. This comes from Satya Nadella. Anybody recognize that name? is, yes, Microsoft CEO, right? And he said this in a conference a month ago. And his number was something like this. Maybe, I love these maybes, 20% or 30% of the code in our repos today are written by AI tools. He said this in sort of a dialogue with Mark Zuckerberg. So this was at LamaCon, which is a meta Facebook thing. Zuckerberg was surprised. And when Nadella asked him for similar figures at meta, Zuckerberg said, I don't know what those numbers are offhand, but predicted greater than 50% by next year. for that question. I tried to look for the numbers backing up this probably maybe 50% assertion. And this is an area where, you know, as an academic, I'm an open source guy. I sort of expect everybody to be saying what they're doing. Nobody's saying what they're doing. Right? So you have to do some tea leaf reading and that sort of thing. But I did find a LinkedIn posting. by Saranyan Vigraham. How many people recognize this guy's name? Almost nobody. I didn't know him before I looked it up. He's the director of engineering at MEDA. So a big mucky muck, although you hardly ever see him interviewed and all that sort of thing. But he posted something on LinkedIn in which he said they did the following. They used vibe coding. So kind of what Kerry was talking about, except they didn't want to use it to do video games. Right. He's doing engineering, a lot of back end. and stuff and that sort of thing. In 10, what he called closed loop production projects. Now I'll do my own interpretation of this, because he didn't explain what he meant by that. A closed loop production project means meta The standard technique as you know is to move fast and break things. They take production code and they go do stuff to it. And in some sense what they're doing here is they're reusing code. Because that's what Meta does a lot of. And he said they "varied the amount of AI involvement." And I'll put quotes around AI involvement because he didn't specify what he meant by that, but they did that variation from 10% to 80%. And if I could get this to display, I'll just write it down here. Why not? Came up with something that looks like this. Let's draw a big version of that. Because the gist is important here. This is our 10% to 80%, right? So this is the AI involvement. This means percentage of lines of code generated. Maybe this means percentage of time developers use AI versus not using AI. He didn't say, but whatever. And then he had over here bugs introduced. And the figure looks like this. Where this knee is in the 40 to 50% range. I guess 80% is way over here. So the idea being that if you didn't use AI at all and did everything by hand, you tended to write buggy code. If you used some AI in the 40% to 50% range, presumably to focus on getting rid of the bugs, you did well. If you kept using AI to do more and more of the work, to do the original developer and that sort of thing, So this is AI pluses are the following areas. First boilerplate and framework code. If you have the same sort of code being used over and over again slightly with modifications and different projects it was a good way of sort of tailoring that. And doing large scale refactoring. You're making a big change. It's global over the whole software. You're changing the API of one of your modules. You've got to change the API of everybody that uses it, that sort of thing. You seem to be pretty good at that. Migration scaffolds. You're moving your code from one platform to another. You're trying to migrate. Change the underlying base. You want to minimize the amount of changes you make to the most part of the code. You write shims in order to make the migration easier. It was good at doing that sort of thing. And then last, and what you're doing in homework six, test case generation. This list wouldn't be complete if we didn't also list where AI didn't do very well. So where AI stumbled. The following areas. Complex logic paths. by complex, but presumably he meant a bunch of the if-then-elses or similar sorts of things where you have lots of different cases. The AI started sort of going bananas with that. Second, context-heavy features. The idea being that in order to understand this part of the code, you have to know the context that it's Relying upon that context is fairly complicated, the AI will get lost. Alright. Next area was anything requiring real systems thinking. For example, if you're thinking of changing your architecture, it could be something really major like, let's do this in Python instead of JavaScript. or it could be something relatively small but still systems thinking. It didn't tend to do very well in terms of thinking through how those changes would be made. And last, anything stateful or edge case heavy. If the state of your system is considered fairly complicated, and you're exploring some sort of weird combination of states. Again, that sort of folds into the complex logic past part of AI weaknesses. It tends to not do it very well. All right. So, Begram's conclusion was that software developers are not going to be out of a job anytime soon. His claim is that the future of this kind of development is One of collaboration, not a handoff. In fact, he has a bullet at the end of his LinkedIn post. And I hope that tends to relieve some of one's natural concern about being out of a job or graduating into a job market that no longer exists. All right, any comments on these numbers or this perspective? As you can tell, I'm not quite as positive as Carrie, but I mean, a 10% performance improvement is nothing to, you know, that's pretty good, and I expect it to be even better now. Yes? A stateful, oh, anything stateful. that your system operates on, you know, you have a shared state. It might be like the contents of your database or that sort of thing. And if that state is complicated, that is, you have a complicated scheme in your database with a lot of sort of moving parts inside the database, you're going to probably have some problems, right? Or it's more likely that you'll run into problems. Let's put it that way. All right, so I looked for other numbers like this, and how shall I say it? There's very little sort of high-quality computer science research that's been published in this area. It shouldn't be too surprising. The field didn't get named by coding until February, right? PAPERS and that sort of thing to be published on it. But I did find some other issues mentioned in some recent publications. And you should be sort of, you know, concerned about this thing, regardless of whether you use VIBE coding or just AI agents or just traditional prompt engineering and all that sort of thing. Some of these I think people already mentioned, but it's worth putting down. and sort of writing down so that we don't forget it. Right? First is you have to worry about leakage of private information into the models. When you talk to chat GPT, it's looking at your queries and it's saving them and it's using that to train so that it can do better next time. That means you have to be careful if you're writing applications involving anything private. If I'm writing an application for say the UCLA Medical Center and I'm using patient data, I can't use any of this stuff for that application at least if I have any test cases or that sort of thing because there's all these regulations on how that data can operate. So that's one issue that's going to come up sort of over and over again and it's not just or student grades or that sort of thing. If you are writing code for a company and you're worried that the competitors will find out what you're doing, you have to be careful about how much of your code you're going to be sharing with these models because data like that can leak. Many of the models will sort of offer you a private mode in which they promise not to train, you know, based on the data data that you give it, you should at least be using that. Second issue, who owns the end product intellectual property? This is often called IP in the legal world. you're using in your system and you market copyright Paul Egger or something like that did Paul Egger actually write it it might be a word-for-word copy of another program you know written by Todd Milstein or something I'm not going to know that it just spit out the program right that's definitely an issue there are several lawsuits currently in progress based on this sort of thing and to be honest nobody's who knows what's going to happen in the long run, but it's definitely a concern. A third issue is the cost of maintenance and operations. Almost all the research that I saw published was about development cost. Development is sort of the fun part of software construction. and we always want to bring down development costs and focus on that because that's fun to think about. But in reality, development is often not the biggest source of your costs. It's this other stuff. There's been very little work published on the effect of using AI-generated software and then later on you have to go maintain it. It could be, for all we know, that it drives up the cost of maintenance overall. because of the various problems or that it generates a lot of bloated code and all that sort of thing. Similarly for operations. Nobody really knows what's going to happen here. We haven't had enough experience with it, to be honest. And so that's going to be a concern. And the last one, and this one I think a couple of students mentioned, is insecure code. generated by Gen. AI. I did find a couple of papers saying that experienced developers perceive that their proficiency in secure coding decreases when using these tools. There are sort of a lot of horror stories or war stories about these tools generating code with obvious mistakes like being vulnerable to SQL injection and that sort of thing. Like Carrie, I expect many of these problems to be fixed at least somewhat in the near future, but it's not clear whether or not they'll be fixed well enough. And in particular, one of my longer term worries here is the problem The idea here is that if your tool is trained on bad code, on insecure code, then it will generate insecure code. There's a lot of insecure code out there in the world for these tools to pick up on and they're picking up on it and regurgitating it. Worse, And here, I hope you don't mind if I get a little paranoid, if I wanted to break into a competitor, I might publish some code that I know has some insecurities and hope the GenIA tools pick up on that and insert that bug into my competition. So this is a definite worry amongst the people who really want highly reliable or highly secure software. And, you know, the jury's still out here as to whether the GenIA tools can be more helpful than harmful in this area. All right. Well, I just gave you a bunch of my opinions and I ran out of time. Sorry about that. Maybe we'll have more time to talk about it in a later lecture.