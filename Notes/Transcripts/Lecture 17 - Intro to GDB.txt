So today we have a little bit of a grab bag of topics ranging from building and packaging to Python classes and modules and debugging and all that sort of thing. A little bit of leftover from earlier days when we were so rudely interrupted by the end of the class. So I'm going to start off with debugging since that's the freshest in your minds from the things that I talked about last week. By the way, what did you think of last session before we get into this? Interesting. So it wasn't a waste of time? You guys could have done better than Carrie? Okay, well, there is that sort of thought, yes. All right, so you guys could have done better. I'm sure collectively you could have. All right. Should I do this next year when I teach the class or should I just skip over this stuff? I think you should do the demo instead of that. I should do the demo? Oh. I worry that after every lecture I do the demo. No, no, no, no, no. Well, it's tricky. because demos are really good because you can see how things really work and they often don't and Kerry showed you that and I was very proud of him for doing that. The problem with demos is that they slow us down. If I were to do a demo of GDB for example I could only cover maybe a third of the topics that I can talk about when I'm just on the blackboard. The blackboard is really fast for doing this sort of thing. So there's pros and cons to doing demos. I get it that you would like demos but I'm worried that if you're if I do a demo of this or of any other sort of software development stuff it looks too much like you're looking over some other developers shoulder and falling asleep. I know I've done that. I'm very good at falling asleep. So it's a tricky business. I feel like we should be doing more sort of Gen. I, Gen. AI sort of software technology and lecture but it's hard to lecture about the fields kind of moving too far. Anyhow other thoughts about The day before yesterday? Alright, well in that case, let's talk about debugging. I'm going to do a few extra GDB commands that didn't make the cut last time, but which if you're debugging at the low level, you should know about if you haven't already started using it. One is this command, I space F. As a general rule, if commands have really short names, that's a suggestion that they get used a lot by experienced debuggers, right? You first look for the short abbreviations in any system. All right, and this is short for info frame. And the frame is the stack frame. So this shows you information about the currently executing function. Well, it's not currently executing because GDB has stopped the program, but it will will be executing once you continue and this will help you sort of deep figure out what the current function is up to and there's a related version called is which is short for info stack but I normally type it as bt which is short of verbatim they all mean the same thing and that gives you information not only about the current frame but about the caller's frame, dot, dot, dot, all the way back to main, which started off the whole thing. So if you're nested five deep in function calls, bt or is should give you five frames worth of information. Each frame will be decorated with the name of the function, its arguments, maybe the return address, maybe some other useful information. you got to where you are, right? But not really. A backtrace only tells you how you got here in a way that the machine was going to keep track of anyway. So if you execute a loop 500 times and then call a function, that information won't be here. because it's not saved. This backtrace really only reports info that the system needs to continue from here on out. It needs that backtrace because when this function returns, we need to know where it's going to go to next. That's back to the caller and so forth and so on. It records how you got here, but it only records enough of that information so that you can continue. And so it's an interesting subset, or you might call it sort of a little slice of information about the execution of your program. And once you see why it's there and sort of what it'll be used for next, that will help you understand anomalies. that you can sometimes run into when you're debugging a program. For example, sometimes this backtrace can be incomplete in the following sense. It could be that what the program did is this, main called f, called g, which called h. But when you run the backtrace, the backtrace will report, oh, we're currently in H, and the color is F, and then F's color is me. So it'll report something like this. It'll leave out G. Why would it do that? There's no G anymore. How come the backtrace only has a subset of the true backtrace? Can anybody explain this information, someone who perhaps has taken CS33 and knows how backtraces work at the machine level. There are some CS33 veterans in the house, right? Nobody wants to raise their hand. Oh, we have a brave person. Yes? Right. So what happens is you look at the machine code for g. It does some fairly simple thing. The last thing it does is it calls f and it commits to returning whatever f returns. Right? So in the source code, what you see is something like this. Int g. Do a little bit of stuff and then return f of n. Right? Where n is defined up here. The last thing it does is call f. And then it returns to whatever. And then in that case, the machine The last thing G does is it just jumps to F without saving the return address. It doesn't need to save the return address because F can just return to main and we've already saved mains return address on the stack which means G's frame no longer exists. So you have to sort of be cognizant of the fact that the backtrace is a machine level thing that doesn't necessarily correspond to your source code exactly. There can be optimizations that throw things out. You can also see that if you look at info frame or info stack and try to look at the values of local variables. Sometimes GDB will say the value of this local variable has been optimized away. I don't know what the value is because it's not available anymore at the machine level. It's not stored in memory. It's not in a register. I don't know what it is. Same sort of idea. All right. Next command. Let's see what other commands do we have here. You can make your breakpoint smarter. For example, you can put a breakpoint at the start of the square root function. B square root says whenever the square root function is called, stop. a look at it. A problem with this kind of approach though is that if your program calls the square root function a million times, you're going to be hitting this breakpoint a million times and then have to continue a million times. You don't have the patience for that and I don't either. So it's reasonably common once you've created a breakpoint and GDB will sort of say, oh, okay, you know, this is breakpoint number 27. at the start of the square root function, whatever, right? Then what you can do is put an additional condition on that breakpoint. So you can say something like this. Condition 27x is less than 0. Because what you really want to do is find out who's trying to calculate the square root of a negative number. You're not supposed to do that. That's a bug. And so you really want to keep running to the program until somebody does that. So you can place a condition on the breakpoint. This will cause GDB to put in the hardware breakpoint as before, but it then tests the condition. If the condition is false, then it just continues automatically. It doesn't stop. And so you can then, when the breakpoint is hit from your point of view, that means you've actually found the condition that you like. Alright. Another one. you can do something like this. This can be any expression that you like. Before, when I last talked about GDB, I talked about this command, right? This says, all right, print out the value of that expression. You know, usually it's a number or a pointer or that sort of thing, and GDB will go and do it. is fancier. What it will do is it can tell you as it's exploring, as it's evaluating the expression, what happens. It's sort of a more detailed evaluation of that expression, right? And this is fancier. And the reason it's fancier is that this part of GDB is implemented in Python. And GDB will let you even add Python code of your own to help GDB explore expressions. Which means that if you're writing some fancy data structure that's kind of hard to print out, maybe you need some conditions in order to print it out correctly, you can program GDB to print out expressions in a useful way. You've got some sort of, I don't know, directed graph. And this thing will maybe put up a nice picture of the directed graph on your screen rather than give you a bunch of low-level hexadecimal numbers. All right. Let's see what else have we got here. You can also do low-level stuff with something like this. Yeah, that's good enough. This says print out the value of the XMM0 register. X8664, well, you'll know what register this is, what it's used for. If you don't know machine code at all, this will be sort of a mystery to you. But, you know, for CS33, you need to be able to do this stuff. More generally, there's another I command. This is short for info registers. This prints out all the machine registers that are are of general interest, not all of them, there's way too many of them, but all of the ones that you're likely to need at the machine level, and print out their values in a sort of a compact way, and it's handier than trying to print them out one by one. It's a good thing to take a look at this every now and then, even if you're not interested at the machine code level, just to see what's sitting in registers, because the registers are fast. This tells you what the program's focusing on, right? It can sort of give you an idea of what part of your program will be faster or slower because if the stuffs in here it's probably going to be pretty fast. All right. Next command. Now we're starting to get more sort of specialized stuff. When you're debugging embedded systems, that is systems with a small CPU designed to work in a particular device, Often the CPU is not x86-64 or not ARM. It's not what your development platform is. It's something else. You need to be able to debug this program that's running on a different machine and a different architecture. And for that, GDB has a whole bunch of commands. I'll just mention a couple of them. Target, and you specify here an architecture. This tells GDB what machine type will be used to run your program. That is, GDB has got in its executable a knowledge of lots of different kinds of machines. A GDB that's running on an x86-64 can debug a program that's running on ARM because the x86-64 program has a bunch of knowledge about ARM. is the way that you tell GDB what you plan to be running on as opposed to what machine GDB itself is being run on. All right. And then we also have sort of remote debugging. You can debug a program that's running on a different machine than you are. your machine, which will have a lot of memory, it'll have Python and all that sort of thing, there'll be a little stub GDB running on the other machine, or perhaps no GDB at all, with a connection by a TCP/IP, or maybe by a hardware serial port or something like that, in which your GDB will be debugging this program on the remote machine. If you do a lot of low-level Internet of Things stuff, you'll be doing a lot of this, because the thermostat that's running the program that you're debugging on isn't going to be powerful enough to run GDB. Next thing, macros. You can think of macros as being extra instructions you give to GDB to make it easier to debug your program. So, for example, you can run this sort of thing. Here's a GDB command. Define you're defining a macro called print look which is I guess short for print location and the body the macro looks like this by convention are one will be the first argument to the macro that's currently being run and this basically says if I ever run this command What I really meant to type was this command. Print star long star 0xAAFC08. Right? Except this is presumably easier to remember and type than the long version. You can put whatever stuff you like in here and then you end the macro with an N. So there's a little programming language built inside GDB or we could call this an extension language. We've seen this idea before of having an extension language. is an extension language called Emacs LISP. Chrome has an extension language called JavaScript. GDB has an extension language which is the GDB macro language. Not only that, but GDB supports multiple extension languages. One of them is this specific GDB macro language I've mentioned. That's one of them. Another one is Python. You can write this kind of code in Python. It's going to be a little bit more verbose, but if you're used to Python, you'll be used to that. And a third extension language is Lisp, because Lisp is sort of the simplest extension language that you can think of that can almost work anywhere. and turn GDB into an all-singing, all-dancing debugger that will kind of do whatever you like. At least that's the theory. Any questions about this? I've left out a lot of features. I wanted to give you a feeling, though, of what's involved in a real debugger. Yes? Is there a way of saying break on square root if it was called from other? Well, the simplest way of doing something like that is to go to the other function, find where it calls square root, and put a breakpoint there. Okay? Is there a condition that says break here if I was called from there? I don't offhand think you can do that, but I could be wrong. I'd have to look at the manual to double check that. Question? Yes. Oh, that's a bunch of low-level stuff that I haven't done in 20 years. Basically, though, it typically works this way. You need a copy of the executable both on the target machine and on the debugging machine because GDB needs to know about the code. So you have to arrange for that. You tell GDB, okay, I'm debugging this file, but then you issue the target command to say, I don't want to run on my current machine. I want to run on this other machine. And this other machine has that particular architecture. So the target command has some options that I left out that will specify the architecture and also how to get started on that other machine. You know, what command to run. Maybe you're debugging over SSH. You just put some SSH stuff in there. And I've forgotten the details. Actually, I never did it over SSH, but I'm sure you could do it. So there's some configuration stuff that needs to be done here. But it can be done. All right. Other comments? All right. In that case, I wanted to finish off our discussion of debugging by going back to the thing that I was hammering at over and over again before we talked about debugging, which was, which I hope you'll recall, is try not to use GDB. So I wanted to finish off with some other alternative strategies. And you should at least consider these before you dive into a 10-hour debugging session with any debugger. It doesn't matter whether it's GDB or something else. The first one is assertions. Most programming languages have this kind of notion. If you're doing them in C, it looks sort of like this. And then somewhere in your program where you worry that you might have a bug, the bug might be that you've calculated the index of the expression is true. And if it's true, things sail along just nicely. And if it's false, your system crashes. So this is a macro that's roughly equivalent to one of two things. First off, it could be equivalent to this. If it's not the case that this thing is true, then abort. Usually there's something more than just crashing. and it'll print out a nice error message just before it dies. Oh, I'm dying on line 27 because this assertion fails. The other possibility is that this thing is equivalent to nothing. It's a NOAA. It's up to you. And the way that you can choose between one or the other is you can compile with this option. If you use GCC or Clang minus DNDebug, NDbug says, I don't want to debug, and that means it does this option. If you don't use this option, that is the default, is to go this route. So assertions by default put in runtime checks, but if you're worried about performance and you want your program to run faster and you say, you know, I've tested it enough, I think the assertions are all going to work out, then you can easily prevent them from doing it. doing anything at all by compiling your program in a different way. So the idea here is you sort of put extra checks in. Can you see any problems with this approach? Basic idea seems pretty simple. What can go wrong? Yes. If you're shipping this code out, it's going to be together. So the question is, are you shipping it with this flag, so the assertions do nothing, or without this flag, so the assertions are enabled? What? Okay. Any issues that you can see? All right. I have an issue. Suppose you do something like this in your code. Assert getcare is not equal to EOF. Getcare is a function that reads one byte from standard input and returns that byte, or if you were to end a file, it returns this special EOF value, which is minus one. Can you see what is wrong about this code? Yes. If your assertion has side effects, then all of a sudden this approach is really dicey because your program will behave differently when the n-debug flag is on versus when it's off. Right? So an important sort of restriction on assertions of this style is make sure your assertions are free of side effects. They shouldn't ever change anything. They should just be read-only expressions. And some compilers are good enough to yell at you in case you violate that rule. Any questions about this approach? Notice the difference between assertions and the unreachable trick that I talked about a couple weeks ago. Unreachable is a message from you to the compiler saying, is always true or always false depending on which way you want. Assertion is a message to the compiler saying, I'm not sure this is true or not. Please check for me. So they're quite different messages. All right. Next alternative is exception handling. If you're not sure whether your program might have a bug, the bug might be, I don't know, Well, your program might divide a number by zero, do integer division by zero or something, right? Well, put it inside an exception handler, right? So you say try, and here's your questionable code. And then you say catch, and you say division by zero. And then in here you do some sort of cleanup. Or warning or something like that. Right? This can transform code that ordinarily your program would die and not make any further progress into a program that it can at least limp along and recover from mistakes as long as the mistakes aren't too serious and you've anticipated them by, you know, in advance. very popular way of trying to prevent bugs from taking over the system. It can have problems though. So what's the idea here? Suppose instead of writing the code this way, you write it this way. If questionable code is less than zero, Let's say if it returns minus one that means you have an error. Then you do the cleanup. So in some sense these two ways of writing the code are different APIs for doing the same thing. Under this approach the idea is you write sort of the code that you normally want to execute and you put the special cases down here. Under this approach, every time you're worried that something bad would happen, you do have an explicit test for it. And so in this approach, you tend to have code, you know, check, then clean up, and then here's some other code, right? Questionable code 2, then you have clean up 2. Whereas over here, you could say questionable code 2 and put it all inside the try. do exception handling here. One is with explicit exception handling features of your language. And the other one is you just use regular function calls and look at their return values. Both approaches have their advantages. The advantage of this approach is you can put the mainline code here. And you can put sort of the special cases, right? The error handling here. Which means that if someone else reads your code, they can first easily see what your program does normally and worry about the special cases later. This approach tends to mix up the two which can make the code harder to read. The disadvantage of this approach though is it can be harder to follow the relationship between bugs in your code, well bugs in your code, failures or that sort of thing, and the cleanup. With this approach, you can be nested five deeps in loops here and then all of a sudden kaboom, control transfers out to here. Over here, that can't happen. So this code can, in some ways, be clearer to understand if you're worried about checking the entire code, including the cleanup cases. Both approaches have their advantages and you can find software developers who swear by one versus the other, but you should know the pros and cons of the two approaches. Any questions on exception handling? All right. Let's try something else. Traces and logs. Under this approach, you're worried that your program has a bug, which is a good worry because most programs do. The way you ameliorate the bug, at least to some extent, is you put in a lot of print statements. Maybe not exactly print statements, but something with the effect of a print statement. Associated with the execution of your program is a log. It just may be in a file somewhere, or you may be sending messages to some other system somewhere else. The point is that your program logs what it does. Every time it does anything important, it issues a log message. Then, when your program breaks, as it probably will because most programs do break every now and then. You can go look in the log to try to figure out what happened. You can look at the tail end of the log just before the program crashed. And that will give you useful information about what happened. So then you can debug it and fix it later. This doesn't prevent the bug from happening. But it can make debugging a lot easier. Because a lot of times in production runs, if the program crashes and you have no evidence, it's going to be hard to reproduce the bug. The difference between a trace and a log is that logs are generated by on explicit requests from the program. So you modify the program by adding print statements to say please log this bit of information. That gives you the most control over what's going to appear in the log. You can specify exactly what you wanted. A trace, on the other hand, is typically generated more automatically. You don't have to modify the program at all. You can simply tell some other program to watch your potentially buggy application and trace important information. A classic example of a trace program is the strace command of Linux. like cat, etc. Password, strace will run this program. Every time cat issues a system call, strace will log it to standard output or standard error or wherever you want to log it. This way, you don't have to modify cat. You don't have to recompile cat. You just get a trace of what it did. And that can be a very valuable thing to have as well. This kind of technology, of course, isn't limited. to just running Linux programs. You'll see this on every operating system, every platform. If you're doing a web application, for example, one of the nicest things to be able to do is go look at the log of the web server. If you're running Nginx, you just look at Nginx logs. That will give you a log of what your server was doing. And there's lots of flags on every web server to make those logs more verbose or less verbose to help you debug what's going on. And that way, you don't have to run a debugger. You can just see what went wrong. All right. Any comments about this approach? I hope I'm preaching to the choir here, right? Most of you have seen this stuff before. You just have to be reminded about it. All right, next one. This one I've talked about with GDB, but it's applicable elsewhere. It's checkpoint restart. And the basic idea here is you modify your program so that it has a checkpoint command of some sort. And what that checkpoint command does is it saves the program's entire state into a file, a checkpoint file, such that you can then start up the program later and tell it, instead of starting up normally, just Read this checkpoint file and go back to the state that were that where you you were before that way if you're worried that your program will crash which you should be worried about you can tell it hey checkpoint yourself once every 10 minutes or once an hour or however often you're worried about some crash happening and then you can then recover the state that happened soon before the crash That can help you either resume what you were trying to do in the first place or reproduce the bug. Checkpoints are very nice ways of reproducing bugs in long-running programs. Any questions on this approach? Let's try another idea. Barricades. This approach is One that I commonly run into when reengineering a large program, but it can be used in other situations as well. But I'll give you the scenario that I'm thinking about. Suppose you have a large, messy program. It's like the garage with the car and all the junk piled into it and the car hasn't moved for 20 years, that sort of thing. It's this big program. You've been given the job of maintaining this program and bringing it into the current software development environment, but it's some old thing that's written in like what, C++ or some ancient technology like that. All right? What you would like to do is fix it up so that it's a clean program, but it's too big to rewrite from scratch. So a common technique to sort of help fix this is to to divide the program into two pieces. The first part is going to be the large messy piece. Using the old technology full of pointers and it crashes all too often. It's a real mess and all that sort of thing. And then over here is the new clean piece. Might be written in C++ or it might be written in some something cleaner, take your pick. Rust Python, I don't care. New technology. And what's happened, or what should happen here, is this piece should have sort of clean data structures that should be reliable. The different parts of the new clean piece can trust each other, that they won't have sort of corrupt information because they're only talking to the clean data. But, of course, It's just part of a larger program. And you're worried that this large messy piece will have mistakes that will go and infect the new clean piece. So what you do is you divide them with a barricade. A barricade is a hopefully small piece of code, an interface, a shim that divides the messy piece, from the clean piece. And the only way these two guys can talk through each other is through the barricade. The barricade can check every argument that's passed from this large messy piece into the clean piece, and it'll be suspicious. There's going to be a suspicious check here. We'll make sure that all the pointers are valid. We'll make sure that all the strings are sort of conformed to the right syntax. That checking, might be somewhat slow, but we're willing to sort of do that extra suspicious checking in order to keep the clean piece clean. So this barricade is designed to at least have this part of your program be reliable and understandable and all that sort of thing. And then once you've built this, you start moving the barricade. This is what version one looks like. Version two You clean up this section of code. Your clean area is now bigger. You have a barricade in a different spot. And in each release you keep moving the barricade over until in the end you're finally done and all that old messy COBOL code or whatever it is has gone away. At least that's the goal or the idea. Sometimes this works reasonably well. Sometimes it doesn't but at least it's something you should have in your This idea of insulating one part of your application from another is so important that you should know about two other major ways of doing it that are even stronger than this. One of those methods is called an interpreter. And the basic idea of an interpreter is you have a part of your application that you don't entirely trust. Maybe it was written by your predecessors who wrote buggy code all the time. Maybe the code is actually written by someone out on the internet. You don't even know who wrote it. You don't really trust them at all. So the way you defend your application against bugs in that other part of your program is you write an interpreter. So for example, you have a JavaScript interpreter. You can make it as efficient as you like. You can write it in C or even in assembly language. But the point of this JavaScript interpreter is that you feed it a whole bunch of JavaScript code and it runs it, but it runs it suspiciously. Every time this JavaScript code computes a sub i, that is every time it does a subscript, it checks the bounds of the array and makes sure everything's in range. And if not, the interpreter says, I'm not going to run this part of the program. I'm going to throw an exception or I'm going to return back to the C level of something. I won't let that bad code run. In effect, what an interpreter does is it takes this sort of dicey code, and in some sense single steps through it looking for errors all the time. A downside of this approach of course is it's going to be slower but an upside of this is that your interpreter will catch a wide variety of bugs in the DICY code and refuse to let them infect the rest of your program. So it's slower and it's defensive. And this is a standard technique in a lot of systems. It's used in Chrome, it's used in Emacs, it's used in GDB, lots of other systems use it, and you can use it in your own application as well. Okay. Suppose this isn't enough defense, though. Is there something that you can do that's even more resistant to bugs in that code that you've downloaded off the network? Can anybody think of another technique that's even stronger than having an interpreter? Yes? Louder please. Virtual machines, very good. I ran out of room here so I'm going to make a little extra room here. There's a lot of virtual machine technology out there. Basic idea originally was pretty simple. Instead of running your program on a real machine that's really running Linux, you run it on a pretend machine. That pretend machine isn't hardware. It's just a program that pretends to be the hardware. And since it can pretend to be the hardware, it can do anything the hardware can do except The virtual machine will be somewhat suspicious. So this runs instructions suspiciously. I'll put suspiciously in quotes because the exact level of suspicion depends on what kind of virtual machine technology you're going to use. In the classic virtual machine technology, you do every instruction individually in software. That's going to be too slow. So there's a bunch of hardware assist approaches that will let you do the non-suspicious instructions at hardware speeds and only do a few instructions in software and that sort of thing. There's a variant of virtual machines called containers, which one can think of as being lightweight virtual machines in which both the emulated operating system, and the real operating system are the same. So you're sharing the OS code. That makes containers cheaper and lighter. But it also means there's more opportunities for attackers to get at the underlying machine, even if you're only letting them run a virtual machine, because they can see the operating system that they're running on. So there's a tradeoff here between safety and performance that people are constantly tweaking. We're coming out with new versions all the time. But the basic idea here is that this is probably on the extreme of these other sorts of checks. When you're running on a virtual machine, you don't need to make changes to the source code, right? Because as far as the program is concerned, it's running on a different machine. I guess you could go even further. You could run the program on a completely different set of hardware. That's even further than virtual machines, right? And there's other sort of approaches you can do you're willing to go the hardware route. All right. Well, that's enough debugging. Why don't we take a break? And when we come back, we'll be talking about, what was I going to talk about next? Oh, Python. We'll do a little bit more Python. I know you guys love Python, so we'll see more of that. All right, let's start up again. So, well, it's time to go back to our favorite little programming language, and by some measures, the world's most popular language, and you've already written code with this. But instead of focusing too much on the low level, we'll start with that. There's some low level stuff. I want to work our way up to how Python packages work. So first off, what does this piece of code do? A little bit of review. It defines a function f, which takes two arguments. It returns the sum of the arguments, plus one. And then what does this do? in your mind sort of a model of how Python works to understand this code. And that model we're going to be reusing later. So it's important to get it done right in the first place, right? What this sort of thing does could be written in a different way. Python has a syntax that looks like this. And this thing which I will now parenthesize is called a lambda expression. It's a nameless function. It's an object in Python. So you can assign it and stick it in a list somewhere. Do whatever you want with it. But it's an object that you can call. So this nameless value that I immediately give the name f is an example of what we can call the callable objects. in Python, right? Or callables for short. The simplest callable is a function, and here's an example of a function. Since it's an object, you can pass it around. Here we say take the value of f, which we can define this way, and assign it to g. f is just an object. Internally, it's a pointer to a piece of memory that contains the machine code for this. Whatever, it doesn't matter. We'll take that pointer and we'll assign it to this G pointer. Later on we can assign 27 to F. The function's still there and you can call it by the name G. So if we now say something like this, finally this X plus Y plus 1 code will be executed and that will compute the number 31 and assign it to X. Alright, so that's how functions work and it's how method works. I don't know I'm just making stuff up doesn't really matter right the point here is that you can put a bunch of stuff under this class but when you're done when you finish defining the class c is another object its type is not function its type is class and later on if you don't like the name here you can say you know c and this has the same meaning that g equals F did over here, you're just assigning a reference to a class from one variable to another, and you can now use the class under either name, it doesn't really matter, right? Okay. Now, in Python, we have the notion of inheritance, which you were probably told about in CS 31. Everybody here remembers inheritance and how it works and what it's good for. Can somebody tell me what inheritance is good for? Nobody wants to tell me. Yes. It makes code less verbose because? How does it actually do that? You're onto something, yes? So the idea is that the child inherits code from the parent and thus you don't have to sort of reproduce all that code in the child. You just get it automatically from the parent, right? And C++, like Python, has the notion of multiple inheritance. It's better to have two wealthy parents than one. Let's inherit from two parent classes, A and B. And you can put an X and Y and Z here. Inherit from as many people as you like. So what inheritance does is it sort of works as follows. If you have this class C in Python and you have two parent classes A and B, those parent classes might have further parents. Say P and Q and R and S, right? And it could be that A and B will share some parents. So for example, it could be that A and B have a common parent R. Now, suppose you create a new object of type C, right? say, oh, I'd like to invoke the M method on that object. Python now needs to look for method M somewhere in that diagram. Now, the simplest case is we're down here in class C, and class C defines M. In that case, we're calling the method M right here. But it could be that we invoke some other method. Let's say method N. that isn't defined textually in C. And in that case, Python has to go looking for an ancestor that defines N. Which ancestor should it look for first? The algorithm Python uses is the following. It does a depth-first left-to-right search over the ancestor tree, or ancestor graph in this case. at this point it could look here but why bother it's already been there and whichever one of those ancestor classes defines n first the first one of those that defines n that's the n that gets called. Is that the same algorithm that C++ uses? We got a bunch of C++ experts in the house. You learned about inheritance. C++ has multiple inheritance. So you should know the answer to that question. I will make a confession though. The last time I wrote C++ code for real was in like 1993. And I ran screaming away from the programming language and resolved to never write a line of C++ code again. So I don't necessarily know the answer to the question. just gave you but you should know and I hope you look it up before the final exam comes around. Because inheritance is a tricky thing. Different languages will treat it differently. This is how Python does it. JavaScript does it in a different way. C++ does it in a yet different way. If you're serious about doing object-oriented programming you've got to know how it works. All right. So next topic. Classes involve namespaces. A namespace is a fancy word saying, "I have a bunch of names in here," and each of these names have a value. So it's a space of names. The names all have to be unique within a namespace. You can't have two duplicate names in the same namespace. But other than that, there's not too many restrictions on it. So here's how it works with Python. First off, I already said this, but I'll write it down. A class is an object. Is that true in C++? Are classes objects? All right. Remember, I don't know C++. You're going to have to help me out here. Man, we taught you C++ for 10 weeks. 10 weeks? This is like one of the simplest questions you can ask about the language. Somebody's going to be giving you an interview question like this. If you don't have the answer right away, boom, they're off to the next interviewee. This is true in Python. It's not true in C++. I don't know very much C++ so I at least know that. Second, it has a member which is called underscore underscore dict underscore underscore underscore. That is, if we define a class here, you can say C underscore underscore dict underscore underscore. These underscores are danger signs. Whenever you see a bunch of leading underscores in a Python name, that's a reserved name. you're not supposed to mess with it unless you really know what you're doing. And if you do mess with it anyway and you get into trouble, that's your problem, not mine. So this name, as you can sort of tell, this member here, is a dictionary. A Python dictionary. You remember Python dictionaries. We talked about them like three weeks ago. It's like a hash table and that sort of thing. And this dictionary contains its names. So if you have the class C, you can look up its dictionary and in that dictionary you can look up the name M like this. And that will tell you what M is in class C. It's a method. this object, this expression evaluates to an object of type method. It's a bound method and then you can call it and all that sort of thing. It's as if you just wrote O.M except now you've got it sort of by another path. So be careful about these names, right? You can sort of, you know, these are private. a little bit of the details about how Python works internally. Okay. That's one way we have namespaces in Python. Right? Every class establishes a namespace for the things that you define in that class. But it's not the only way in which you can use namespaces in Python. There's a different way. And that's Python modules. Python modules typically live in individual source files. So you will have a source file called foo.py and you will implement the Python module foo by putting some Python code into foo.py. So here we go. This code can say 8 equals 19. This code can say define f of x to be return x plus 1. You know, whatever you want. Oh, let's put in a class here too, right? Why not? Class C. Class C does some stuff. Put whatever Python code you like in here. You will then have a module. And the idea is that you can use the module with a statement that looks like this. import food. So what this is is an executable statement. And in that sense it's different from the way import works in C++. In C++ import is a directive to the compiler. Hey go look up all this stuff and all that sort of thing. Python doesn't work that way. In Python, you import stuff at runtime, not at compile time. You can do this in Python. If x is less than zero, import foo. You can't do that in C++ because you can't do directives like this in a way that depends on runtime behavior. It's just not allowed. All right. So what does import foo do? Three things. First, it creates a new namespace. Basically, you can think of a namespace, at least in Python, as being a dictionary. It maps names to values. This new namespace initially starts off being empty. There's nothing in it. Second, it reads and executes the code in foo.py in the context of this namespace. So when we assign 19 to A, we're not changing A in the program that called import foo, because that's running in its own namespace. We're changing A in this new namespace that just got created. The names that we define here won't collide with the names in your program. And then the third thing we do, after we've done the first two things, is we add a name, foo, in the importer's namespace. And this name foo is bound to the new module. which has this new namespace. The syntax for using sort of one of these names that we've read in will then look something like this. foo.a. If I write foo.a here, I am looking up in foo's namespace the identifier a that got defined here. All right? See how that works? Nothing to it. What can go wrong? Your engineers, one of the first things you should do when you see a feature either newly or explained is to think through how can this screw up? What can we do wrong here? Yes? A imports B which imports A. Very good, right? So this thing says import bar and this thing says import foo. Oh, we're going to be in trouble, aren't we? One of the ways Python tries to get around that problem is if you've already imported foo and it says, oh, I've already done that. I'm not going to do it again, right? To save time. That will break the circularity, but not always the way you like. Anything else? Yes. If you already have a variable named foo, yes, this will cause a problem. And the fix for that is going to be the same fix you use for any other variable collision. That is, if you have a bunch of code here and you're using foo for two different things, stop doing that. Rename one of your variables. Right? So rename the other foo that you've got so you don't have a collision. Yes? Yes. Now we're starting to see some of the issues. So, for example, you can do something like this. From foo import f. This acts like import foo, except at the end what it does is it binds f in the importer's namespace to f in foo. You see what's going on here, right? So the idea here is all we want is f out of here, and we just want it at the top We don't want to have to say foo dot f all the time. That's just verbose, especially these module names get long. Who needs all that stuff in the code? So you're going to see a lot of code that does this sort of thing to limit or control sort of what names get imported. Occasionally, you'll see something that looks like this. This means take all the names in the imported module and plant them in the importers module and What that means is at this point not only can we use F we can use a we can use C All the names that were in foo We can just use them directory without that nasty annoying foo dot in front of each name There is a downside to this approach though If you write code like this You are at Fu's mercy. Fu, a later version of Fu, might define a new name that overwrites one of your names. You lose. You told it, I wanted all of Fu's names, even if they override mine. So my suggestion, at least unless you have a lot of expertise or you're very friendly with Fu's author or something like this, don't do this. That's trouble. Just use a plain import or use a from foo import and then list exactly the names that you want. That way you'll know all the names that ought to be visible in the importer's namespace. All right. Let's see what else. Once you've done something like this, say import foo, you then have, for example, a built-in function called dir. of foo, it will tell you all of the stuff that's in foo, right? All the names. You can sort of get a dictionary of everything that's in that. You can also run this because there is a primordial sort of module called the built-ins module that defines all the names of all the built-in functions that you know and love already. There is a different way to import a module that's very commonly used by people who want to write standalone Python applications. And it's worth mentioning in its own right, which is, if you're at the top level in the shell, you can just say, Python, fu.py, right? In some sense this acts like this. Right? What it does is it creates a new namespace, the startup namespace, it reads and executes the foo.py code in that namespace and then it adds the name foo in the importers directory but by then that time your program is exited. However, there is an additional hook that you can use in something like this. So this sort of acts like, you know, import foo. at the top level. Except with an important change. It binds this name, underscore name, underscore, underscore, to this string, underscore, underscore, main, underscore, underscore. The idea being that if you look inside foo.py, there's a good chance that you'll see somewhere, typically towards the end, something that looks like this. If name equals equals main, run some test code. Very common convention, right? So for modules that are normally not intended to be run as standalone programs, you make the test cases the standalone program, right? And the unit tests, these unit tests will basically just test the code in your module, and this will be a quick way to make sure that the module is working in your particular environment. If, on the other hand, you want to write a standalone application, a Python application that actually does something, then here, don't run unit tests, run your app. This is, in effect, like the main function of a C++ program. It's the stuff underneath this if, and you put in here whatever you like. All this in mind, let's see what else we have here. We have packages. Let's put them over here. Why not? So far we've covered classes. Then we did modules. Now we'll do packages. And one way to think about packages is that they are a hierarchical As your application grows or as your the set of things you want to use grows you'll find that you'll have more and more and more modules and yeah you can keep them all in a bunch of files in a directory but eventually that directory is going to have hundreds thousands of files it's going to be hard to keep track of them all so it's very common to put them in a tree structured hierarchy instead top level we have a directory underneath that we have some subdirectories underneath that we maybe have even some more subdirectories and eventually we get down to something where okay now we can just have some modules the modules are boxes the directories are circles the modules don't all have to be at the same level you could put you know a module at this level as well but basically the tree structure that you come to know and love from the Linux file system. You just use that to organize your packages. And by convention, Python uses namespaces and dots to keep track of this stuff. So if at the root here we have subsidiary subdirectories A, B, and C, and under here we have A, and D, and F, and A, and that sort of thing, and that sort of thing, and here's a G, and an H, and an a.py, then you can say something like this. Import. Let's say we wanted to do foo.py here. Well, we'll do b, d, g, foo. And this has the effect of this import foo.py over here, except in effect it takes all these dots in the name, turns them into slashes, in the directory hierarchy and then uses that to find out where your file is. You can think of packages organized in this way as being sort of like a library of packages. I'll put library in double quotes because there's no sort of standard definition of library, but this might be one definition of library. It's an organized collection of packages with a nice set of names. Now, I guess I didn't really say what a package was yet. Alright, so far I've only talked about modules and how to get them. A package is one of these directories with a special sort of file. It's called _init.py. So if in here we have an init.py, the convention is if we load any module from this directory, what Python will do first is do whatever initialization you have in this file. This specifies initialization for the package. For example, it could say, oh, if you want to use any modules in this package, you really need to import all this other stuff first. So it'll import all the other stuff first and that sort of thing. So once you have this, you now have a way of organizing your modules and sort of saying which of the modules sort of, fit together and that sort of thing. And you can now do things like this. This dot dot can be interpreted in the same way as dot dot in a file name. It means if I'm sitting here, go up a level and then import foo dot p1. So you can navigate through this this hierarchy using things that sort of look like path names in the from directive. Now I have like a dumb question here. You have a smart question I'm sure. Yes. Yeah. source code never mentions underscore underscore name underscore underscore. That means it won't notice. It'll act the same way. Regardless of whether you're running it from the top level or you're importing it, it'll do the same thing either way. However, if this source code refers to underscore underscore name and does something different, if it equals the string main, that means this code is prepared to be invoked from the top level. And when it's invoked from the top level, it'll run this code. It'll just kind of just do a bunch of definitions and then do nothing else. Right? So this is Python's way of letting you know, hey, you're being run from the top level. Do you want to be an application? And if the answer is, yeah, I want to be an application, you should probably have an if that looks like that in your code. Question? Yeah, you can think of it as a predefined name. The Python interpreter will define it for you to be your name. And at the top level, that's your name. Yes? Python will set name for you, right? It's one of these underscore variables. So you shouldn't mess with it unless you really, really know what you're doing. And I would be reluctant to do that myself. I mean, you can do it. maybe something will happen but you're better off letting it set name for you and then you just read it and do stuff based on that. Okay other comments about this sort of thing. Now it's possible that you will have more than one library. So for example if you've logged into CSNET there is a standard Python library sitting somewhere in the system. but you may want to have your own library or maybe you're doing a programming assignment and it says well to help you do this assignment here's an extra library you can use and then you decide well I'd like to have that library and I'd like to have my own library too right so how do you manage multiple libraries there is a simple way to do it and then there's a fancier way to do it we'll start off with the simple way There is an environment variable called python path. Now you'll recall the path environment variable for the shell. You can set this in the shell, in the environment, just the same way that you can set ordinary path. You can set python path. And you can set python path to be something like this. And then after that, I don't know, u class CS35L pylib. And then after that, I don't know, user local pylib. Something like that. As many directories in here as you like, separated by colon. And the algorithm Python uses when you say import is it searches that path. Just the same way an ordinary shell execution searches the path, the Python import directive will search that path. It'll first say, oh, if you're trying to import A slash B, I'll look in home python live slash A slash B and I'll look there. If it's not there, I'll look in uclass CS35L pyylab A slash B and so forth and so on. So it walks through your Python path left to right, looking for a match, and the first one that it finds, it imports that. Now, there is a more complicated, but I think it's fair to say a more common way to deal with this sort of thing, right? With is the Python package manager. or maybe I should say managers. There are several different Python package managers out there. Probably the most popular one is called pip, which I think is short for Python install package or something like that. I've forgotten what it stands for. But the basic idea is that this is a program that will help you get libraries, typically from some source off the net and install it into your personal Python library or with some options and if you have privileges into some of these other libraries, saving you the overhead of setting up this Python path because the default Python path will look into wherever pip tells it to. So you can run the command pip install say numpy. This says I want the numeric Python library installed in my Python pass somewhere. It will pull that stuff in. There's a lot of stuff there these days and install it so that later on you can simply import modules from NumPy without having to worry about all this other stuff. You can also do pip uninstall NumPy to get rid of it. in your own package library because you don't need it anymore. Maybe it was getting in the way. You can do something like this. This will list the packages that you have installed. You can also use an option that looks like this. This will list all the packages that you have installed that are out of date. That means upstream. with a new version of NumPy. You're still running the old version, right? So this might be some sort of impetus for you to update. You don't want to necessarily update, but this will be some way of sort of telling you what you've got. Or you can do something that looks like this. Use the JSON option. That will list out all the packages you have in a standardize JSON format that a lot of other software tools will understand and be able to use as opposed to the text format that pip uses by default. There's a bunch of other options like this. I'm not going to go into all the details, but basically what are some of the problems that we run into when we're installing packages like this? First off, dependencies. You might want to install scipy, but it's not going to work unless you have numpy already installed. Each package will tell you what it depends upon. And by default, when you say, please install this package, pip will say, oh, there's these other packages that this guy depends on. I'll install them for you too. Often, that's a convenience. Sometimes it can be a disaster. You pull in stuff that you didn't want. So dependencies are sort of an often running issue here. Another issue here is that occasionally there will be something you need from Python that you can't fix by writing libraries. This is the most common way of extending your Python installation. In effect, you're saying extend Python. by making available a bunch of modules that weren't there before. That's an extension. But sometimes you'll need something from Python that you can't implement this way. The Python developers hope that never happens, but in practice it does. In that case, what can you do? Instead of trying to extend Python by adding libraries, and try to extend Python by changing the Python interpreter itself. And I hope your initial reaction to that is what? That sounds like something that's fairly heavyweight. It's actually not all that hard, right? written in C. You guys know C. You've actually done a programming assignment in C, right? So you can get the source code to Python and have it behave differently and run it the way you like it. And as long as the change you make is compatible with all these libraries, you'll be happy and everybody else will still be happy because they'll be running the standard Python. Have I gone nuts here? Or am I on to C? something. People are looking at me like I'm nuts. Yes? How does it what? How does something like a virtual environment change the Python path? How do we change the Python path? Well, the simplest way is you just go change the environment variable, but I'm not talking about changing Python path or changing libraries or something. I'm talking about, you know, say you don't like using semicolon to separate statements. You'd like to use AccentGrav instead. All right. Well, that's kind of a dumb change. But you get the idea, right? More likely, it's something like this. Your Python doesn't do asynchronous I.O. You want to do asynchronous I.O. because you want to do our class assignment, right? Now, Python, 20 years ago, did not have asynchronous I.O. You want to change Python to make it easier to write the kinds of applications that we're doing in class today. How was this done? People wrote some code. They changed the Python interpreter and then they did what's called a PEP, which is short for a Python enhancement proposal. This is a written proposal that's sent to the Python community in which you're saying we should really change Python so that it has this new feature. You typically don't want to remove an old feature because people don't like that. But as long as the change that you make is upwards compatible and if it satisfies a real need and if you can show that you've built it and it works for you and it doesn't hurt performance and a whole bunch of other stuff like that, you can talk them into it. They'll add it as a new feature to Python version 3.whatever. Right? So the idea here is that we have a living language here, not a dead one. Right? There is no perfect programming language. They all have shortcomings. The ones that are active, the ones that are being mutated, are the ones that are actually sort of working. Yes? Typically what will happen is you'll change the CPython which is the most common implementation. You'll have a patch for it or something like that. But you need to write up a spec for the change that you want that's independent of implementation because not everybody uses CPython. Some people use the Python that comes with the Java virtual machine. There's a Python interpreter there that's not CPython. It's their own Python that actually compiles into JVM bytecodes. And so whatever proposal you make you want them to implement eventually so you need to write up the spec for it as well as typically provide an implementation. Other comments on how this works right so we have a community here that's evolving as opposed to being something static. Pretty much everything I've written here is also done in node right where I wrote pip here. You can mentally substitute NPM. The options may be spelled differently, but all the things I said you can do with PIP, you can do with NPM. The problems that I mentioned with PIP, those problems are also going to occur with NPM. So there's a whole bunch of sort of package management issues that once you've seen it in one of these interpreted systems, you can sort of see how this is going to happen with other interpreted systems. Well, I have one sort of major area to talk about in build systems that we haven't, we won't really have time to finish, but I am going to try to get started with it. So far, we've been doing, in some sense, the easiest kinds of builds, the easiest kinds of packaging, because we've been focusing on languages like Python and NPM, that are essentially interpreted languages. So you can ship sort of Python source code from point A to point B, and as long as it's reasonably portable, which most Python code is, it's going to run. There's a generalization of this problem that's quite common that's going to be harder to solve. And that's the problem of how do you do all this stuff if you're not writing Python code, but you're writing C++ code? or C code or Rust code or something like that. So here the problem is we want to sort of have build tools for, how shall I say it, executables. That is, executables built by compilers that generate machine code for particular machines. This is going to be a more complicated situation because you can't simply ship an executable from point A to point B and expect it to work. You can't even ship an executable from one x86-64 machine to another one and expect it to work because there's lots of different variations of x86-64. I just got a bug report today from someone who's running on an old Apple Mac that's still x86-64 base, and it doesn't have a particular vector multiply instruction that I thought everybody had. Right? So we need to have a system that will work even for sort of things like this. And once you try to build tools for this, what you'll discover is that your audience is going to be a little bit more complicated than it was when you were doing sort of libraries for interpreted libraries. languages. So the audiences include the followers. First off, developers, of course. The people who are actually writing the code and trying to get it to work. Second are the builders. These are the people that compile the code. They're given the source code. Their job is to build the executables. They might be developers, but more typically there's somebody The third audience is the distributors. These are the people that take the executables that the builders built and get them out to the audience. That is to the machines where the code will actually be run. The next part of the audience are the installers. These are the people that take the executables that have been distributed to them by the distributors and actually install them on the end users machine. I suppose I should also list end users as part of the audience but I'll leave that out for now. These audience members here can either be human or automated. The builder could be totally automated. There's no person involved at all. somebody supervising them and we have to talk to the supervisor to make things work. All right. So with this audience in mind, you've got a bunch of interesting software here. And of course, it'll be a lot bigger than this. Here's the source code. And your job is going to make sure that it all gets installed correctly. What are you going to do? Well, the simplest way to do it is write a script to do it. You'd like this stuff to work, right? So your script will say, okay, let's run g++ of foo.cc, and let's write gcc of bar.c, and then let's link them together, g++ of bar.o, foo.c. And what I'd like to argue is that although this approach will work, it has problems. And a lot of the technology that people have developed over the years to automate this sort of thing attempts to work around the problems of simply using a shell script or maybe a Python script. program or whatever your favorite scripting language is in order to automate the build of a system. And I don't have enough time to go into the details now but let's take a break now and start up next week and we'll talk a little bit more about this. So next week I plan to do more build tool stuff. I'm going to talk about database systems and I'm going to talk about security and that'll probably be enough to finish off the course.