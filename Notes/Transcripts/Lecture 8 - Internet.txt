Speaker 1
So the idea today is to talk more about Internet technology that eventually leads to the web and sort of web technology as well. Right? So we're doing networking and we're doing something leading up to web services which is the kind of technology that you're doing in the third homework assignment and that you'll be doing in your project. Last time we got the very start of this because we talked about the difference between circuit switching and packet switching. The internet of course is based on the latter technology and I wanted to talk about some of the limitations of this technology and how we overcome these limitations when we're building the web. All right, so basically, what's a packet? It's a piece of data, usually not too large, maybe a kibibyte. It's sent as a unit over the network and something I didn't tell you last time, it's kind of divided into two pieces, a header and a payload. The header is looked at by the network. The network wants to look at that header because it wants to know where is the packet going to go, what kind of packet it is, and all that sort of thing. Right? So this is meant for the net. It's meant, for example, for routers. The payload is intended for the application. The sender puts whatever data they like into the payload. The recipient grabs whatever data they like out of their So this is for applications. I would like to say it's a hard and fast decision as to what goes into the header and what goes into the payload. Sometimes it's a judgment call. Different people can disagree about what should go into what, but this is the basic idea. And something else that's important when you talk about packet switching is this funny word called protocols. Packets are exchanged over the network via protocols. That word comes from diplomacy where you have a protocol. If I want to get a message to President Putin in Russia, I can't simply get them on the phone. I have to go through diplomatic channels. There's ways to ask permission for me to talk to President Putin. And if I don't follow those proper channels, I'll get ignored. That's what we're going on here. So network protocols are agreed upon ways to get the attention of a server or of a client. And if you follow those protocols, your application will work. If you don't follow the protocols, your packets will get ignored. Just like a diplomat would be ignored if they didn't present their proper credentials. Now, something that's different about packet switching versus, well, circuit switching or versus diplomacy are three basic problems with packets, right? You're shipping the packets out on the network. The network's making its best effort to get the packets to where you want them to go, but it doesn't always work the way you want, right? So the three basic problems is packets can be lost. You put the packet into the nearest router saying please send this to MIT and it never got there. Now this could happen for any of a number of reasons. You might superficially think that it'll happen because one router tries to send a packet but the wire goes bad so the recipient router doesn't get the packet but that's usually not what actually happens. the reason the packet doesn't get there is a router gets overloaded. Routers have a finite amount of memory. They're getting packets left and right. They're sending packets left and right. That finite amount of memory is a buffer that basically says, well, if we get too many packets all at once, we'll save them into RAM and then we'll ship them out eventually. If that buffer becomes exhausted, packets get dropped silently. It's not like there's an error message that pops up on the screen. It's just a normal thing. Packets get lost all the time. So that's the first problem that we have to deal with if we want to do the web or if we want to do web technology is realize that those packets are going to get lost. How bad will it be? Depends on how busy the internet is. If it's really busy or if your corner of it is busy or if the corner your packets go through is busy, you're going to see a loss rate that can be 1%, 10%,

Speaker 2
50%,

Speaker 1
I hope it's not that bad, but it's definitely non-zero. And you're going to have to take that into account when you write your application. The second thing, the second problem with packets is packets can be received out of order. The sender sends packets in the order A, B, and C. The receiver receives them in some other There's six possible combinations. You can get them in any of those six combinations. If each individual packet is an independent thing, it's an independent request or something like that, then getting the packets out of order might not be that bad a deal, right? Okay, well, I didn't get my request in until something that I thought I'd sent it or that sort of thing. But out of order can be a more serious thing. Suppose you are submitting your assignment to Gradescope via the internet and the packets in that submission are received out of order on Bruin Learn or on Gradescope. Does that mean you should get a zero? Right? Because the auto grader says this is totally bogus. This program makes no sense whatsoever. No. We want this sort of thing to work, you know, even if it's received out of order and even if the application really cares what order it gets the data. The third thing, and this one seems a little less obvious, is that packets can be duplicated. Right? That is, the recipient can get two copies of the same packet. Now, one might say, How in the world could this happen? Is it like a wire has gone bad or something? No, that's not going to cause it. Is it because a router gets confused and sends the packet twice? Not really. Usually this can occur because of network misconfigurations. It can occur for other reasons. There are different kinds of routers or different kinds of network connections there are routers and bridges. If a router gets misconfigured as a bridge or vice versa, then all of a sudden it can start spewing out excess packets. Network misconfigurations unfortunately happen all the time. I'm sure there are many routers at UCLA right now that are misconfigured. The network's designed to be robust in the presence of these misconfigurations, but this is one of the things you have to be robust in the presence of. You might get duplicate packets. All right, so we want to address this problem. One way to address this problem would be to write every program and deal with this in that program, right? So you'd write a C++ program, program would receive packets and it'd start checking, oh, did a packet get lost? Did a packet get received out of order and all that sort of thing? Everybody who wrote a program that dealt with the would have to deal with all three of these major issues. There's some minor issues but these are the top three. That would be insane. It would be like saying all I wanted to do was send a little message to grandma and I gotta write all this code. That's sort of ridiculous. Now one might say well we'll just put this into some standard C++ libraries and have everybody use the library code. That would be a better approach. And in a sense, that's what we want to do. But the problem with that approach is that C++ might use one technique, JavaScript might use a different technique, and then you couldn't have a JavaScript client write to a C++ server. So we can't simply solve this problem by asking people to write code in every application. that's ridiculous or even using libraries because people use different libraries and the libraries have to talk to each other. So the way that we address this issue is a more general solution. It cuts across software development technologies and that solution is something I've already mentioned. It's to have protocols where the protocols specify standard ways to deal with this problem and then as long as long as your C++ or JavaScript library is compatible with the protocols, you'll be off to the races. You can just use the library, not worry about these problems, they're solved at the protocol level. The library support the protocols and then you can just say print "hello world" and actually expect this message to get through. All right? So we want protocols to solve these problems and the basic way this is done is not just to have protocols, but to do the protocols in layers. So I'll call them layered protocols. The reason we want to have these layers is we don't want to necessarily solve all these problems at once. We want to be able to, maybe some applications really need to, you know, deal with lost packets on their own, maybe some don't. We want to have multiple levels of abstraction. across what goes, you know, about what goes across the network. And we do those multiple levels of abstraction by having multiple layers of protocols. Okay? Now, different computer science experts might disagree about how many layers we actually have. You can go to different sources. Some say we have three layers. Some say we have four. Some say we have seven. I'll give you my own opinion, but don't try to think that this is like the only way it's done or that I'm the only right, no, no, no, no. This is just Dr. Eggert's opinion. It's, you know, it's good enough for us to get things going in this class. All right, so here are the layers. The lowest layer is the link layer. A link layer protocol only talks about two devices talking to each other. to each other. And this protocol is typically going to be very specific to the kind of networking hardware that you're using. You'll have one link layer protocol, say, I don't know, for Wi-Fi. In fact, you might have one for Wi-Fi 6, right? You might have one for, I don't know, Ethernet. And so forth and so on. Whatever device is sitting here on this node, it has to be, say, an Ethernet device. We have to have a similar Ethernet device here. They have to talk the Ethernet protocol. That protocol is written down somewhere on this big documentation thing, all that sort of thing, but it's specific to the particular networking type you're using. That's one thing. And the other thing about the link layer point of protocol is it only talks about a single link from one node to an It doesn't talk about how to talk to, you know, Boston from here, not unless you have a wire stretching all the way from here to Boston, which you don't. Any questions on the lowest level here, the link layer? To some extent, this is kind of an obvious thing to have. You need to have devices talk to each other, right? So, I don't

Speaker 2
know,

Speaker 1
there'll be a link layer for USB, whatever. Everybody sort of can see the need for that. The next level up has the somewhat unimaginative but very important name of the Internet layer. The Internet layer doesn't just talk about a point-to-point connection. It talks about the whole Internet, all billions of nodes on it. So all of a sudden the picture looks a lot more complicated. All these guys are talking. There might be a satellite connection, all that sort of thing, right? So this might be a long ways, and so forth and so on. All right, so it's much more, a much bigger network than the link layer. But still, the idea here is we want the Internet layer protocol to be as simple as possible while still capturing everything you need to do in order to do packet switches. So the basic idea of the internet layer is that all you have here are packets. And the packets need to be sent to destinations, right? From sources to destinations. But we don't really specify too much more than that, at least at the protocol. Now, implementing the internet layer might be fairly complicated. We have to make sure that those routers don't get, you know, over, get too busy and all that sort of thing. But the protocol itself is pretty simple. We are sitting here, we want to get over here, we put a packet into the system and we hope the packet gets out here. The assumption at the internet layer is that we have all three of these problems and that it's up to each endpoint and figure out what to do about it. All right. The next layer above the internet layer is the transport layer. It builds on the internet layer by saying, oh, we have the same nodes as before. They're sending packets like crazy just like before, but we want a higher level of abstraction because we want to assume that you can set up a data channel between point A and point B, whatever those two points are. So you can think of this as sort of introducing the notion of data channels. The difference between a data channel and a packet is a data channel can contain as much data as you like. It's not limited to one kibibyte or some really small number like that. So you have a terabyte of data to send, you create a data data channel you send it over and it gets there. Also data channels at the transport layer typically anyway are reliable. Those three problems that I mentioned don't exist at this level of abstraction because the implementation of the transport layer sort of addresses this problem. But yes question? All right but with the transport layer basically what you have are data channels and nothing else. The level above this is the application layer. Here the idea is you're not just sending streams of bytes from point A to point B. You're sending streams of bytes that have meaning so that the sender and recipient know what kind of application is being run. for example web services or video channels or that sort of thing. Each of these sort of protocols at the application level might be very application specific. It's only good for one particular kind of application. It's built atop the transport layer, maybe built also partly on top of the internet layer, and it's more specific to what you actually want to do. Any questions about these four layers? Yes? Are

Speaker 3
these layers just like a set of rules that these packets or these data channels must follow? What exactly is the actual entity that you play

Speaker 1
with? Well, what's actually going on at the hardware level is this. We're sending a packet from one node to a neighboring node in the internet. That's all there is at the lowest level. This level provides a level of abstraction where, yeah, sure, what's really going on is da-da-da-da-da. But at this level of abstraction, all we really care about is we're sending it from here over to here, and the network sort of deals with the details. We don't have to worry about it as a user. So, but still, underneath, this is what's going on. Similarly, up here, from the sender and recipient's point of view, you just set up a data channel, you start sending bytes, they get here reliably. You don't care that they're split into packets. It's the transport layer protocol that figures out how this is implemented in terms of packets and so forth and so on. So all of these layers are built atop of lower layers and at the machine level it always looks like this. Yes? Well, that's a good question. The link layer doesn't know about IP addresses. All it knows is I'm shipping data from here to here. The internet layer knows about IP addresses. All the layers above the Internet layer, because the Internet layer has IP addresses, can use IP addresses. So IP addresses are introduced here. Other comments? You may, if you've ever configured a Bucky device, seen something called the MAC address. How many people here have dealt with a MAC address in the last week? The rest of the internet doesn't care what your MAC address is, it's just a private secret between you and your router. So there are addresses down here, it's just that nobody up here cares about them. Sorry, I think I cut off somebody. Another question? Alright, yes over here. Which

Speaker 1
layer is solving? Solving these three problems? Well, to some extent, the transport layer solves all three of these problems. Okay? These other layers solve some other problems that I haven't mentioned. I sort of picked on this because packet switching, this is sort of the biggest problems, and to some extent, the transport layer was an attempt to implement something that kind of feels like circuit switching, even though the underlying network is really packet switched. Question? Yes, that is, it's not like this is codified into law and you have must have an application layer knowledge or thing. This stuff is to some extent loosey goosey and there are other sort of ways of dividing the layers. What really happens is something more like this. People invented a link layer protocol and people invented an and this protocol and there's like hundreds, thousands of protocols out there. And I'm kind of split them into four categories here. Somebody else might split them into more or fewer. And to some extent, even with this four choice, which is, you know, pretty common, you can find protocols that kind of cross the boundaries here. A protocol will be both an application layer protocol and a transport layer protocol. And we'll see examples of Boundary Crossing like that later in this lecture, assuming I get to them. Yes? How do I make a protocol? I think. And then I write down the set of sort of rules that an application should use when it goes on the network. For example, something that's very important as part of the protocol is to specify exactly what goes into the header. Because if the recipient doesn't know what the header contains, the recipient will get a packet, won't have a clue what to do with it. So protocol will specify header contents very carefully. But it's not simply the contents of headers. The protocol will specify behavior. It will say something like, if you get a packet that looks like this, then you should respond with a packet that looks like either X or Y or Z. Right? So it specifies not only the contents of the packets, but the interaction that any sort of node on the network should have with all the packets that it gets. Did that help? No. It's like a spec. So for example, in CS31, you learned how to program C++. Did you learn the whole language? There is a formal specification for the C++ programming language. It's constantly evolving. We're currently at C++ 23, I think, and there'll be a C++ 26. The specification for the C++ programming language is, I don't know, I'll guess, a thousand pages long. Right? So in some sense, you have to follow the rules of that very complicated spec in order to write your code and get it to work. If you write code that doesn't conform to that thousand page spec and give it to a compiler, might not work. Maybe it will if you're lucky, but probably won't. Network protocols are the same way. They're specs. They're not code. They're specification. They say if you send packets that look like this and if when you receive packets you do this, then your network application will work. If you don't follow the protocol, Who knows what's going to happen? So that's really what a protocol is. It's a spec. Okay. Other comments on these protocols? All right. So let's take a look at the Internet Protocol. In some sense, this is like the core protocol. This is like a big deal, right? And the Internet Protocol, which is called IP. It has come out in various different flavors. It's the foundation for everything above the internet layer. So you have to get the internet protocol to work or else you're not going to be anywhere. It's come out in several versions. The two most popular versions now are called IPv4, which is obviously version 4, came out in 1983. And it was specified by a team which included John Postel. And I'm mentioning this because he's a UCLA alum. We name our Computer Science Department lecture series after John Postel in honor of all the work he did in the third floor of and this is one of his sort of crowning achievements. So basically it talks about packets and because it talks only about packetlets it's what's called a connectionless. A connection is when you have a connection between you and a web server somewhere. There's no such thing as a connection. You either send a packet or you don't. But there's no connections.

Speaker 3
If

Speaker 1
you want a connection, you're going to have to implement it yourself. However, an important part of IPv4 is the header. Every packet following IPv4 has to have a header. And the header contains the following information. And it's a little tricky business here, right? Because the more stuff They put into the header, the bigger the header got, the more complicated the protocol got. They didn't want to do that. They wanted to make this header as simple as possible because everything else was going to be built atop that. If you need a fancy feature, implement it yourself. These are features that everybody needs. So if you look in an IPv4 header, you're looking at stuff that these designers from 40 years ago thought were the most important things to do on the Internet. So what were they? A length field. The length is how many bytes are in the packet. And you might think, wait a second, why do you need a length field? Can't you just like get that information from, I don't know, the link layer or something? The link layer will tell you how many bytes you got, right? Why do you need a length field? It's because they were worried about link layers that couldn't do that. They wanted to have support sort of, you know, a length field for, I don't know, radio or something like that, which is a continuous medium, and there isn't really any length field or that sort of thing. I'm just hand-waving here a bit, but basically, yeah, it's pretty important for every packet to know how many bytes it has. Second thing, a protocol number. And you may be thinking, wait a second, we're talking about IPv4. Why does it need a protocol number? It's IPv4. We know the protocol. It's because they were thinking, we want to specify the internet layer. We want people to write protocols atop the internet layer, so we want to have sort of that protocol number there. All right. We also have the source and destination. Oops. Sorry. Over here. Source and destination. IP address. An IP address in IPv4 is a 32-bit number. All right, so that's 32 bits each, 64 bits total for these two IP addresses. By convention, we take those 32-bit quantities, divide them into four 8-bit quantities, and write down each 8-bit quantity in decimal separated by dots. So you might see an IP address like this, 1.92, 54, 239, 1. What that means is the underlying bit pattern here is a 1. This underlying bit pattern is 239, and so forth and so on. So it's really, well, this is big 1, 1, 0, right? It's really just a 32-bit number. They could have written down IP addresses as big decimal numbers or maybe as hexadecimal numbers, but they decided to write it this way and to some extent that notation has taken over. Notice that each packet says where it came from, where it's going to, and the assumption, at least in the simple model here, is that every node on the internet has a unique IP address. A few of those addresses are reserved for other purposes, but pretty much, you know, most of those addresses, we just plant them on nodes. In 1983, the assumption was, we, you know, 2 to the 32nd, that's 4 billion IP addresses. That's plenty. That's like a computer for every other person on the planet. In 1983, that sounded like overkill. All right. Any questions on this information, yes? Can

Speaker 3
you re-explain what protocol number does?

Speaker 1
So you can have several, oh, I should mention these numbers are fairly short, right? I think this is a 16-bit quantity. Not sure about this one. You can look it up. It's either 8 or 16. It's a fairly small number as opposed to these 32 bits. The protocol number says I might want to implement different protocols in IPv4. I might want to have transport protocol A and transport protocol B and that sort of thing. The protocol number is going to help routers know which protocol this particular packet is using and the routers might use that information to give some protocols higher priority or some lower priority, that sort of thing, or to schedule sort of sending the packets in a different way. a hook at the low level of IPv4 in order to let you implement other stuff on top of it. All right. Let's see, what else do we have? Oh, we have a time to live for TTL for short. This is a small number. I believe this is an 8-bit number. And you can think of it as being sort of a death sentence on the packet. Every time the packet goes through a router, the time to live counter is decremented by one. And when it reaches zero, the packet is dropped. It's never delivered to where it's supposed to get. So when you, you know, throw a packet into the network, and give it a time to live of say, I don't know, 60, right? That means it will cross 60 routers on the way from source to destination, but if it tries to go one more router, it'll say, this packet has been around too long. I'm just going to ignore it. Remember, routers are busy. They're constantly getting packets all the time. One of the things they love to do is to drop packets. Less work for me. I know the feeling. You probably So why do we have time to live at this very low level of IPv4? Why is it so important to have a time to live field in every packet? This is a big deal, right? Any thoughts? Yes? Yeah, otherwise you might have a loop in your network. Each router in the loop says, I don't know, do I do it? what to do with this packet, I'm going to ship it off to this other guy. And the packet loops around and around forever in the network and chews up bandwidth and you're paying good money for nothing.

Speaker 3
So

Speaker 1
basically the time to live sort of field is there as a sort of device to prevent your network from becoming overloaded instantly. All right. Another field is a checksum. This is a 16-bit checksum. and it's computed with an algorithm. It's a pretty simple algorithm. It's not simple exclusive OR, but for now let's pretend it is. Just take all the bytes in the packet, exclusive OR, you know, every pair of bytes, you come up with an exclusive OR of the result, and then that's the checksum. Okay? The checksum is intended to catch packets that are corrupted, which I guess I didn't list here. Hmm. We'll put another dot here, corrupted. And the idea is that if a router gets a packet and the checksum doesn't match that of the data, the router should just basically, you know, either give up or do something. It shouldn't try to keep sending the data along. It's bogus data. Now, one might object and many network engineers initially did object that why do we need checksums? Because The link layer should handle that problem. These link layers like ethernet or wifi and all this thing have a lot of error detection and error correction. If you have a bad wifi signal, your connection to the router is constantly checking the data and making sure that the data that you send is the data that you actually wanted to send. Why do we need to check some at the IP level? And the answer is one of the design principles of the internet is to not trust the next lower layer all that much. Assume that the people who configured the link layer screwed up and they turned off checksums or they did something bad at the lowest level, right? So the idea here is we want end-to-end checking. This checksum should be good in Boston, even though I sent the packet from LA and it went through 10 routers in between. That checksum should still be good, and the recipient in Boston should be able to check that the data is what I sent. Now notice that this checksum is only 16 bits, which means that if there's some error in the packet, lots of bits got flipped at random. What's the probability that I will not detect the error? 2 to the 16th, right? Could have just had bad luck. The checksum will match even though the checksum, you know, the data were corrupted or maybe the data and the checksum were both corrupted. So this is not enough to do sort of like ironclad guarantees that the data, the packet is exactly what you sent. It's really designed mostly to catch stupid hardware mistakes, right? So this is for sort of hardware or configuration errors. It's not enough to catch things like malicious networks. Routers deployed by bad guys that try to corrupt your data into doing, and try to convince you into doing something that you shouldn't. Those guys can easily construct checksums to match whatever they want to do. It's only intended to catch inadvertent air. Any questions about IPv4? Yes. Connection is, you know, I pick up the phone. Oops. I just talked to somebody. And there's no, you know, this is actually circuit switching. This is using 1960s technology, right? I have a connection to somebody in Campbell Hall that is in charge of the in charge of disasters when people give talks, right? And a packet is something else. A packet says, I'm just sending data and I hope it gets there, but it might not. And every packet I send might go to a different spot and all that sort of thing. The connection means you have two people talking to each other for presumably a somewhat extended period of time. Question. Does

Speaker 2
IP protect against VDOS? No,

Speaker 1
no, no, no, no. I mean, you want to overload the network? Just send a bunch of packets, right? And there's no protection here against that, at least at the level we're talking about. The protections that are put up against denial of service attacks are something higher up than this. Okay, question?

Speaker 3
What's the difference between the IPv4

Speaker 1
and the IPv6? Oh, well, for our purposes, not all that much. IPv6 came out in 1936. 1998. It added a bunch of information to its header. The thing that people tend to focus on is the addresses got bigger. So it has 128-bit addresses. Under the theory that we will never build more than two to the 128th computers that want to talk to each other. And so far we haven't. we're in good shape. It has a few other fields as well that mostly are beyond the scope of this course and I'm not really going to focus that on sort of the extra fields that it has. The networking class will talk about that. If you take a look at the actual internet out there, roughly 60% of it of the packets that are sent over the network still use IPv4 and about 40% use IPv4. IPv6. Why is that? Why would you want to prefer this old-fashioned technology? Any thoughts? Why is IPv4 still popular? Yes? That's a big thing, right? The dead hand of inertia operates very well in computer science, right? And particularly at UCLA, we love IPv4 because, hey, We invented the internet. We have lots of IPv4 addresses. Why should we switch to IPv6? Alright. Any other reasons? Yes. Oh, yeah. Obviously the protocols are quite different. However, there are standard ways to gateway from one to the other so routers know how to take an IPv4 packet and sort of convert it to IPv6 and then back again, all that sort of thing. That's a hassle. A lot of people don't like to do it, but there it is. Okay? So the reason people mostly want IPv6, in my experience, is we ran out of IPv4 addresses. They're stuck. They got to use IPv6, right? When in 1983 mainland China was considered to be an economic backwater, so it didn't get very many IPv4 addresses. They quickly have grown to be so big that they can't possibly use IPv4. So this is very popular in China simply because that's what they have, right? This is popular partly due to inertia and partly because it's more efficient. Why? Because the addresses are smaller. The packets are smaller. You save an epsilon of efficiency here. You save, you know, an epsilon worth of electricity. Why not, right? The extra features of IPv6 mostly go unused. The main one that does All right. Question? How does an API call happen here? Well, there are APIs for sending and receiving these low-level packets, but most people don't use them. They operate at a higher level of abstraction. We'll get to that. All right. So, well, let's start talking about higher levels of abstraction. Almost nobody writes code that deals with IPv4 or IPv6 directly. Instead, they operate at a higher level of abstraction at some sort of transport layer or at least, you know, halfway between the Internet layer and the transport layer. So I want to talk about some of the protocols that are built atop IP. One of the simplest ones is called the User Datagram Protocol. or UDP for short. It was designed by David Reed of MIT back in the 1980s. It's a very thin layer over IP. Just introduces one or two more concepts. concepts and basically it's designed for applications that want to deal with these problems. You know, if the packet gets lost, I'll deal with it. If the packet gets received out of order, I'll deal with it. All that sort of thing, right? And you know, to some extent that's the simplest protocol and widespread use amongst applications. It's also perhaps one of the most painful to use and the applications that do this, you generally have a very good reason for one to use UDP as opposed to something else. A more commonly used protocol is the transmission control protocol, or TCP for short. And it was designed by Vidsurf, who's a UCLA alum. and Bob Kahn who unfortunately is not from Princeton. And the basic idea of TCP is to be sort of a standard transport layer protocol that you can use if you're building a higher level application and you don't want to worry about these three problems that are hiding behind the screen here. is that we -- this sort of is a protocol that establishes a stream of data that provides the following sort of features. First off, it's reliable. If you have a TCP connection between point A and point B, you can rely on the data getting through without all this garbage about packets being lost or or that sort of thing. Question? No, they'll be nodes on the internet, right? So I'll use TCP to talk to MIT from here and I'll ship it, you know, 47 gigabytes of data and it's reliable. It's going to get there. They could be adjacent, but they don't have to be. Now, of course, by reliable, do I mean it will always work 100% guaranteed? No. It could be that someone cuts the wire between my workstation and the wall. In that case, obviously, it's not going to get through. But as long as the network's up and running reasonably well, it will get through and you won't have to worry about those three problems. The second thing is that it's ordered. The recipient will get the data in the same order that I sent it. So they won't have to worry about all the packets being being received out of order, the message being garbled, or that sort of thing. The third thing that it guarantees is error checked. And again, one might think, wait a second. We have error checking at the link layer. We have error checking at the internet layer, because there's this checksum here. The TCP guys follow the same end-to-end convention as everybody else. They say, I don't I don't trust IP's error checking. I don't trust the link layers error checking. I'll do it myself just in case those lower levels screw up, right? So in order to implement all this, the TCP protocol has to specify some things to do in a standard way for every node on the internet that supports TCP to understand how to implement reliable and ordered and error check stuff and unreliable and unordered network. And what those things are includes the following: Division of a stream of data, which can be arbitrarily long, into packets. You'll take your 47 gigabytes, split it up into one kilobyte packets, and do that in a standard way such that the recipient knows that that's what you're doing. Second thing, if we are shipping our 47 gig of data, a very common problem would be the following. Oh, we'll split this up into, you know, 4.7 billion packets and then we'll ship all the packets out the door right away. What's going to happen? We're going to overload our router, right? Or if not the router next to us, the next router down the street. So if we just try to ship data as fast as we can, what we'll probably do is lose a lot of data and, you know, we're wasting everybody's time. We're sort of flooding the internet with packets that aren't going to get there. So very important part of TCP is flow control. The idea being that you should be able to like stick your toe into the water and say, the internet right now. And if it's not very busy, you start shipping lots of packets. But if it's kind of busy, you back off. You slow down. Because you don't want to swamp the internet. If you swamp it, you can't get through and nobody else can get through either. So in some sense, this flow control part of TCP is part of, and it's a core part of the internet. It's sort of how can we cooperate all together. As long as we all follow these rules, we won't swamp the internet. And the third thing we have to do is retransmission. No matter how good we do flow control, some of our packets won't get through. We're going to have to find out which of our packets did not reach the destination and we'll have to retransmit them because we want reliability. So retransmission attacks the reliability issue. In the worst case, we might have to retransmit two times or three times or more, but we'll keep doing it until it gets through. The other thing we'll need is reassembling. We're sending a stream. We break the stream up into packets. The recipient gets packets, but that's not what the recipient wants. The recipient wants the stream we originally sent. And what can be involved there is that the recipients can get the packets out of order. Not the same order that we sent. It needs to reassemble those packets that are out of order in its memory to a higher level inside the applications buffer where they appear to be in order. That process is called reassembly. And reassembly handles the ordered problem. The recipient's responsibility is at a low level inside maybe some C++ library or somewhere to reorder the packets that it got into the order that the application actually wants. Any questions on these fundamental problems? Yes? It doesn't. It's up to the application to not screw up. screws up and starts shipping UDP packets like crazy. Eventually, you know, Charlie Fritzius, that's my network operations manager, will come down the hall and knock on the door and say, Professor Eggert, we're going to have to disconnect you from the Internet, right? Which he will do if the faculty member screws up, right? Same thing will happen to you if you have a laptop that starts using UDP to send packets like crazy. Eventually, the router is going to say, I'm going to blacklist you and all of a sudden your network connection will go dead, right? That's the whole point of protocols. Follow the protocol, things will work. Don't follow the protocol, it's not going to work. Other comments, yes? So an application will use UDP if it only wants to send very short messages, right? They fit into a packet. Also, it doesn't really It merely has short little blips. Right? So a place where UDP might make sense is if the device is, I don't know, a thermometer sitting in a building somewhere and it needs to ship the temperature off to some central server. So once every 10 minutes it sends a UDP packet. Here's the temperature. Done. If that packet doesn't get to the central server, no big deal. Next packet will get through. It's just the temperature, right? So we're 10 minutes out of date. Who cares, right? So that would be a natural application for UDP. If on the other hand, you're submitting your answer to homework three, you don't want UDP. You want something reliable that gets that solution to the server reliably. Yes? Louder please. Yes. And it's worth it. Right? I mean, let's put it this way. If you look inside your laptop, what proportion of your flash drive is devoted to error checking? It's non-zero. Right? And it's more than 1%, I can tell you. There's a definite overhead. But we're willing to pay that overhead because, let's face it, flash drives are really unreliable at the hardware level. Right? Same thing that goes on here. is kind of flaky. I won't say the more error checking the better. Obviously there's some limit to it. But you know each of these layers has error checking and there's a good reason for that. Yes? The simplest way to do retransmission is exactly that way. That is the sender you know start shipping off packets but it keeps the packets that it sent in RAM. and it waits until it knows from the recipient that the recipient got the packets then it discards them. That's the simplest way. There are more complicated ways in which the sender can regenerate the packet from other information if it knows how to regenerate it. But most people I think do it the simpler way. Yes?

Speaker 4
So when you send information, sometimes something gets lost, right?

Speaker 1
Yes. Oh yeah! When you retransmit, the recipient will notice that the data are arriving more slowly than they would otherwise come in. And they'll just chalk that up to the network, you know, being slow, which in some sense it is. That's a real problem, right? That is, suppose you're missing a packet, right? And you tell the sender, can you please resend it? In the meantime, you've got a hundred packets from the sender. You just got to keep them somewhere in a buffer somewhere. and not use them until that missing packet like the little lost sheep eventually shows up and then you can finally deliver all them to the application and that's a problem with TCP absolutely. Question? I'm sorry I'm just not here. There has to be some way that the sender knows that the packet didn't get through so that it can retransmit otherwise it would have to be constantly retransmitting it wouldn't know what got through or not but we also don't want it to be so inefficient that the recipient every time it gets a packet it sends an acknowledgement back to the sender that's that's ridiculously inefficient so TCP is much more optimal than that you also have to take into account the fact that when the recipient tells the sender oh I've got this that packet could be lost right So it has to work in both directions. And the TCP guys have thought through this problem and have some fairly fancy algorithms to get it all to work. Yes? For the TCP you

Speaker 1
mentioned that you're not using data channels per se, which are going from point A to B. So are these two protocols part of the internet layer or are they still part of the transport layer? TCP is going to be part of the transport layer. I think officially UDP is transport, but to some extent it's very close to the internet layer. It's the lowest level transport layer protocol that I have. I know of offhand, that sort of thing. Yes?

Speaker 3
Yeah, but the

Speaker 1
TCP

Speaker 3
doesn't necessarily

Speaker 1
use data channels, so if you mention... Well, TCP really is talking about a stream of data, and for you to support a stream of data, you really need the idea of a data channel. You're saying sending from point A to point B, so I really think those notions are closely linked. All right, why don't we take a break, and we can start up again at, say, 8 past. The layer that

Speaker 2
deals with all of

Speaker 3
the problems that you mentioned about packets is the transport layer?

Speaker 1
Yes, those three problems.

Speaker 3
Oh, so when you said that the internet

Speaker 2
layer doesn't ignore the three

Speaker 3
problems, it just means that it doesn't address those three

Speaker 1
problems? It doesn't address the problems. The problems occur, yes. Okay.

Speaker 3
Okay, that makes sense.

Speaker 1
All right, let's start up again. So I want to talk about the web next. And in fact I have on the screen here, if I play my cards right, we have a picture of the very first web server that ever existed. I've actually seen and touched this web server. It's at CERN, which is a nuclear research facility between France and Switzerland and that sort of thing. And that little sign that says do not power down is because Tim Berners-Lee's workstation was hosting the first web server and he didn't want the janitor to turn it off, figuring that nobody wanted to use this workstation. And basically the idea of the web has two parts. The first is a protocol, HTTP, which is short for the hypertext transport protocol. These days most often used in its secure variant which has an S after it, but initially it wasn't secure at all because, hey, every physicist is a friend to each other. The other thing that's in the web is something called HTML, which you can probably guess what the H and T stand for, hypertext, and ML is short for meta-language. So what's going on here is that when Berners-Lee started out, the way people read physics papers like or that thing on the left or that thing on that right is they copied files from point A to point B on the Internet using a protocol called FTP, short for the file transport protocol, which was designed by somebody at MIT in the 1980s and was really complicated and hard to use. and nobody really liked it. And so one of Berners-Lee's ideas was to come up with a simpler protocol that would let you read physics papers more easily, right? And that's the hypertext transport protocol that this machine is supporting. His other idea was to come up with a standard format for presenting physics papers that sort of everybody could see and easily use as opposed to what was common practice at the time. There was lots of different formats. PDF was sort of there but it was kind of flaky and all that sort of thing. So the idea here is to standardize the data that gets shipped across the net and make it really simple and easy to implement and to standardize the protocol you use to ship the papers across the net and to make that simple and easy to use as well. And to demonstrate what that looked like at the time, Here's a picture of the first web browser developed by Berners-Lee on the Next Workstation. That's what Steve Jobs was developing before he went and took Apple and turned it into a big company. Notice everything's in monochrome because he couldn't afford color. Okay? But the thing that sort of got everybody interested here is that text and pictures were sort of presented at the same time in the sort of thing you would like to see in a physics paper as as opposed to just seeing text which was what most people saw on screens at the time. Now looking at it with today's eyes this doesn't seem to be that big a deal and actually when I looked at this the first time I didn't think it was that big a deal either. I thought all he's doing is he's putting you know physics papers on wheels. What's you know what's big about that? But there is something important here which is that he was Gluing together sort of very simple networking based on a transport layer protocol. We'll see more about it later. With very simple formatting, right? Which is HTML is much simpler than PDF or that sort of thing. And he got it running in like an afternoon. All right. It took him all summer to implement it. And it looked nice. And people could use it easily, right? So his basic contribution was taking sort of ideas simple to networking people. to people, ideas simple to publishers, gluing them together in a way people hadn't done before and getting it all to work without too much overhead. All right, so let's look at these two components in a little bit more detail and we'll start at looking at HTTP, right? It's a protocol that's built atop TCP, right? But the idea is that it's TCP plus some sort of for exchanging documents. And let's see if we can run it. So I'm going to start up Emacs and I'm going to type control u meta x telnet, which means Create a telephonic, tele just means far, a far away network connection. And it'll say which host do I want to talk to? And I'm going to pick leapsecond.com since I'm a time nerd. And it's going to ask for a port. So in TCP and in UDP, every sort of packet or every connection has to talk to a particular port. Port numbers start at zero and go up to, I believe, 32,000 or is it 64,000? Right? So each packet will tell us what the port. I'm going to pick the port 80 because I know that's where leapsecond.com has a server. And now I have a TCP connection to this web server. Since Telnet doesn't know anything about the web, all it knows is you can you have a stream of data that you can send there and you have a stream of data that comes back. I will issue commands by hand here that conform to the HTTP protocol. So the protocol says if you issue this command, the get command says what resource do we want to get? We want to get the root resource, that's slash. Which protocol version do you want to use? I'm asking for like the This is like 1980s technology or something like that. And I hit enter. But this is just the header for my request and by definition in HTTP the header is not done until we see an empty line. So I'll do the empty line here. And what I saw as a response on my terminal here is the response from leap second dot So everything starting from the cursor on down is the response. That's a stream of data as well. And at the HTTP level, this stream of data has a header and it has a contents. This header is at our higher level of abstraction. We're not talking about IP headers anymore. We're talking about from the IP point of view data in the packets. But at our level of abstraction, this stream of data starts with a header that goes to the first MP3 and then everything after that first entry line is the data that is coming back from our web server. If we look at the header, we can see that its first line is a response that says, "Oh, here's the protocol version I wanted to use. I wanted to use 1.1. You said 1.0. I hope you don't mind." It also specifies a three-digit number that tells us whether or not that response indicates requests by convention 200 means it worked. It also contains a comment, "OK." That's for us human readers, the computers can ignore it. Underneath that part of the header are a bunch of header lines, each of the form name colon value. So you can think of them as it being an attribute name and an attribute value telling us meta information about this particular web page. The meta information that we have are first the date, that this particular web page has. It has a date of tomorrow because it's running in GMT, right? You would expect leapsecond.com to be in GMT. That's not unusual. The name of the web server, in case I'm curious, is Apache. The last modified date for this web page, this web page hasn't changed since 2020. It's a very stable site. An e-tag, which is a unique identifier for this particular version of the web page that we can look to see later on has that web page changed or not. In some sense it's a nicer version of the last modified date. Accept ranges bytes that is a message from the web server back to us that says hey if you just wanted a few bytes from this web page you could tell me that and just say just give me bytes 1000 to 1500 and I'll do that for you. but you didn't so I'll just give you the whole thing. Content length which is the number of bytes in the body of the message. It's another length field, a length field appropriate for this level of the protocol. Connection close says I'm an old-fashioned web server and you've given me an old-fashioned request. I'm going to immediately close the connection after I send you this response. So we created a TCP stream, we sent a single request, it sent a single response, and then it closed the connection right away. And then the last line of the header is content type, which is the header that a web server uses to tell you what kind of data it's sending you in response. This particular kind of data is HTML because that's the sort of thing I wanted to lecture about. But it's possible for the content type to be anything else you like. You can make it, I don't know, JPEG. image or whatever you like and then the data that comes after the first empty line is going to be a bunch of JPEG data. All right so that's the header. Everything after that first empty line the thing that starts doctype blah blah blah blah blah is HTML which I'll talk about later. All right any questions on this interaction? All right this is basically how the web looked like back when Tim Berners-Lee invented it. He got a knighthead for it and all that So e-tag is a tag for this version of the webpage. You can think of it as being kind of like a checksum, but it doesn't have to be a checksum, it's just a string. It's possible for me to query the web server later and say, "Can you tell me the e-tag for this webpage?" Don't give me the contents, just tell me the e-tag, right? And then if that e-tag hasn't changed, I don't have to ask for the contents later. Or I can do a conditional request. I can say, I'd like a copy of this webpage, but if it's etag matches that string, don't send it back to me. Just tell me it hasn't changed since last time. So it's an efficiency thing. Other questions about this interaction. We've just, of course, scratched the surface. There's lots more headers where this came from. HTTP is grown to be a fairly complicated protocol, but this shows you in some sense that it started off really simple. Single request, single response, done, right? Use TCP so we don't have to worry about packets out of order. It's like one of the simplest protocols ever and also one of the most widely acceptable ones. Nowadays, this protocol is to some extent looked down upon. The reason it's looked down upon is all the data that you see on the screen went across the internet to some server in like England or something. All the routers between here and England saw this data. There are no secrets. People tend to like privacy. Worse, suppose one of the routers in that connection was malicious and saw the packets coming back from England back to me and say, oh, I'm I'm going to pretend that this line over here that says color FF000 is color 0000. That is the routers can tamper with this data and have the data on your screen be not at all the data that the web server wanted to appear. This is particularly a problem in places where the government doesn't like what people are doing and tries to interfere with what they can see on their screen. So these days this has been superseded by a protocol called HTTPS which is like HTTP except the streams of data going from point A to point B and back again are encrypted. If we want to do an HTTPS connection we have to use something that's a little fancier than Telnet. So let's see we can go we can start up a shell Let's say. And we can say run the GNU TLS CLI command. That's short for the GNU Transport Layer Security Command Line Interface. Right? So this is, most people don't run this command by hand but you know if you're running a shell script it might be something that you might want to know about. And then we can connect to some random spot on the internet. like developer.mozilla.org, which is a good place to look at if you're trying to find documentation about internet protocols or browser. The Mozilla guys are pretty good about that sort of thing. So when we do that, we see a whole bunch of stuff that's on the screen because it's telling us all the handshake information in which we're exchanging public keys and credentials and all that sort of thing so we can make sure we're actually talking to mozilla.org. and not to some fake website pretending to be mozilla.org. And then when I type in a similar sort of command here, get /http and at this point I think they don't like 1.0, we'll have to go to 1.1. This set of characters that I typed is encrypted and send over the internet in encrypted form. the intervening routers can't see exactly what I'm typing. They know I'm sending packets to the location but they don't know what's in the packets and they can't tamper with the packets. HTTP 1.1 requires at least one parameter, the host parameter. So I'll type the host and you might wonder why I have to do that. It's because most web servers these days can serve multiple domains. The same physical web server can be the web server for developer.mozilla.org and for www.mozilla.org and a whole bunch of other websites. When you talk to the web server, you have to tell it which website are you right now, right? Because it might be any of several and it wants to know which one you're interested in. So we did that. Then we'll type enter. That ends our request. And it will say, oh, not found. Oh, see it said 302. 302. Oh, it said found, but it said, oh, 302 says the webpage that you're looking for is not here, it's somewhere else. And the place where it's somewhere else is given by the location part of the response of the header. So we have to now say something that looks like this. Get enus, right? HTTP 1.1 and then say the same host line as before and then an empty line. And then we get the web page that we're interested in which as you can see is, you know, pretty complicated. Right? Notice that this is a fancier website. It has headers that contain lots more information. Look at all that fun information. and then the webpage which starts after this is also kind of long and has long lines and so part of what's going on here is they're trying to make the webpage as compact as possible and thus avoiding new lines and all that sort of thing. Alright, so you know this is an example again of a simple request response. One of the main innovations of HTTP 1.1 is that it was no longer a simple request response hangup protocol, but instead you could send several requests, right? Send request one, receive response one, send request two, and so forth and so on. Receive response two. and do this over and over again. And you didn't have to create a new TCP connection for every webpage. You could grab several different webpages just off of a single TCP connection, thus improving network efficiency because there's some overhead involved in setting up a TCP connection. So to some extent, what happened here is that Berners-Lee, who knew networking but was in kind of a hurry They designed a very simple but inefficient protocol. And one of the first things they started doing is saying, "Let's make things more efficient." So you can get several web pages off of a single data channel. This process continued. So, HTTP 1 and 1.1 are sort of 1990s technology. in 2015, basically the goal of this version of HTTP was for transport efficiency. That is the idea is that the HTTP 1 connections were not using the internet efficiently enough and we wanted the transport layer to become better from a user's point of view. At this point, I should say, by the way, that HTTP 1 is used by about 10% of requests on the Internet. And there are companies that go out there and measure this stuff because they want to make sure that, you know, routers are being designed for the actual network traffic and that sort of thing. HTTP2 is used for, according to current measures, 59% of requests on the web are using this protocol. So some of the features that are used in HTTP2 include the following. First off, header compression. has been used in HTTP ever since the beginning. One way you can use the network more efficiently is you can compress the web page, ship the compressed version of the web page, and then let the recipient decompress it. That will place less strain on the internet. And already in HTTP 1, we have that possibility because it doesn't care what the data are. All it cares about is the headers. So if the header says, So I'm an HTML compressed file, well then the web server can compress the data, the recipient can decompress the data and the network is going to be less stressed. The problem with HTTP/1 is that you can compress with gzip or whatever else you like, you can do that with the data, you can't do it with the header, that long text header, the first half of this screen in HTTP/1 cannot be compressed. It's designed to be a text sort of header and there it is. So the idea in HTTP/2 is support header compression in the protocol. The client in the server can agree on which compression method that they like. It might be gzip, it might be xz, it might be anything else. It's part of the protocol and after that the headers will be much smaller in the internet than they are in their original text formats. And that improves transport efficiency. Another idea In HTTP2 is the idea of server push. In HTTP1, it's very much a request response protocol. The server doesn't tell you anything unless you ask it a question. You always have to start, "Please tell me about this," and then the server will give you an answer. Some applications don't like that. You might be, I don't know, and you want to tell the server, let me know when anybody scores. Right? In a basketball game, tell me every time somebody scores. In HTTP 1, the way you have to do that is you have to constantly be talking to the server, has anybody scored yet? You have to keep polling over and over again, say once every three seconds or something. That's inefficient. In HTTP 2, there's a way you can tell the server, okay, just let me know when somebody scores, and then and the server can give you several responses to a single request. That's called server push because the server is pushing out the data to you rather than waiting for you to ask it. Third idea in HTTP/2 is the idea of pipelining. Here the idea is in some sense pretty straightforward. Here's the client, here's the server. In HTTP 1, it has to work like this. The client, here's time is going this way. The client sends a request, the server sends a response. The client sends another request, the server sends another response, and so forth and so on. Pipelining says it doesn't have to be this way. The client can send a second request before it's got that response back. So here's HTTP 1. Here's pipeline. The client and the server still talk to each other, but the client can send several requests without waiting for the response, and then the responses will come back like this. And notice how much more efficient use of time we have here. This can really cut down on latency. We have four requests and responses here. Here, if we wanted four requests and responses, we'd have to wait all the way here until we got all of our answers back. Next idea in HTTP/2 is multiplexing. One way to think about multiplexing is you can maintain several different conversations on the same HTTP connection simultaneously. Another way of thinking about it is that we can do even better than this in some cases. Here's a simple application of multiplexing as time goes on. The client sends several different requests like this, just as with pipelining. But the server notices that some of these requests are easier to answer than others. So the client sends requests A, B, C, and D. The server says, oh, A is kind of hard to answer. Let me think about that. but I can answer B right away. So here's B's response. And then C is pretty hard, D is easier. So the answers come back and finally we get the answer for A down here in a different order than they were sent. Multiplexing complicates the protocol and makes things harder on the clients and the server. It basically says every request now has to have a tag attached to it When you see a response, you know what it's responding to, all that sort of thing. But the advantage of this sort of thing is that you can now get sort of faster answers to the questions that you had were easy. In advance, the client might not know which questions are easy and which are hard, but it can get answers faster this way than if we insist on the pipeline order. Question? People are impatient. At least I'm impatient. Like for example, BruinLearn is really slow. Have you noticed this? I mean, give me a break. I log into BruinLearn and it takes like five seconds to fill in the screen. I can't wait that long, right? If I have a properly pipelined approach, a properly multiplexed approach, say, It could be that I'm logging into brew and learn only because I want to set up a zoom session or something, right? So I want to press on that zoom button part of my user interface before the rest of the stuff has shown up. And then I want to get answers to that part before the other stuff comes, right? And with multiplexing I can do that. With pipeline, no, I got to wait for all my previous requests to come in. And if I don't even have pipelining, it's going to take 50 seconds instead of five seconds, right? What's going on here is we have people who that are impatient with their browsers. Question? How

Speaker 3
does it gauge? What's an easy question versus a hard

Speaker 1
one? Oh, I'm assuming here that you've got some, you know, software running in the server and it's like doing its best. It's just running. But some of this stuff requires actually talking to a database server somewhere else and it's going to take a while for those answers to come back, right? I'm assuming, you know, this might all be written in Node and each one of these requests, the Node server is trying to answer as fast as it can, but it's and some events come back slowly. So the server in fact may not know in advance which requests will take a long time to answer. It doesn't know until it actually tries to answer them. Other comments about this kind of performance improvement? All right. Let's do the next one. HTTP3. which currently I've seen an estimate of having about 32% of web traffic. It's relatively new. It came out in 2022. And basically the big change in HTTP/3 came because people wanted to do Zoom. And of course you could run Zoom before HTTP/3 came out, but the way people did Zoom back in the dark ages is they had a separate Zoom app that they had to download onto their laptop or their cell phone and all that sort of thing. And that separate Zoom app used UDP or something like UDP, it actually used RTP, which is a different protocol. The reason the Zoom app who couldn't use TCP is because of the problem that was alluded to earlier in the lecture, right? The problem here is that TCP is bad for audio and video streams, right? If it's live anyway. Why is it bad? Because suppose I'm implementing a video stream over TCP and a packet gets lost. What's going to happen is what does the recipient see? The recipient application is basically forced to wait for that packet to be retransmitted. It won't see the remaining packets that came after the lost packet until that packet gets retransmitted. What that means is the recipient will see me walking across the lecture podium like this and all of a sudden, and then it'll all start up again when the missing packet finally arrives. Okay? And that's a disaster if you're doing live data. I mean, it's okay if it's a web page. The web page sort of hangs at one point and you say, oh, the network's flaky, but you want to get that data reliably so you wait. If it's a video stream and you lose a frame, it's okay. You'll get a glitch in the data, right? You'll see me walking across here and all of a sudden I'm here and I keep going. That's much better than me being frozen for a second, right? So there's a large need nowadays for doing solving problems that TCP is unsuited for. And that's pretty much what HTTP 3 is all about. And in some sense, the biggest This change in HTTP 3 is until then all these guys were developed atop TCP because we wanted to assume we had reliable data connections on all that sort of thing. And HTTP 3 says, "No, we're not going to be built atop TCP at least not necessarily." All right? So what we see in HTTP3 is we use UDP. It's sort of a top UDP, not TCP. But that's actually not right. It's really a top another protocol called QUIC, which is a great marketing term invented by Jim Roskind, from UCLA, it's from Google in 2012. And basically Google was worried that people would start developing all these applications like Zoom and all that sort of thing and that people wouldn't be using browsers anymore and then Google would stop making money, right? So they said let's fix the protocol being used by browsers so that we can run video applications inside the browser and have it work well, right? So think of QUIC as being sort of like TCP version 2. So you can still build streams with QUIC. But you can also do sort of streams with some losses. That is, if the application says, "I'm willing to lose some of the data in the stream," QUIC will say, "Oh, how much data do you want to be willing to lose, right? And as long as you specify an acceptable level of reliability, you'll get a video stream that mostly works. Maybe sometimes it'll be, you know, you'll lose a frame or two. More likely, you'll see some pixelation, but then you'll be off to the races and it sort of looks good enough. All right? And the basic idea here, I guess I should use here, is to avoid head of line blocking delays. technical term for what I was just talking about. Head of the line blocking a delay occurs when there's a packet at the head of the line of the incoming packets. You're using TCP. That packet got lost. You've got to wait for that packet to show up so that it can finally unblock the remaining packets that come after that. All right. Any questions about all of these protocol levels? Yes. So my

Speaker 1
question was in Zoom, we can freeze in time, right? We can do what? We can freeze

Speaker 1
in Zoom, like the camera freezes, like you have one thing in Zoom. So is that no longer because of like... Well, yeah. I mean, Zoom now has, I mean, there are still Zoom apps, right? But Zoom now runs atop in a browser. When it does that, it's using QUIC. If you're still running the old application, it can be running the old RTP-based protocol. I assume it's RTP. You know, it's proprietary. I'm not sure. I don't know how the handshaking works if you're talking between the old and the new clients. I know Zoom is getting, you know, really hard trying to get people to use the new stuff. I don't know. Did I answer your question? Okay, yes. When your internet

Speaker 1
is slow, is it because you have your protocol? So when your internet is slow, is it the protocol? Well that's an interesting, so this morning my machine was really slow. I was working from home. I wasn't using Zoom. I was just doing random stuff on the internet, running SSH and that sort of thing. Why was it slow? It was really slow. It was really annoying. I tried to log into UCLA and it took like five seconds. It was because of DNS, which is a protocol I haven't even mentioned here. It's a very simple protocol. You give DNS a domain name, it tells you the corresponding IP address. If that protocol is slow, everything gets slow because the internet is hooked together via domain names but at the lowest level, at the IP level, you've got to know the IP address of what you're talking to. So in some sense, yes, a lot of slowness that you observe on the internet is due to some sort of protocol not working as well as it should. Why was DNS slow? Some hardware So it's a hardware-software combination. Yes? Well, don't quote me on that. I'm guessing. As far as I know, it was proprietary, but I assume it was a real-time protocol of some sort. RTP is built atop UDP, yes. There is a standard RTP protocol, but I think they were using their own variant. Yes?

Speaker 3
RTP has

Speaker 1
the same property as Quick. That is, if you're You can sort of, you know, get some packets through and not others, but then keep going. Absolutely. Other comments about these protocols? Yes. Oh, yes. All of these guys going all the way back to HTTP 1 have a secure variant. Nowadays, I would say most traffic is probably encrypted because there are more more bad guys on the internet so I don't really trust plain HTTP anymore other comments on the protocols yes yeah and then you'll get in you know it's like an ice flow blocking the stream right and the stream piles up and eventually it gets through and then you get a whole rush of data but you wanted to in a real-time application you want to get a nice even sort of set of and you won't necessarily get that. Yes? If it gets a packet

Speaker 1
out of order for video streams, is it just discarded? It depends. Most video streams, I think, will discard it. In some sense, maybe it can use that information for the next packet if it doesn't arrive, but I don't know the details. One more question on protocols. Yes? So, like, the benefit of multiplexing is that you get some of the data, like the back and forth, that take longer, right? But doesn't TCP, No, it only makes you wait until the missing packet has arrived, and then it will give you all the packets that it's gotten since then. The sender is still sending stuff. You don't have to wait for the entire transmission in order to start acting on what you have already. Then you have to wait until you get that packet before it can deliver that packet and all the packets that were sent after that. Yes. All right, we have a little bit of time. much, but I wanted to talk about this thing, which is the other part of Berners-Lee invention and why he got a knighthood and I didn't, all right? So the basic idea here is like HTTP stole from TCP, it's basically a TCP plus a little bit of other stuff, HTML is where Berners-Lee brilliantly stole from something else called SGML plus a little bit of stuff for hypertext. Hypertext is sort of web stuff. SGML is a markup language. Did I say meta language? Go back and change meta to markup. SGML is short for standard generalized markup language and it was invented by IBM as a way for authors and publishers to agree on how to format their papers so that they would look nice when they were printing. Before SGML was invented, there were all sorts of different markup languages. One system would say, here's how you begin a paragraph, right? Another system would say, no, no, no, here's how you begin a paragraph. Another system would say, no, no, here's how you begin a paragraph third party will come up with a standard way of doing paragraphs the IBM way and will support ways of translating between all these other formats and here's how you do a paragraph. There's the start of the paragraph, oh sorry SGML it's capital P, over here is the end of the paragraph, no we'll get there, like this, right? And you can think of this as being like a big open paren and this being a big closed paren and they set up You know, basically paragraphs and quotes and sections and italics and all that sort of thing in a standard way using this basic idea. What Berners-Lee did was take SGML and do two things. First off, he hated the uppercase. He thought it was ugly. So he said, "You can write it in lowercase." Second and more important, he added new elements that were designed for the web for interactive applications where you were pointing at or had a mouse or that sort of thing, things you could click on. These guys were doing books. They didn't care about that. And so he added just a few sort of extra features for the web and it took off from there. And next time we'll talk a little in considerably more detail about why HTML worked.

