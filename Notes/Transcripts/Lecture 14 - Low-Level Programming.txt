So today, I want to talk a little bit about low level software development. And to some extent, I suppose you could say that what we're doing today is a little bit of repetition or collision or some sort of thing with CS33, right? In the sense that in CS33 you see what machine instructions look like. We don't actually make you write machine code. That's like dark ages stuff. But we have you look at it and understand how it works and that sort of thing. Today's lecture will be a little bit higher level than that in the sense that we're not going to really be looking at the machine code very often. at the back of our minds. We're writing programs that are running at a reasonably low level. These programs are written in C++ or C or some sort of low level language like that. We, despite the fact that we're writing at this low level, we want our code to work correctly and we want it to work efficiently. That's part of the reason why we're doing it at such a low level. So to some extent, what we're looking at today is sort of the software consequences of this low level attitude. Now, most software these days is not written in C++ or C. It's written in other stuff. It's written in Python or JavaScript or that sort of thing. C and C++ still have, I don't know, 10 or 20% of the market and that sort of thing. So in some sense, we're lower level than average here. Still, there's an advantage to focusing at this low level. Not simply because we're in the School of Engineering, we're all engineers, we should know everything down to the machine level, all that sort of thing. But because the tools in this area have had the longest time to be developed and in some sense they're the fanciest sort of either debugging or performance improvement tools out there. Because people have been working on this stuff for like decades. And they have stuff available here that isn't yet available for the likes of JavaScript and Python and that sort of thing. I guess, and this is just a guess, that we'll see some of the ideas that you see in today's lecture eventually propagate into more modern languages. But, you know, it'll take time. All right. So with that in mind, kind of the idea of today's lecture, that's one thing, right? Another kind of idea is it's tempting when you have problems at the low level to say, "Oh my goodness, my program isn't working. Let's jump into the debugger and figure out what's going wrong." And what I would like to urge today is to resist that temptation as much as possible. So the idea is here, let's not use a debugger. Or another way of putting it might be, let's put off using a debugger for as long as possible. And this attitude is helpful at the low level. It's perhaps not quite as helpful if you're writing Python or code or that sort of thing because these, you know, the tools there are not as advanced. But you should also sort of have this attitude when you're developing your own code in at whatever level. In some sense, if you debug code with a debugger, you are already admitting failure. Your code isn't working and the best you can do, the best idea you have is to pull out a debugger. Oh man, there's lots more efficient ways to debug, at least oftentimes there are. Not always, but oftentimes. you should strive to avoid using a debugger when there are better alternatives available as there often are. So to some extent you could think of this as being procrastinating. I want to talk about debugging and debugging techniques and all that sort of thing and I'm putting it off and I'm putting it off because I hate to debug just like you do. But in some sense this is a better way, a more efficient way to make sure your code has adequate efficiency and reliability. Don't use a debugger. Use other stuff. They'll be more efficient. Debugging is so sort of human intensive and all that sort of thing. So instead of using a debugger, what can you use? A good chunk of today I'm going to argue you should be using compilers instead of debuggers to debug your code. There are lots of compilers out there, but the two major free compilers that have a good chunk of the market are GCC and Clang. To a first approximation, they're kind of the same compiler. They have a lot of the same options. They accept the same programming language and all that sort of thing. today's lecture on GCC because I use it more than I use Clang. I'm more of an expert in it. Right? But the stuff that we're talking about today is almost always usable in Clang as well. You may have to spell the options a different way or that sort of thing, but it's basically the same idea. So what sort of things can you do? What is GCC good for? Obviously it's good for getting your program to run. That's why you're using a compiler. You want it to generate machine code and then go run the code. But I'm going to be talking about things that at least superficially are other things, right? So, right, how should I say it? Besides getting your program to run, right? Often time, neophyte software developers, or sometimes their bosses, think that the job of software development is writing code and sort of getting it to run and that's all it is, sort of meet the specs. But what you'll find as you're doing practical software development is often times a good chunk of the specs don't get written down Because the customer, or you, or both, sort of assume some properties of the program because it should go without saying. So, for example, one thing that you should assume that your program is going to do is it should be, you know, reasonably secure. Your program shouldn't give away secrets that it's not supposed to. And unfortunately, if you're writing low-level code, and sometimes even if you're writing high level code, you can write programs that are insecure even though when you read the source code they look okay. It's just that they have reasonably subtle bugs that cause them to misbehave and let attackers go and attack the program either by the internet or other means. Alright, so GCC and other compilers have several techniques in order to help sort of improve the security of your program. Here's a couple of examples. There's an option called minus D underscore fortify source. You can set it to various levels. I suggest two. And also turn on optimization while you're at it. are going to be too slow. This means make your program go faster by spending more time trying to think about how to generate good code. And what this option does, oh, minus D means define a macro. We are defining the macro underscore fortify source to be two. And this sort of tells GCC and also the libraries. to sort of substitute different implementations of standard functions and these different implementations do more runtime checking than the default. So for example, they substitute sort of for the standard memory copy function A sort of pickier version. The way memcopy ordinarily works is you call the function like this. memcopy destination source size. And the idea is that D is a pointer to some area of storage. S is a pointer to some other area of storage. These areas of storage are both of the same size. And memcopy at a fairly low level and reasonably efficiently copies all the stuff from here to here. Now a standard way that people try to break into poorly written C or C++ code is to attack memcopy. They find a part of your program that is copying stuff around. Look for a way to trick your program into specifying a size or a destination or a source that isn't quite right. Once they've tricked your program into doing that, they have overwritten some of its internal data structures in a way that makes your program unsafe and then they're off to the race. Alright, so what does the pickier version of memcopy do? With the compiler's help, what happens is we not only pass sort of the destination, the source, and the number of bytes we want to copy, we tell the underlying runtime system how big the destination region really is and how big the source destination really is. That is, oftentimes this is used to copy part of an array from here to part of another array over here, and what the compiler will do is it will pass to the underlying runtime system where the underlying arrays are. And that way we can check to make sure that the program hasn't been fooled into doing something like this, right? In which the destination doesn't actually exist. It's actually some other object that's going to be a victim. going to do this sort of thing, you're going to slow the program down. Why? Because at runtime now we got to pass a couple extra arguments to memcopy and people who really like performance aren't going to want to do that. So by default in C, this option is turned off to make your code go fast. But if you want it to go slower and have better checking, then you can call the slower routine by defining this macro. Any questions about how this works? or what the motivation might be? Well, I have a question. Does this guarantee that your program will never misuse MemCopy again? And I think you probably can guess the answer. The answer is no. Even when you turn this option on, There are still ways in which a program can still misuse memcopy. For one thing, it could be the case that the caller to memcopy doesn't know what size the underlying objects are. It's just some subroutine that got past pointers and past the size. It doesn't know what the underlying objects look like at all. For calls like that, GCC and GLEPC will give up. They'll just call the ordinary memcopy. no extra checking and that means that a bad guy might be able to break into your program. So this kind of checking isn't perfect, it's just a partial defense against the problem. The goal however being that it's a good enough defense to catch most of the issues. It's not just memcopy. There's a whole bunch of other routines in C and C++ in which you basically are passing pointers to things after all programmers love pointers. They spend like what? Three weeks of lectures in CS31 telling you about pointers, right? And so, you know, to some extent there's a lot of other routines like this and Fortify Source is going to improve the defenses in uses of pretty much any one of those. All right. Any other questions about this one? All right. Let's try some Here's another option. Minus f stack protector. Minus d means please define a preprocessing macro that causes a whole bunch of header files to behave differently. Minus f tells gcc please generate different code for one reason or another. In some sense, minus f changes the It changes the machine instructions that GCC is going to execute. And what FStackProtector does is it says insert a canary word on every non-trivial stack fray. Canary word on non-trivial frays. What am I talking about here? Well, for those of you who've taken, but I guess I should ask, how many of you have not taken CS33? About a third. All right. So, well, here's a brief summary of what's going on. You've got a stack. On most machines, the stack grows negative, certainly on x86-64, so I'm going to draw it in this way. Here's a high address. Hierarche das ist. This part of the stack is unused. Whenever a function returns, it gives up that part of the stack that it's been using and lets somebody else use it. The problem with this approach is that if your program has a bug and that bug has to do with accessing a local array on the stack, there's a good chance things will go bad very quickly. Why? Because you've got this something like that and this is a valid use of memcopy right because we are making sure that we're going into the array but sometimes it'll be more complicated there'll be an arbitrary size here if you draw a picture of what this looks like in memory right h internally has a local variable called buff that's allocated somewhere in h's frame the most likely The mistake you'll make in your code in accessing buff is going to be a subscript error. If you have a subscript error most likely it'll be on the high end of the buffer and if by mistake here s is too big what's going to happen? You'll be stepping onto the buffer and you'll keep stepping and stepping and stepping and now you are now stepping not only on buff but on other stuff in the frame. In particular what the Bad Guys Want To Attack More Than Anything Else Is A Single Word Here Sitting At The High End Of G Called The Return Address The Return Address Is The Instruction Inside G That Will Be Executed When H Returns If You Overrun The Buffer You Will Overwrite The Return Address Which Means The Attacker Now Has Control Over What Instructions Will Be Executed Next They Love This Stuff All right, so how do we defend against that with fStackProtector? We do the following. We insert one extra word in h's frame. That extra word comes before the return address, all right? And this extra word contains a random number that we create when h starts. That's what we do here if we have some local variable here and we set it to some random number. The compiler inserts this code, the programmer doesn't bother. Just before H returns we execute the following extra code. We look at that local variable and we see if it's the same random number that it used to be. If it's not, We crash. The abort function is a standard function. Whenever you call it, your program immediately crashes. It's a very useful function to have. And in effect, fStackProtector inserts this line of code and it inserts this line of code. You don't have to write it. The compiler does it for you. It's called a canary because it's like the canary in the coal mine. If this local variable that otherwise is not not touched by your code has changed. That means somebody is trying to mess with your program. And that means there's like carbon monoxide in your program somewhere. The canary dies. Your program immediately says, "Help, help, help," and everybody rushes out of the mine. All right, any questions on F-Stack Protector and how it works? Can you see any problems with it? Any downsides of having F stack protector? I'll give you a hint. On CSNET, if you use GCC or Clang, this is turned off. On Ubuntu, they've jiggered the compiler. So the F stack protector is on by default. If you don't want this stack protecting on Ubuntu, you have to issue the option F no stack protector. So evidently opinions differ as to how important it is to have stack protection in your code. Why do you suppose that is? Yes? Does it have to do with not knowing how much of the stack, how much you're going to need the stack going after college? So you have more, your cache is now being used. and use less effectively. Also you have to execute these instructions, store and check and all that sort of thing, so that's going to slow you down a bit. For this reason, f-stack protector typically doesn't occur on every function. If your function is really simple and it doesn't have any arrays declared locally, then f-stack protector doesn't insert this code under the theory that, you know, well, there's nothing to protect. Why bother, right? So it'll only insert this code code if you have a local array of size, I don't know, eight or more I think is the usual number. So there's a performance downside of F-Stack Protector and that's the reason why it's a little bit controversial. Other comments about this? I'll ask the same question I asked for FortifySource. Does this prevent stack overflow attacks? Because that's essentially what's What's going on here? We have a stack overflow attack. Your program had a bug. The attacker tried to exploit the bug. This system caught the bug. Will it catch every stack overflow attack? And I hope you see why the answer is no. I have all these leading questions today. Sorry about that. If you're a bad guy, put on a black hat. You're trying to attack a system that's protected with f-stack How can you do a stack overflow attack anyway? I can just see the headlines in the LA Times tomorrow. Professor asked students to break into software systems. That's kind of what I've just done here. But to some extent you have to play this game if you're trying to write reliable software. You have to think of what the bad guys will do. Even if you don't do it yourself. Yes? Yeah, if the attacker can sort of discover the canary's identity, you're toast. Because the attacker will carefully arrange to, maybe the attacker is in charge of the string that's going to be put into the buffer, they'll carefully arrange for the canary to be right and everything else to be wrong. In order to defend against that, it's standard in, stack protector implementations to use a random variable rather than a constant. So there'll be some random place, maybe it's in a register somewhere, and every time you run the program, the canary will be different. So the attacker will have to discover what the canary is for this particular program run. As long as the canary is wide enough, say 64 bits, the chances of that happening are so low that they're not worth worrying about. All right. Let's try another one. All right. Put it up here. This option is not turned on by default in distros like Ubuntu because it's too new. It only works on Intel and AMD devices that are sort of new enough. Right? And what this does is this uses what's called CET, which is short for, it was originally developed by Intel. I believe AMD also supports it now. Don't quote me on that. Intel control flow technology, control flow enforcement technology, I should say. And what it does is it causes sort of the compiler to generate even more instructions which have the following property. You have the ordinary stack, what we've already talked about. and we have a much smaller thing called the shadow stack. The ordinary stack contains your local variables and all that sort of thing, right? Here's main, which calls f, which calls g, and all that sort of thing. In that ordinary stack, you occasionally see return addresses, right? So here's the return address where f is returning to main. Here's the return address where G is returning to F. On the shadow stack what you see are just the return addresses. In this case there's just two return addresses so the shadow stack is very small. It contains just two words. And a key property of the shadow stack is that it's invisible to the program. This part of memory, this shadow stack memory, is sort of invisible or I should say inaccessible to sort of the user code. Your page table basically marks it as being a page that your program can neither read from nor write to unless it issues a special instruction like call a return. What the call instruction does is it pushes the return address on the ordinary stack. It also pushes a copy of the return address on the shadow stack. The return instruction pops words from both stacks and if they don't agree, your program traps. It aborts. It crashes reliably. So what this is doing in hardware is sort of like what fStackProtector is doing in software, except Because it's done in hardware, you don't have to issue anymore machine instructions. And you don't have to worry about someone guessing the canary because the canary is hidden in another part of the cave in a part that you can't even get to in your ordinary software, in your ordinary program. And so what this does is it turns on instructions to use this kind of technology. These instructions, when they're running on older Intel CPUs, are no ops. Only new ones cause these instruction to have an effect. So the idea is you can compile with this flag. Your code will still run on older machines. It won't be as safe, but when it runs on a new machine, it will be using the shadow stack and it will reliably crash in places where other code would not. This option is turned off partly for performance reasons. You want to have a bunch of no ops in your code that a lot of embedded applications are basically going to not benefit from. It's just going to chew up a lot of instruction space. But you can use this if you want reliability on newer CPUs. As you can see there's been a lot of work put into detecting and preventing stack overflow attacks. All right? Software developers are very bummed by stackoverflow.com being one of the more popular websites out there, right? They want to make sure that that stuff doesn't happen again. All right, any comments or questions? Yes. Why did I add three here? Oh, because So basically in real code you'd probably be computing where in the buffer you want to be copying to and I just put this in as one example of a computation. But more generally you can, you'll do a bunch of address arithmetic before you decide where in the buffer you want to copy it to and that's the assumption. The second question is, why does it abort exactly? Oh, the reason it aborts is because if this memcopy has a bug such that it didn't actually modify just the buffer, it went beyond the buffer because it had a subscript overroute the return address, in the process of overriding the return address it's scribbled on everything from the buffer up through the return address and L lives there. So it'll have scribbled on L and almost, you know, with a very high probability, L's value The value will no longer be the value that we set it to initially and that's what that if will check for. Oh this is supposed to say if L is not equal to the number then abort. Comment. Louder please. How does it hide the shadow stack? I mean the ordinary stack is still visible right because you're The program is constantly loading and storing from into here. Well, this, I'm going to wave my hands a little bit because this is really a CS33 thing. But most modern CPUs have what's called a page table which maps the addresses that your program sees, the virtual addresses, to the physical addresses which are inside the actual hardware. In that page table, there's not simply a mapping. For each page, which may be, say, 8 kibbytes or whatever it is, there's Permissions. Some pages you have only read-only access to. For example, if it's a page containing your program's code, that's read-only. You're not allowed to modify the program's code for lots of sort of reliability reasons. This page in the shadow stack is marked unreadable and unwriteable. Your code cannot access it. The operating system keeps track of the fact that this page is associated with your program. Ordinarily, ordinary instructions can't access it. The only instructions that can access it are instructions like call and return. And they do it only indirectly by placing, you know, by putting these doppelgangers of return addresses into them. They can't actually do anything other than push and pop. It only contains return, it's only the shadow stack, doesn't contain anything else. Other comments? All right. So we've been talking about sort of security improvement. Another major area where the compiler can help you more than just calling a debugger is performance improvement. Now if your program is slow, sometimes the best you can do is fire up a debugger, figure out why in the world it's executing this long of code five billion times. It should have just been 50 and that sort of thing. So sometimes you have to use a debugger. But there's a lot of times where you don't have to use a debugger when your performance is bad. You can instead use a compiler to sort of give you some help. I've already referred to one option in this area, which you probably already know about, but it's probably worth mentioning again. The minus capital O option. O is short for optimize or optimize. Mathematicians hate it when we call this process optimization. Because in math, an optimal point is the very best point on the curve. You cannot do better than optimal. Whereas in computer science or in software development, optimization just means it runs faster. It doesn't mean it runs as fast as possible the way a math person would say. So in some sense this is like a sales pitch. It's not really true but we're kind of stuck with the word no. Anyhow, so minus O means optimization but in GCC and many other compilers you have various optimization levels like this. Or if you want to turn off optimization you use minus O zero. What's going on here? There's a trade-off in compilers. between optimizing so that you generate very efficient code and sitting there twiddling your thumbs waiting for the compiler to finish. So, O4 means it's going to take a long time to build your program so long that you might lose patience. Whereas, O0 means, oh, you'll be generating your program in no time flat, but when your program starts to run, it's probably not going to be very fast. here because different developers want to be at different spots in that trade-off. I should warn you though that there's a one and that sort of thing. Once you get up to this level, things are not only slower, but because the compilers are getting fancier and fancier and trying harder and harder and using more and more algorithms that aren't tested all that much, tend to be buggier as well. So you get faster code, but watch out. Buggier maybe? The reason for that is that this is probably the most common optimization level. It gets tested all the time. Everybody compiles with this. If there's a compiler bug at the O2, people will notice it pretty quickly and it'll get fixed. At the higher optimization levels, not so much. It's more likely you'll run into obscure compiler bugs which are really painful to deal with because the source code is right and the executable is wrong. You really don't want to be in that situation. There are other possibilities as well. GCC for example has minus OS. Capital O lowercase s. This says don't optimize for speed. Don't try to make the program run Run as fast as possible. Instead, optimize for code size. Make your executable small. If there's two different ways of implementing the operation, one takes five bytes worth of instruction, the other one takes ten, choose the one that takes five bytes even if it's slower. This flag is used a lot in embedded applications where you may have a small amount of RAM that's available for your program and you want to squeeze it down as much as possible. Use minus capital OS. There's another option which I'm going to tell you basically only in that it's kind of a failure. At least it's been a failure for me in GCC and Clang. Minus OG means for debuggability. What's going on here? Typically, the more you optimize your program, the further it departs from the source code in terms of what really happens at the low level. Once you get to O2 or higher levels, for example, it could be that in your source code, you have have something that looks like this, x equals 1, y equals 2. But if you look at the machine code, for whatever reason, it might decide to do things in the opposite order. It'll do move long of 2 to a y, and it'll do a move long of dollar sign 1 to x. Why would it flip things around? Because the compiler knows how the internal pipelining of the CPU works, and it knows that you're actually doing these instructions in parallel and in some cases flipping the order around can improve parallelization at the low level. Stuff like that. I'm not a compiler expert so I'm sort of hand waving a bit but it does it for good reason. But then what does that mean? If you try to put a breakpoint on your program, you try to debug or single step your program, what's it going to happen? You're going to single step through here and it'll say I'm sorry you know y has been set to 2 but x is still 0. That will be behavior that you can observe in a debugger. The code is fine. The program will work. The user can't tell the difference. But you'll go crazy when you try to debug. Anyhow, what OG tries to do is it tries to optimize as much as possible so long as the resulting code is still debuggable. You don't get sort of code being moved around so much that you get confused when you try to debug the thing. I have found to be honest that it doesn't work very well with me in the sense that I still get confused. It still optimizes enough that I still run into trouble. If I want to debug something that's pretty hairy I end up using this instead. I tell it don't optimize it all because when you try to optimize it even you know a little bit then all of a sudden it gets harder to follow. Any questions on optimization? Yes? The default is, I think it's like equivalent to 0 of 0.5 or something like that, but it's close to 0, the default. Most people don't use the default because the code is just too slow, right? You just can't stand it. But when you're debugging, oftentimes it's worth it to have a really slow program so that you can sort of see what's going on. All right. This way of improving performance ideally is one where you don't have to change your program. Right? Source code stays the same. All you change is that when you build the program you use one of these options and it goes faster. However, this is often not enough to get reasonable performance. And so, question. Well, that's a big topic. I'll give you a short summary. Traditionally, there is a script or a file somewhere that says how to build the program, right? That could be a shell script. The shell script just says, you know, here's how you do the program, right? Run this GCC command, run this GCC command, all that sort of thing. More typically, traditionally, there is a file called makefile. And this makefile contains a bunch of recipes for how to build pieces of your program. So it might say, well, in order to build foo.o, you need foo.c and bar.h, and you run gcc minus o2 foo.c. I guess we need a minus c, that sort of thing. Actual make files are much more sophisticated than this and that sort of thing, but you have a recipe basically for building the program. This idea of having a make file or build script or build instructions happens pretty much in every system. It's safe to say that in the GCC, C, C++ world this stuff is fancier than it is in something like Python. All right. So, so far I've been talking about how to ask your compiler to help you generate faster code, higher performing code. by giving it some options. But sometimes, in order to get good performance, you need to actually change the source code. This is a bigger deal, right? It's more of a hassle because now you've got to go edit the source code as opposed to just fiddling with compiler switches. So here's an example of how to improve performance. In this include file, include standard def.h, in some sense this is the lowest level and simplest include file in C. So we're really getting down to the low level here. In that, there's a definition of a small number of things like the null pointer and that sort of thing, but the one I want to draw your attention to is the unreachable function. You call the unreachable function with no arguments and the definition of the unreachable function is as follows. If your program ever calls the unreachable function, anything can happen. All right? Which means you better not call it. And that's why it's called unreachable. Unreachable says this part of the code cannot be reached. Right? Because if it's ever reached, then, you know, demons could fly out of the program's nose. Anything could happen. the world can this help your program go faster? Obviously you can't call the function. That's not going to make your program go faster. It's going to make your program do something random. If you're lucky it'll crash. If you're unlucky it'll do something terrible. So why is this a performance improvement trick? I'll give you one low-level example that I hope helps to explain that. Suppose you are trying to to how shall we say it? You've got a whole bunch of code in here and it involves a lot of integer arithmetic. And there's a lot of code in here of the form A/B, right? You're doing integer division, which as you know, if you've taken CS133, CS33 I should say, is like the slowest thing you can do with integers. your division takes forever, right? And you want this code to go fast, and you also know that it's often the case that b is a power of two, and you also know that the way your program is compiled, it's often the case that the compiler can deduce b's value. And the reason it's known at compile time is it's, you know, maybe defined as some constants at the start of your program and that sort of thing. You just happen to know that this is going to happen, all right? Now, the problem with trying to sort of generate fast code for this is, as you probably know, suppose the compiler knows that that b is equal to 1024. It's deduced that. It's figured that out. What you would like the compiler to do is to mentally substitute this expression instead. Because right shifts are way faster than division and 1024 is 2 to the 10th. So shifting right by 10 bits is the same as dividing by 1024. Unfortunately, GCC and Clang and the rest won't do this optimization because it's invalid. It's not a correct optimization. What's wrong with this optimization? If we know that this is 1024, and this is some integer of some sort, why can't Why don't we replace this instruction with this instruction and generate faster code? Yes? That's right. This truncates towards zero. This truncates towards minus infinity. So if a is minus one, this yields zero. This yields minus one. Right? Because it rounded down towards minus infinity. So this optimization is only valid when a is non-negative. It's not valid if a is negative in general. If a is negative in a power or two, it's still valid, but all right, that's an issue. So here's how we can tell GCC it's okay to make this optimization. If we happen to know that this integer is non-negative because we wrote the code and we've carefully made sure it's always non-negative, but GCC isn't smart enough to figure it out on its own, which is pretty We can tell GCC that by doing something like this. If A is less than 0, call the unreachable function. This line of code is a message from the programmer to the compiler saying, I know that A is non-negative and I'm telling you that here. What the compiler can do with this line of code is say, If I get here, I can do whatever I like. Which means I can, from here on out, assume that A is non-negative. Because if it's negative, I can do whatever I like. Including assuming that it's non-negative. Which is wrong, right? That's okay. Because Unreachable says you can do whatever you like. So this is a way you can pass information from the human developer to the compiler when the compiler isn't smart enough to figure out something on its own. And that way you get better performance. Any questions on unreachable? All right. Well, this is a good time to take a break. Let's take a break and we'll start up again at the hour by the clock in the back. Thank you. All right. You can tell the compiler that some part of your program is, yes, go ahead. Yes. Abort is slow. If you say abort, the compiler is obliged to generate code that will indeed make your program crash. But you want your program to be fast. So you don't want it to reliably crash if the condition is false. You want to give the compiler the freedom to do whatever it likes, including just running into whatever code happens to be there. The reason for unreachable versus abort is performance. Now, it's a good practice sometimes if you have some code that says unreachable, unreachable, unreachable. Compile it with a flag that turns those unreachables to aborts. Compilers will have that. Then you can run your program in sort of debugging mode, right? So if your unreachable assumptions are wrong, the program will crash right away, which is very nice. And then once you've debugged it, then go turn it back to the normal mode, in which case you now have a fast program. Yes? If we were 100% sure that G would be greater than 0, couldn't we just write the strip ourselves? Yeah, my assumption in that code was sort of like, well, it's part of a larger piece of code. Sometimes the divisor is a power of 2. Sometimes it's not. I'm not quite, you know, I want it to be generic, and I don't want to have to sort of put in a bunch of conditions. Is it a power of 2? So I want the compiler to do it for me. But yeah, that's a valid concern. So if it was less than zero and not a multiple of two, would it just reach the unreachable question? What would happen after that? and that's kind of what unreachable is about. In some sense it's a dangerous tool, right? Because the more unreachables you put into the code, the more likely it is that you made a mistake in putting an unreachable somewhere and that mistake lets the compiler generate whatever code it wants, right? So you have to be careful about unreachable. You should only use it when you know you're right. All right, let's try another one. You can declare a function like this. Here's a function that takes no arguments and we have after its declaration something that looks like this. This is a GNU C extension, underscore, underscore attribute with some attribute of the function afterwards. The reason it's kind of spelled in this funny way is that if you If you want to write code that's portable to non-GCC compilers, the idea is at the start of your program you do something like this. If you're not GCC, then let's define this to be nothing. Define attribute of X to be nothing, end it. So if you put this at the start of your program and you're running on a non-GCC compiler, this turns into nothing and you've just declared a function void f. But if you are running it with GCC, it sees that attribute and what this does is it tells GCC that f is a cold function. By cold, I mean that it's not called very often. And by that I mean if you see sort of some code in some other part of your program that looks like this, right? If i is less than 0, let's call f. The compiler will know that most of the time i is going to be greater than or equal to 0 because f is cold, right? This is a cold part of execution. And if you have other stuff in here, right? N plus plus, then call f. Because f is cold, is cold. What can the compiler do with this information? It can generate machine code in which your cold stuff is stuck somewhere else in your executable. It's not next to your hot stuff. And because it's sitting somewhere else in your executable and it's almost never executed, it can just sit there in RAM. even in secondary storage. Nobody ever looks at it. Nobody cares about it. It's not going to be sitting in your instruction cache, which means your instruction cache can now be used for more useful things like the code that you actually want to run. A large part of making programs go fast these days is worrying about memory accesses as opposed to things like dividing by a power of two or something. And the more often that your program can access cache as opposed to things like dividing by a power of two or something. And the more often that your program can access cache as opposed to things like as opposed to accessing RAM or something else, the faster it will be. And so this advice is advice to make sure your program is more cacheable and therefore faster. You can probably guess what attribute hot means. Hot just means the opposite of cold. It says this function gets called all the time. Right? You should expect that if flow of control can go and call this function. And so it will arrange to put the hot stuff into the part of the program that presumably gets left in the iCache and that sort of thing. Any questions about this attribute? Yes? Can the compiler ever just figure out on its own how to specify? Exactly what I was going to talk about next. Oh, you're the greatest student. I love this, right? A problem with this thing is that it requires you the programmer to know which parts of your program are hot and which aren't. Okay? Sometimes that's pretty easy to tell, but a lot of times it's going to be, you've got to be kidding me, I don't know which parts are right. Or it could be that you've been given a 10 million line program and your boss has said, make it go faster by using cold and hot. And now you've got 10 million lines, I could go faster. Well, here's where the compiler can help you by using profiling. GCC has an option. It has several options. I'll just mention one to you. The coverage option. This option will compile your program with some extra junk in it that will slow down your program. What this extra junk does is is it will count the number of times each function is executed. It will even do more detail like count the number of times each if is taken, that sort of thing. At the end of your program execution, your program will then output a table of all the counts that it sees. You can then feed this table into a later call to GCC on the same program. So GCC can use the counts of an earlier run of your program to optimize the compilation of the program for later runs. Once you do this sort of thing, then you kind of don't need cold and hot anymore, right? Because the system is figuring out for you which functions are cold and which are hot. Now, this system of using coverage isn't perfect though. It has some drawbacks. is it takes longer because first you got to compile it one way, get the tables, then compile it another way, it slows down development. People don't like that. More significantly, the assumption of the coverage option is that your production runs behave like your test runs. Right? Problem with that assumption is that it's sometimes wrong. The test runs that you're using to generate these coverage, you know, these profiles, might have quite different performance behaviors. They might call function F a lot, they never call G, but then in production it turns out, oops, we call G a lot, we never call F. So this coverage approach isn't perfect, but if you need to do, you know, real performance improvement on a program that gets used a lot, say, I don't know, Chromium, say, then using coverage can be a good thing to have. Any questions on this kind of performance? I should mention one other flag that's been a bugaboo for me, at least for the last week or two. The Link Time Optimize flag. Here's how it works. Normally, when you run GCC or Clang and tell it to optimize, it takes each module that it compiles and it optimizes it like crazy. Oh, I'm going to really optimize this. Oh, I'm doing this. But when it optimizes a module, it only looks at the source code to that module and the header files that it includes and all that sort of thing. is optimized independently without knowing what's in any other.o file. And a problem with that approach is it can sometimes generate the code that's pretty bad. For example, it could be that you have a very simple function in here in bar.o where the source code looks like this. Kind of a dumb example, but let's go with it. All right? And then this is in bar.c. When you compile it, this generates a lot of very short machine code, right? Here's the machine code for ID, right? Ret, something like that. Oh, no. MoveL RSI Rx Ret, right? Something like that. I guess it'd be ESI, E, X, whatever. It's a single sort of copy, sort of an argument to the return value and then return. It's just two instructions really fast. But now look at what the caller is going to do. If the caller, which is in foo.c, calls ID of N, it has to actually do something like this. Move long N into RSI, E, I, E, I, E, I, E, I, I, I, I, I, I, I, I We'll get there. Then call ID. So we're executing all of these instructions to do nothing. Now usually this function is going to be more complicated. Maybe it returns n plus 1. So maybe what we have to do is do an add long of EAX here. But still we have to go blah blah blah blah blah blah when all we wanted to do was add 1. What FLTO does is it tells GC to do the following. When you're compiling a module and generating a.0 file, first, generate really bad machine code because we don't care how bad it is. You know, it's like the equivalent of minus capital O digit zero, right? So this means ordinarily generate bad machine code. How shall I say it's slow, not incorrect code in the.o file. But do one other thing while you're at it. Put into the.o file not only the machine code that sort of implements all of these functions, put into that.o file extra information that basically consists of the source code of the original program. So your.o file now gets fatter. It has slow code, probably pretty big, plus a copy of the source. It doesn't have to be a complete copy. It can throw out the comments. It can have some abbreviations and all that sort of thing, but the source code is in there. So far, so bad. This means generate bad code. Why would anybody want to do this? Because if you link a bunch of.o files you do something like this. What GCC will do, assuming that these.o files were compiled with link time optimization, is it will reconstruct the source code to your entire program and then do whole program optimization on the whole thing. When it does that it will notice, oh, this function, it only adds one to its argument. Oh, well, I don't need to do this. All I have to do is this. This is way faster than this because everything's in a register. You don't have a jump, you don't have to do a whole bunch of other stuff, right? So you can do a lot of inlining optimization with FLTO that you can't do otherwise. And your program should go faster. So, given this, what promises to be a major win, what's the downside of FLTO? Why is it causing me headaches? Two things. First off, compiler optimization is kind of a slow business and it doesn't scale very well. the algorithms are, but they gotta be at least order into the fourth. Maybe n cubed in some cases. They're just, the idea is that you really want the program to go fast? Okay, we'll think really hard. The problem is, if you have a huge program and use FLTO, it's gonna take forever to optimize. So one downside of FLTO is it greatly increases development time. You gotta wait and wait and wait for that link phase to finish because it's doing whole program optimization. The second problem with FLTO is that the bigger job you give a compiler like GCC, the more likely it is you're going to run into a compiler bug. Same problem I mentioned earlier with compiler bugs, right? This flag isn't used all that often. It tends to stress the compiler. GCC is like a 50 million line program. It has bugs in it and you're more likely to find those bugs if you use FLTO. So be careful when you use it. But when you use it and it works, it works great. All right. So I've been talking about security and I've been talking about performance improvement. I want to go back a little bit to security and focus on sort of extra checking that you could do. Improving the checking of your program. Because after all, More than half of software development typically is spent fixing bugs in your program. If you can have the compiler find the bug for you instead of you find the bug, you're going to greatly improve your software development efficiency. This kind of checking can be done in one of two major ways, either statically or dynamically. Static checking means the compiler will check your program before it runs and find the bugs before your program starts to run. When static checking works, it's great. You know that for all possible runs of your program, a certain bug cannot occur. However, it tends to be less smart and less flexible than dynamic checking. In dynamic checking, the compiler puts in extra checks in your program as it runs, sort of breaks a rule or does something that you don't want it to do. At that point, your program will print out an error message or exit, that sort of thing. Dynamic checking is more powerful and more flexible, but it slows down your program and there's no guarantee that your program won't crash. Static checking means you know that the error can't occur. Dynamic checking means, well, if it occurs, at least we'll find out about it. Both approaches have their advantages. and you should be familiar with sort of both kinds of techniques. So we're going to start with some static techniques and here's one. In your source code you can do this. I don't know. Int max is less than u int max. So static assert. says, "Please check this condition statically at compile time. If the condition is false, print out an error message and refuse to generate machine code for the program. This program is busted. If it's true, do nothing. Just keep compiling. Everything's cool." The thing inside the parentheses here has to be an expression that can be computed. Its value can be computed at compile time. It has to be an integer constant expression. This particular one, assuming that you've included the appropriate include file, basically says the maximum int value is less than the maximum unsigned int value. On the machines that you're used to, this is 2 to the 31st minus 1, this is 2 to the 32nd minus 1, and so this static assert will succeed. There are machines in which intmax and uintmax have the same value. On those machines, this static assert will fail and you will know that you're trying to run the program on a machine that it's not designed for. Any questions on static assert? A very handy thing to have. You can see any downsides of it? machine code doesn't make your program any slower so it's sort of the essence of static checking here. Let's try some other stuff. With static assert you have to modify the code and there's some other static checking techniques like that. More commonly people don't like to modify the code. They just want to tell the compiler please check my program more carefully. So instead what we can have are some compiler options. That will let your compiler catch common mistakes in code. And one compiler option is this one. GCC minus wall. The minus capital W options of GCC is short for warning. There are like dozens and dozens of these options. And the w all option originally meant turn on all the warning options. But it was found that there are many options that are only useful in some programs and not others. So what this means is enable all sort of commonly useful warnings. by default, your compilation will still work. It's just that it will chatter at you saying, "Oh, this looks kind of dubious. Are you sure about this? Oh, this looks bad. Are you sure?" But your program will still work. So it's not sort of telling you that stuff is wrong, except it is. And it's fairly common amongst many sort of development efforts to say, "Your program must compile cleanly with wall or maybe with a different set of options." of the options that wall implies. All right. Wall includes. Give you a feeling for what's going on here. There'll be a comment. This warns about suspicious looking comments. Here's a suspicious looking comment. It's a valid comment in C. It just happens to have a slash star in it, which means there's a good chance that the programmer actually wrote some code meant to put a slash star here, had something like this in the middle, and then here's a comment, and then mistakenly left off the star slash here. Right? And so you want to warn about that. You should write comments like that. It's sort of, they look kind of funny, right? And you'll likely catch real program errors if you warn about it. I'm listing these warnings roughly in ascending order of controversy. Right? Not everybody likes these warnings. They don't like to be nattered at by the compiler. Or they write programs that violate the rules that these warnings are about. But this first one, hardly anybody complains about it. The next one is W parentheses. It will warn if you write code that looks like this. There's nothing wrong with that code. It's perfectly valid C code. Why do you suppose the GCC developers want to warn about it? Any thoughts? Yes. Yeah, well, informally it's not ambiguous. C has a rule. And the rule in C is plus has higher precedence than shift. Right? So this means add A and B and then shift the result by C bits. The problem is too many people read it as the other way around. Why? Because, you know, this is really a, It sort of feels like this, right? You know, I'm getting there, right? That is, if this is 3, this really feels like multiplying b times 8, right? Except here, times has higher priority than plus. So this, the feel that you get from this is opposite from the feel, I mean, you can't, it's so confusing, right? Right? So what GCC will do is it'll warn about this because it knows or it assumes that it's a common mistake when you're writing code. More controversially, it will warn about this. Now AND has higher precedence than OR, but the developer of GCC actually had, Ph.D in Mechanical Theorem Proving and in the theorem prover he was using, and and or had the same priority and he got confused by this code. So he said, I'm going to warn about this too because it confuses me. And we're still sort of following his rules. So you can obviously fix the problem by putting in parentheses here. Here you can fix the problem by putting in parentheses here. Those parentheses are not necessary, but they will pacify W parentheses. Alright, let's try another one. Minus waddress. This warns about something like the following. Suppose you have a character star pointer and you set it to something and you fool around with it and all that sort of thing and then you have code that looks like this. this code is perfectly valid code and the if is always false. Why? Because this is a pointer comparison in C. You have a character literal with an address that doesn't appear anywhere else in the program. Therefore, P cannot possibly equal the address of this string literal. Therefore, this if statement is always false. That's probably not what the programmer attended and so waddress will generate to generate a warning for this particular mistake. All right, let's try another one. This warning is controversial enough that Linus Torvalds at some point blew his top and said, "I forbid you from using this warning when compiling the Linux kernel." All right? So what's this warning about? You can play games in C by fooling around with pointers. So here's a game that you can play. You can have a long variable L, right? And you can set it to value if you like, why not? And you can later do something that looks like this. That is, you can take the address of that long, but take the address as an int star, not a long star. And then, later on, you can do something like this. And then, later on, you can do something like this. What value gets returned in this case? This is a CS31 question, right? They tell you about pointers and all that sort of thing. It's partly a CS33 question though, right? Because you're doing type punning. You're accessing a piece of storage as if it were an int, even though it's a long. Now what happens on the x86-64 is the following. The long is 8 bytes wide, right? And the x86-64 is little endian, which means the addresses here, if this is address 1000, this is address 1001, all the way up to address 1007 internally. The pointer to a word on the x86-64 points to the sort of the least significant byte in the word. that word and that pointer and treat it as if it were a pointer to an integer, it's pointing to this lower half here of the word, right? And when we store into the lower half, we'll be storing a 12 into this lower half. The top half will still be zero, so we know reliably that this will be 12 if we're on the x86-64. There are some machines that are big Indian, in which case the pointers point to the most No significant byte in the word in which case this code won't work. The C and C++ standard says if your code does this the behavior is undefined. The compiler can generate any code that it likes. The implementation can do whatever it likes and that's to give people the freedom to have either big endian or little endian machines. But the Linux kernel does this sort of thing all the time for efficiency or whatever reasons. They like to have that rule. And therefore, they don't want to have this warning. The strict aliasing warning means basically warn about sort of type punning by a pointer. And basically the Linux kernel, they love doing that. They don't want this warning. They want the code to work. They know that it's little ndn and all that and all that sort of thing. But for general purpose programs this morning could be quite helpful. Yes? No, no, they're very careful about it. That is they write code that'll work on a little Indian machine and it'll also work on a big Indian machine and they'll have like an if def for this and an if def for that. They are like, you know, super duper machine nerds, right? And but they have to tell the compiler don't warn about this stuff and don't make any optimizations based on the fact that this is undefined behavior. You can do what you like. No, no, no, no. Don't do what you like. Just follow sort of the obvious rules. Other questions about this warning? All right. Let's take a look at another one. Even more controversial. This says, this tells the compiler look at all of the local variables in your program or maybe pieces of storage that you've allocated with new or malloc. Make sure that whenever you access a variable that you, the compiler, can prove to your own satisfaction that that variable was initialized before you used it. So if you see code that looks like this, If f n is less than 0, phi equals g of n. Do some other stuff. And later on, if n is less than 0, return c. and we'll change this to be n less than minus 2.9. Alright, so here's some example code. What the C compiler will do, what GCC will do, is it will look and say, oh, here's a variable that's only initialized sometimes. Other times it's not. If we assume that this code in here does not use V, or set V, And if we know that the code in here doesn't change n, then when we get to this return v, we know v has to be initialized. How do we know that? Well, the only way to get here is if n is less than minus 10. And if n is less than minus 10, it has to be less than zero. And if it's less than zero, we know we set v up here. So the compiler is constantly making deductions like that. And what maybe uninitialized does is it causes the compiler to issue a warning if it makes a deduction like that and notices, wait a second, I can't prove that it's going to work. So for example, if we change that to a plus 10, you'll get a warning. Because the compiler will say, I don't know, you know, V might not be set up. here, it might not be initialized, I'm not sure. So it will sort of give you a warning in that case. Now, it could be that if you look at the rest of your program, you look at all the calls, f of a, f of b, b, b, b, b, b, b, b, b, b, b because you've looked at the rest of the program and then you know it. So the code's going to be valid. It's never going to access an uninitialized variable. It's just the compiler isn't smart enough to figure it out, so it issues the warning. Is this warning helpful? I've found it to be helpful almost all the time. Sometimes, though, it gives me a warning when I look at the code and I know that it's is always going to be initialized. In that case, you have a quandary. Do you compile with this warning, get false alarms? Do you compile without the warning and lose the benefit of a lot of good static checking? So that's sort of one thing that you'll have to address. Yes? Is it bad for you, correct? Is it guaranteed that the parameters are positive so that you can lose out some checks that you don't? Oh, so like use the unreachable function here? Sort of, yeah. Yeah, you could use the unreachable. If 0 is less than or equal to n and n is less than 10, unreachable. If we put that up here, that will pacify the maybe uninitialized thing. We're just putting it in this end to pacify the compiler. It's the only reason we're doing it. And I confess, I've played this game at times. Because this is a really helpful warning for most of the code. Sometimes it's wrong. Alright. Now, I've talked about wall, which are commonly used options and useful enough that they're sort of on by default if you include wall. There's a bunch of extra options under this category. These are even more controversial options because They cause more false alarms, shall we say. One of the best knowns of this is the type limits warming. The type limits warming, type limits warning can occur with something like this. You have a standard function in in Linux to say give me the current user's user ID and it's going to be an integer and there's a type, a standard type called UIDT that's big enough to hold any user ID. And then you're worried that it might be out of the range for your program. So you type, oh this is going to be U, right? You write code that looks like this. If U is less than zero or U is greater than 100,000 then you say sort of, you know, error. Funny UID. Something like that. What W type limits will do is it might issue a diagnostic for this if on your platform UIDT happens to be unsigned. On some machines it's signed, on others it's not, depends on the operating system. what it will do is it will say wait a second this value here is unsigned therefore it can't possibly be negative and so I'm going to warn you about it you put in this code that's useless right that's the type limits warning right warn about sort of comparisons that don't make any sense once you know the limits of the types of the operands of the operators. Now I hope this context helps make it clear why this warning is controversial If you're trying to write portable code, which doesn't care whether or not something is signed or unsigned, but you have to put in this just in case it is signed, then you don't want to see this warning. In other cases, you might find it useful. I tend to not like it too much. All right, let's see what else we have. There's some other options that are even more controversial than this. One is an entire family of options called F-analyzer. And to some extent, this option, which I think is only in GCC, it's sort of like many of the above options except it's interprocedural. The maybe uninitialized option only looks at individual functions. It traces through the function, sees that a local variable is used and I'm not sure it's set and it'll give you a warning. F-analyzer basically takes a look at F, takes a look at everybody who calls F, takes a look at everybody that F calls and does a similar analysis on the whole program if you use FLTO. For that reason, this is much more expensive, takes a longer to do the static analysis, produces many more false alarms because it finds a whole bunch of connections that it wouldn't otherwise find, and it also finds more bugs. So it's very much a two-edged sword. I find that for the bigger programs that I use, it's so slow that I can't use it at all. But when I use it, sometimes it really turns up some gems of bugs. It would have taken me a long time to find if I didn't have it available. All right. Here's another one that can be useful for static checkers. Checking. Completely different syntax. This syntax was originally introduced in C++ and has now found its way into C. This is the no return attribute and you can declare it, you can use it something like this. This declares the function exit which you've probably heard about. It exits your program. You pass it an integer, your program exits with that status, and the exit function does not return. It never returns because your program exits, right? This keyword, or this attribute, tells the compiler that the function doesn't return, and this can let the compiler do better static checking of your program. For example, if your code looks something like this, exit 10, RETURN20, your compiler can yell at you saying, "What in the world are you doing here? You just exited. That code's unreachable. I know it's unreachable because you told me with no return." Also, the compiler can generate better code. If you call the exit function in one of your functions, the compiler can generate code that doesn't save the return address because the function isn't going to return. You don't need a return address. So there are both performance and correctness, nicenesses that come from using attributes like this. There's a whole bunch of other attributes like this that are similar. Let's talk about a couple of them. Let's do this one. Int square, int underscore, underscore attributes. "partibute underscore underscore const." Okay? Or another way of spelling it these days is unsequenced. Int square. Int. And I'm assuming this is a very simple function. It just multiplies its argument by itself returns it. It computes the square of its argument. So the const attribute or the unsequenced attribute, what it does is it tells the compiler that this function doesn't depend on the current state of the program. It returns a value that depends only on the function's arguments. So it doesn't look at the state and it doesn't change the state. It's a mathematical function. And what the compiler can do is it can therefore say oh when I'm optimizing calls to square I don't have to worry about square having side effects or changing state of that sort of thing I can reorder the code more than I would reorder it normally this can let the compiler generate better code so there's an unsequenced sort of attribute there's also a reproducible attribute might be something like this. Reproducible is weaker than unsequenced because a reproducible function can depend on the current state of your system. So you can follow that pointer and look to see what the string is that it's pointing at, but you can't change the state of the program. So this hash function is not allowed to store anything into any global variable that anybody can see. And so you can sort of reorder some calls to hash but not others depending on what the rest of your program is doing. So these things are mostly about efficiency. But they're also about correctness in the sense that the compiler can now complain. If you declare If you compare a function, no return, and the compiler is compiling the body of your function, it returns, it can yell at you. Similarly for unsequenced and reproducible. If you're not following the rules of the API, the compiler can sort of catch that as being a mistake. All right. We have another minute. One more topic, which we will not have time to finish. I mentioned runtime checking early on. So far I've avoided that. Because runtime checking isn't as reliable as compile time checking. Means your program might work this time but it might fail in a later time. But there's still a lot of things that you can do with runtime checking. You can have some things in which you change the source code. To improve this runtime checking. And you can have other things in which you use I'll give an example the former and then we'll have to save the rest for later. One of the most common problems that attackers are now using to break into systems, given that stack overflow has been largely defeated, is integer overflow. Right? So the idea being, you convince the to compute, you know, I don't know, m plus n, where these numbers are so large that the integer cannot be represented anymore. And then the rules of C are that if you have assigned integer overflow, the resulting behavior is undefined. The system can do whatever it likes. Other programming languages say, oh, well, the result wraps around modulo 2 to the 32nd or something. Either approach is bad. because it means your program isn't going to behave the way you want it to. So, C has a way of checking for this sort of thing. You can include something called standard CKD int dot H. And instead of writing this expression, you can write something like this. Int R and then if CKD ADDADDADDDRESS OF R MN. This expression returns true if overflow occurred in the addition and false otherwise. So you can now reliably check for addition or multiplication overflow, that sort of thing. Whereas you couldn't do that before in C. There was really no reliable way to check for integer overflow in C. Now, as of C23, which came out a couple years ago, there is. And I'm seeing more and more of this stuff in code that really, really has to be reliable in the presence of attacks. Any questions on this one? All right, we have a little bit more of this to talk about next time, and then we need to talk about debugging. Oh, by the way, I don't know if the, I think the TA's told you, but assignment six is now and it's a little different from what the draft form looks like. You need to, this is the first assignment in which we are requiring you to use generative AI tools like Claude or GitHub Copilot or something like that. We list a bunch of options for tools that you can use and you need to use that as part of this assignment.